{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc9d2d3-2960-408a-af5d-cb3b5ffe3d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Model file exists: True\n",
      "2. File size: 100440399 bytes\n",
      "3. Attempting to load model...\n",
      "4. ‚ùå Model loading failed: cannot import name '_print_elapsed_time' from 'sklearn.utils' (C:\\Users\\jdche\\.conda\\envs\\pycaret311\\Lib\\site-packages\\sklearn\\utils\\__init__.py)\n",
      "   Error type: ImportError\n",
      "   ‚Üí This looks like an import/module issue\n"
     ]
    }
   ],
   "source": [
    "# Run this in your Python console to debug the model file directly\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Check if model file exists and get info\n",
    "model_path = \"models/top_model_2_ExtraTreesRegressor.pkl\"\n",
    "print(f\"1. Model file exists: {os.path.exists(model_path)}\")\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    file_size = os.path.getsize(model_path)\n",
    "    print(f\"2. File size: {file_size} bytes\")\n",
    "    \n",
    "    # Try to load the model step by step\n",
    "    print(\"3. Attempting to load model...\")\n",
    "    try:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        print(f\"4. ‚úÖ Model loaded successfully!\")\n",
    "        print(f\"5. Model type: {type(model)}\")\n",
    "        print(f\"6. Has predict method: {hasattr(model, 'predict')}\")\n",
    "        \n",
    "        # Check if it's a PyCaret model\n",
    "        if hasattr(model, 'named_steps'):\n",
    "            print(\"7. This appears to be a Pipeline model\")\n",
    "            print(f\"   Pipeline steps: {list(model.named_steps.keys())}\")\n",
    "        elif hasattr(model, '_final_estimator'):\n",
    "            print(\"7. This appears to be a PyCaret model\")\n",
    "            print(f\"   Final estimator: {type(model._final_estimator)}\")\n",
    "        else:\n",
    "            print(\"7. This appears to be a direct sklearn model\")\n",
    "        \n",
    "        # Try a simple prediction\n",
    "        print(\"8. Testing prediction...\")\n",
    "        # Create dummy input with the right number of features\n",
    "        n_features = 27  # Adjust based on your model\n",
    "        dummy_input = np.random.rand(1, n_features)\n",
    "        \n",
    "        try:\n",
    "            prediction = model.predict(dummy_input)\n",
    "            print(f\"9. ‚úÖ Prediction successful: {prediction}\")\n",
    "        except Exception as pred_error:\n",
    "            print(f\"9. ‚ùå Prediction failed: {pred_error}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"4. ‚ùå Model loading failed: {e}\")\n",
    "        print(f\"   Error type: {type(e).__name__}\")\n",
    "        \n",
    "        # Check for specific error patterns\n",
    "        error_str = str(e)\n",
    "        if \"cannot import name\" in error_str:\n",
    "            print(\"   ‚Üí This looks like an import/module issue\")\n",
    "        elif \"protocol\" in error_str.lower():\n",
    "            print(\"   ‚Üí This looks like a pickle protocol issue\")\n",
    "        elif \"sklearn\" in error_str.lower():\n",
    "            print(\"   ‚Üí This looks like a sklearn-related issue\")\n",
    "        \n",
    "else:\n",
    "    print(\"Model file not found!\")\n",
    "    print(\"Available files in models folder:\")\n",
    "    if os.path.exists(\"models\"):\n",
    "        for file in os.listdir(\"models\"):\n",
    "            print(f\"  - {file}\")\n",
    "    else:\n",
    "        print(\"  Models folder doesn't exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1ff05-dd20-4f41-b8a1-82b4c38617d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161e647e-7b21-4473-8cd8-a442f733633b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]\n",
      "Scikit-learn version: 1.6.1\n",
      "PyCaret version: 3.3.2\n",
      "PyCaret is available\n",
      "Joblib version: 1.3.2\n",
      "‚ùå _print_elapsed_time not available: cannot import name '_print_elapsed_time' from 'sklearn.utils' (C:\\Users\\jdche\\.conda\\envs\\pycaret311\\Lib\\site-packages\\sklearn\\utils\\__init__.py)\n",
      "This might be the issue!\n",
      "\n",
      "Available in sklearn.utils:\n",
      "['Bunch', 'ClassifierTags', 'DataConversionWarning', 'InputTags', 'RegressorTags', 'Sequence', 'Tags', 'TargetTags', 'TransformerTags', 'all_estimators']\n"
     ]
    }
   ],
   "source": [
    "# Run this to check your environment details\n",
    "\n",
    "import sys\n",
    "import sklearn\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "\n",
    "try:\n",
    "    import pycaret\n",
    "    print(f\"PyCaret version: {pycaret.__version__}\")\n",
    "    print(\"PyCaret is available\")\n",
    "except ImportError as e:\n",
    "    print(f\"PyCaret not available: {e}\")\n",
    "\n",
    "try:\n",
    "    import joblib\n",
    "    print(f\"Joblib version: {joblib.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Joblib not available\")\n",
    "\n",
    "# Check if the specific sklearn utility exists\n",
    "try:\n",
    "    from sklearn.utils import _print_elapsed_time\n",
    "    print(\"‚úÖ _print_elapsed_time is available\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå _print_elapsed_time not available: {e}\")\n",
    "    print(\"This might be the issue!\")\n",
    "\n",
    "# Check sklearn.utils contents\n",
    "print(\"\\nAvailable in sklearn.utils:\")\n",
    "import sklearn.utils\n",
    "attrs = [attr for attr in dir(sklearn.utils) if not attr.startswith('_')]\n",
    "print(attrs[:10])  # Show first 10 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77449e-4447-4c8c-80a6-43078dce8b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc18a204-9926-46e7-b55f-064731532270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Investigating models folder: models\n",
      "============================================================\n",
      "Found 7 files:\n",
      "\n",
      "üìÑ File: others\n",
      "   Size: 4,096 bytes (0.0 MB)\n",
      "   Not a pickle/joblib file\n",
      "\n",
      "üìÑ File: pycaret_processed_features_before_model_training.csv\n",
      "   Size: 33,520 bytes (0.0 MB)\n",
      "   Not a pickle/joblib file\n",
      "\n",
      "üìÑ File: pycaret_processed_target_before_model_training.csv\n",
      "   Size: 416 bytes (0.0 MB)\n",
      "   Not a pickle/joblib file\n",
      "\n",
      "üìÑ File: standard_scaler.pkl\n",
      "   Size: 14,119 bytes (0.0 MB)\n",
      "   Type: <class 'numpy.ndarray'>\n",
      "   Array shape: (182,)\n",
      "   Array dtype: object\n",
      "\n",
      "üìÑ File: top_model_1_GradientBoostingRegressor.pkl\n",
      "   Size: 275,322 bytes (0.3 MB)\n",
      "   ‚ùå Error loading: cannot import name '_print_elapsed_time' from 'sklearn.utils' (C:\\Users\\jdche\\.conda\\envs\\pycaret311\\Lib\\site-packages\\sklearn\\utils\\__init__.py)\n",
      "\n",
      "üìÑ File: top_model_2_ExtraTreesRegressor.pkl\n",
      "   Size: 100,440,399 bytes (95.8 MB)\n",
      "   ‚ùå Error loading: cannot import name '_print_elapsed_time' from 'sklearn.utils' (C:\\Users\\jdche\\.conda\\envs\\pycaret311\\Lib\\site-packages\\sklearn\\utils\\__init__.py)\n",
      "\n",
      "üìÑ File: top_model_3_RandomForestRegressor.pkl\n",
      "   Size: 63,474,591 bytes (60.5 MB)\n",
      "   ‚ùå Error loading: cannot import name '_print_elapsed_time' from 'sklearn.utils' (C:\\Users\\jdche\\.conda\\envs\\pycaret311\\Lib\\site-packages\\sklearn\\utils\\__init__.py)\n",
      "\n",
      "============================================================\n",
      "üîç Analysis Summary:\n",
      "Looking for files that contain actual models (objects with 'predict' method)...\n"
     ]
    }
   ],
   "source": [
    "# Run this to investigate all files in your models folder\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def investigate_models_folder():\n",
    "    \"\"\"Investigate all files in the models folder to find the actual model\"\"\"\n",
    "    \n",
    "    models_dir = \"models\"\n",
    "    \n",
    "    if not os.path.exists(models_dir):\n",
    "        print(\"‚ùå Models folder doesn't exist!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìÅ Investigating models folder: {models_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    files = os.listdir(models_dir)\n",
    "    print(f\"Found {len(files)} files:\")\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(models_dir, file)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        \n",
    "        print(f\"\\nüìÑ File: {file}\")\n",
    "        print(f\"   Size: {file_size:,} bytes ({file_size/1024/1024:.1f} MB)\")\n",
    "        \n",
    "        # Try to determine what's in each file\n",
    "        if file.endswith('.pkl'):\n",
    "            try:\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    obj = pickle.load(f)\n",
    "                \n",
    "                obj_type = type(obj)\n",
    "                print(f\"   Type: {obj_type}\")\n",
    "                \n",
    "                # Analyze based on type\n",
    "                if isinstance(obj, np.ndarray):\n",
    "                    print(f\"   Array shape: {obj.shape}\")\n",
    "                    print(f\"   Array dtype: {obj.dtype}\")\n",
    "                    if obj.size < 20:\n",
    "                        print(f\"   Sample values: {obj.flatten()[:10]}\")\n",
    "                    \n",
    "                elif hasattr(obj, 'predict'):\n",
    "                    print(f\"   ‚úÖ This looks like a model! Has predict method\")\n",
    "                    if hasattr(obj, 'fit'):\n",
    "                        print(f\"   ‚úÖ Also has fit method\")\n",
    "                    if hasattr(obj, '_final_estimator'):\n",
    "                        print(f\"   üéØ PyCaret model detected\")\n",
    "                        print(f\"       Final estimator: {type(obj._final_estimator)}\")\n",
    "                    if hasattr(obj, 'named_steps'):\n",
    "                        print(f\"   üéØ Pipeline detected\")\n",
    "                        print(f\"       Steps: {list(obj.named_steps.keys())}\")\n",
    "                    \n",
    "                elif isinstance(obj, dict):\n",
    "                    print(f\"   Dictionary with {len(obj)} keys\")\n",
    "                    print(f\"   Keys: {list(obj.keys())[:5]}...\")\n",
    "                    \n",
    "                elif isinstance(obj, (list, tuple)):\n",
    "                    print(f\"   {obj_type.__name__} with {len(obj)} items\")\n",
    "                    if len(obj) > 0:\n",
    "                        print(f\"   First item type: {type(obj[0])}\")\n",
    "                        \n",
    "                else:\n",
    "                    print(f\"   Other object: {obj_type}\")\n",
    "                    if hasattr(obj, '__dict__'):\n",
    "                        attrs = [attr for attr in dir(obj) if not attr.startswith('_')]\n",
    "                        print(f\"   Attributes: {attrs[:10]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error loading: {e}\")\n",
    "        \n",
    "        elif file.endswith('.joblib'):\n",
    "            try:\n",
    "                import joblib\n",
    "                obj = joblib.load(file_path)\n",
    "                print(f\"   Type: {type(obj)}\")\n",
    "                if hasattr(obj, 'predict'):\n",
    "                    print(f\"   ‚úÖ This looks like a model!\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error loading joblib: {e}\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"   Not a pickle/joblib file\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üîç Analysis Summary:\")\n",
    "    print(\"Looking for files that contain actual models (objects with 'predict' method)...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    investigate_models_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56f1d6e-011e-49e3-bf45-080b43002928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d24bbb69-48df-483c-b37a-d3c25c061ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting batch model extraction...\n",
      "======================================================================\n",
      "‚úÖ Sklearn compatibility shim installed\n",
      "\n",
      "üîÑ Processing: Gradient Boosting Regressor\n",
      "   File: models\\top_model_1_GradientBoostingRegressor.pkl\n",
      "   Size: 275,322 bytes (0.3 MB)\n",
      "   ‚úÖ Loaded successfully!\n",
      "   Type: <class 'numpy.ndarray'>\n",
      "   üéØ Direct model\n",
      "   üß™ Testing model...\n",
      "   Features expected: 27\n",
      "   ‚ùå Failed: 'numpy.ndarray' object has no attribute 'predict'\n",
      "\n",
      "üîÑ Processing: Extra Trees Regressor\n",
      "   File: models\\top_model_2_ExtraTreesRegressor.pkl\n",
      "   Size: 100,440,399 bytes (95.8 MB)\n",
      "   ‚úÖ Loaded successfully!\n",
      "   Type: <class 'numpy.ndarray'>\n",
      "   üéØ Direct model\n",
      "   üß™ Testing model...\n",
      "   Features expected: 27\n",
      "   ‚ùå Failed: 'numpy.ndarray' object has no attribute 'predict'\n",
      "\n",
      "üîÑ Processing: Random Forest Regressor\n",
      "   File: models\\top_model_3_RandomForestRegressor.pkl\n",
      "   Size: 63,474,591 bytes (60.5 MB)\n",
      "   ‚úÖ Loaded successfully!\n",
      "   Type: <class 'numpy.ndarray'>\n",
      "   üéØ Direct model\n",
      "   üß™ Testing model...\n",
      "   Features expected: 27\n",
      "   ‚ùå Failed: 'numpy.ndarray' object has no attribute 'predict'\n",
      "\n",
      "======================================================================\n",
      "üìä EXTRACTION SUMMARY\n",
      "======================================================================\n",
      "‚úÖ Successfully extracted: 0 models\n",
      "‚ùå Failed: 3 models\n",
      "\n",
      "‚ùå FAILED EXTRACTIONS:\n",
      "   üìÑ Gradient Boosting Regressor: 'numpy.ndarray' object has no attribute 'predict'\n",
      "   üìÑ Extra Trees Regressor: 'numpy.ndarray' object has no attribute 'predict'\n",
      "   üìÑ Random Forest Regressor: 'numpy.ndarray' object has no attribute 'predict'\n",
      "\n",
      "üéØ NEXT STEPS:\n",
      "1. Refresh your Streamlit app\n",
      "2. Look for these new models in the dropdown:\n",
      "3. Select a model and try making predictions!\n",
      "\n",
      "üíî No models were successfully extracted.\n",
      "   You may need to retrain your models with sklearn 1.6.1\n"
     ]
    }
   ],
   "source": [
    "# fix_all_models.py - Extract all your models with compatibility fix\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sklearn.utils\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def setup_sklearn_compatibility():\n",
    "    \"\"\"Setup compatibility shim for sklearn 1.6.1\"\"\"\n",
    "    \n",
    "    def mock_print_elapsed_time(func):\n",
    "        \"\"\"Mock version of the removed sklearn function\"\"\"\n",
    "        def wrapper(*args, **kwargs):\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    \n",
    "    # Inject into multiple possible locations\n",
    "    sklearn.utils._print_elapsed_time = mock_print_elapsed_time\n",
    "    \n",
    "    # Also add to sys.modules to catch all imports\n",
    "    if 'sklearn.utils' in sys.modules:\n",
    "        sys.modules['sklearn.utils']._print_elapsed_time = mock_print_elapsed_time\n",
    "    \n",
    "    print(\"‚úÖ Sklearn compatibility shim installed\")\n",
    "\n",
    "def extract_model(file_path, model_name):\n",
    "    \"\"\"Extract a single model with error handling\"\"\"\n",
    "    \n",
    "    print(f\"\\nüîÑ Processing: {model_name}\")\n",
    "    print(f\"   File: {file_path}\")\n",
    "    \n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"   Size: {file_size:,} bytes ({file_size/1024/1024:.1f} MB)\")\n",
    "    \n",
    "    try:\n",
    "        # Load the model\n",
    "        with open(file_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        print(f\"   ‚úÖ Loaded successfully!\")\n",
    "        print(f\"   Type: {type(model)}\")\n",
    "        \n",
    "        # Extract core model if needed\n",
    "        core_model = model\n",
    "        extraction_info = \"Direct model\"\n",
    "        \n",
    "        if hasattr(model, '_final_estimator'):\n",
    "            core_model = model._final_estimator\n",
    "            extraction_info = f\"Extracted from PyCaret wrapper: {type(core_model)}\"\n",
    "        elif hasattr(model, 'named_steps'):\n",
    "            # Find the main estimator in pipeline\n",
    "            for step_name, step in model.named_steps.items():\n",
    "                if hasattr(step, 'predict') and hasattr(step, 'fit'):\n",
    "                    if any(keyword in str(type(step)).lower() for keyword in ['tree', 'forest', 'boost', 'regressor']):\n",
    "                        core_model = step\n",
    "                        extraction_info = f\"Extracted from pipeline step '{step_name}': {type(core_model)}\"\n",
    "                        break\n",
    "        \n",
    "        print(f\"   üéØ {extraction_info}\")\n",
    "        \n",
    "        # Test the model\n",
    "        print(f\"   üß™ Testing model...\")\n",
    "        \n",
    "        # Determine number of features\n",
    "        n_features = 27  # Default\n",
    "        if hasattr(core_model, 'n_features_in_'):\n",
    "            n_features = core_model.n_features_in_\n",
    "        elif hasattr(core_model, 'feature_importances_'):\n",
    "            n_features = len(core_model.feature_importances_)\n",
    "        \n",
    "        print(f\"   Features expected: {n_features}\")\n",
    "        \n",
    "        # Test prediction\n",
    "        test_input = np.random.rand(1, n_features)\n",
    "        prediction = core_model.predict(test_input)\n",
    "        print(f\"   ‚úÖ Test prediction: {prediction[0]:.2f}\")\n",
    "        \n",
    "        # Save the fixed model\n",
    "        safe_name = model_name.lower().replace(' ', '_').replace('.pkl', '')\n",
    "        new_file_path = f\"models/fixed_{safe_name}.pkl\"\n",
    "        \n",
    "        with open(new_file_path, 'wb') as f:\n",
    "            pickle.dump(core_model, f)\n",
    "        \n",
    "        print(f\"   üíæ Saved to: {new_file_path}\")\n",
    "        \n",
    "        # Verify the saved model\n",
    "        with open(new_file_path, 'rb') as f:\n",
    "            test_model = pickle.load(f)\n",
    "        \n",
    "        test_pred = test_model.predict(test_input)\n",
    "        print(f\"   ‚úÖ Verification successful: {test_pred[0]:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            'original_name': model_name,\n",
    "            'new_path': new_file_path,\n",
    "            'model_type': str(type(core_model)),\n",
    "            'features': n_features,\n",
    "            'test_prediction': prediction[0],\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed: {e}\")\n",
    "        return {\n",
    "            'original_name': model_name,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def fix_all_models():\n",
    "    \"\"\"Fix all models in the models folder\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting batch model extraction...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Setup compatibility\n",
    "    setup_sklearn_compatibility()\n",
    "    \n",
    "    # Define models to process\n",
    "    models_to_process = [\n",
    "        (\"top_model_1_GradientBoostingRegressor.pkl\", \"Gradient Boosting Regressor\"),\n",
    "        (\"top_model_2_ExtraTreesRegressor.pkl\", \"Extra Trees Regressor\"),\n",
    "        (\"top_model_3_RandomForestRegressor.pkl\", \"Random Forest Regressor\")\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for file_name, display_name in models_to_process:\n",
    "        file_path = os.path.join(\"models\", file_name)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            result = extract_model(file_path, display_name)\n",
    "            results.append(result)\n",
    "        else:\n",
    "            print(f\"\\n‚ùå File not found: {file_path}\")\n",
    "            results.append({\n",
    "                'original_name': display_name,\n",
    "                'success': False,\n",
    "                'error': 'File not found'\n",
    "            })\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä EXTRACTION SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    successful = [r for r in results if r['success']]\n",
    "    failed = [r for r in results if not r['success']]\n",
    "    \n",
    "    print(f\"‚úÖ Successfully extracted: {len(successful)} models\")\n",
    "    print(f\"‚ùå Failed: {len(failed)} models\")\n",
    "    \n",
    "    if successful:\n",
    "        print(f\"\\nüéâ SUCCESSFULLY EXTRACTED MODELS:\")\n",
    "        for result in successful:\n",
    "            print(f\"   üìÑ {result['original_name']}\")\n",
    "            print(f\"      ‚Üí {result['new_path']}\")\n",
    "            print(f\"      ‚Üí Type: {result['model_type']}\")\n",
    "            print(f\"      ‚Üí Features: {result['features']}\")\n",
    "            print(f\"      ‚Üí Test prediction: {result['test_prediction']:.2f}\")\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"\\n‚ùå FAILED EXTRACTIONS:\")\n",
    "        for result in failed:\n",
    "            print(f\"   üìÑ {result['original_name']}: {result['error']}\")\n",
    "    \n",
    "    print(f\"\\nüéØ NEXT STEPS:\")\n",
    "    print(f\"1. Refresh your Streamlit app\")\n",
    "    print(f\"2. Look for these new models in the dropdown:\")\n",
    "    for result in successful:\n",
    "        model_name = os.path.basename(result['new_path']).replace('.pkl', '').replace('_', ' ').title()\n",
    "        print(f\"   - '{model_name}'\")\n",
    "    print(f\"3. Select a model and try making predictions!\")\n",
    "    \n",
    "    return successful, failed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    successful, failed = fix_all_models()\n",
    "    \n",
    "    if successful:\n",
    "        print(f\"\\nüåü SUCCESS! Extracted {len(successful)} models ready for use!\")\n",
    "    else:\n",
    "        print(f\"\\nüíî No models were successfully extracted.\")\n",
    "        print(f\"   You may need to retrain your models with sklearn 1.6.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942c1bc-f8af-47fd-8880-366a101e6779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
