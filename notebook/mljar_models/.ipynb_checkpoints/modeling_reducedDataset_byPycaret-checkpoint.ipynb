{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d858163-147e-4e71-9b62-fc6caf6e810f",
   "metadata": {},
   "source": [
    "<span style=\"color: blue;font-weight: bold; font-size: 40px;\">PyCaret: ISBSG Data Analysis & Regression </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65f029bb-d607-4dda-bb25-7cd74df69769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <span style=\"color: blue;\">ISBSG Data Analysis & Regression</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1361d7b3-22a6-4a13-85d9-3b2d5872d03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ISBSG Data Analysis and Regression Modeling\n",
    "# \n",
    "# This notebook performs data cleaning, preprocessing, and regression modeling on the ISBSG dataset.\n",
    "\n",
    "# ## Setup and Environment Configuration\n",
    "\n",
    "# Install required packages (uncomment if needed)\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc191195-5abd-4d48-8376-d9de1b57987a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycaret'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpycaret\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pycaret'"
     ]
    }
   ],
   "source": [
    "# Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pycaret\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f3ef8-d3e8-464e-bc57-8be3464f1ddf",
   "metadata": {},
   "source": [
    "<a id = 'Index:'></a>\n",
    "\n",
    "# Table of Content\n",
    "\n",
    "In this notebook you will apply xxxxxxx\n",
    "\n",
    "\n",
    "- [Part 1](#part1)- Data Loading and Initial Exploration\n",
    "- [Part 2](#part2)- Data Cleaning and Preprocessing\n",
    "- [Part 3](#part3)- Data Profiling\n",
    "- [Part 4](#part4)- Module Building with PyCaret\n",
    "- [Part 5](#part5)- Model Preparation\n",
    "- [Part 6](#part6)- Baseline Modeling and Evaluation\n",
    "- [Part 7](#part7)- Advanced Modeling and Hyperparameter Tuning\n",
    "- [Part 8](#part8)- Model Comparison and Selection\n",
    "- [Part 9](#part9)- End\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c58c6d0-4039-44fb-9b6a-515296790a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure timestamp callback for Jupyter cells\n",
    "from IPython import get_ipython\n",
    "\n",
    "def setup_timestamp_callback():\n",
    "    \"\"\"Setup a timestamp callback for Jupyter cells without clearing existing callbacks.\"\"\"\n",
    "    ip = get_ipython()\n",
    "    if ip is not None:\n",
    "        # Define timestamp function\n",
    "        def print_timestamp(*args, **kwargs):\n",
    "            \"\"\"Print timestamp after cell execution.\"\"\"\n",
    "            print(f\"Cell executed at: {datetime.now()}\")\n",
    "        \n",
    "        # Check if our callback is already registered\n",
    "        callbacks = ip.events.callbacks.get('post_run_cell', [])\n",
    "        for cb in callbacks:\n",
    "            if hasattr(cb, '__name__') and cb.__name__ == 'print_timestamp':\n",
    "                # Already registered\n",
    "                return\n",
    "                \n",
    "        # Register new callback if not already present\n",
    "        ip.events.register('post_run_cell', print_timestamp)\n",
    "        print(\"Timestamp printing activated.\")\n",
    "    else:\n",
    "        print(\"Not running in IPython/Jupyter environment.\")\n",
    "\n",
    "# Setup timestamp callback\n",
    "setup_timestamp_callback()\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622026d3-5fe1-4fce-8def-b39811aeb7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08d9ec97-3705-4c9d-a1e6-e0dbcd1d99ac",
   "metadata": {},
   "source": [
    "[Back to top](#Index:)\n",
    "\n",
    "<a id='part1'></a>\n",
    "\n",
    "# Part 1 -Data Loading and Initial Exploration\n",
    "\n",
    "This section is dedicated to loading the dataset, performing initial data exploration such as viewing the first few rows, and summarizing the dataset's characteristics, including missing values and basic statistical measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fd30b9-fe3e-4952-9237-cf18928a4958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "file_path = \"../data/ISBSG2016R1_1_FormattedForCSV_cleaned.csv\"\n",
    "file_name_no_ext = Path(file_path).stem                # 'ISBSG2016R1.1 - FormattedForCSV'\n",
    "print(file_name_no_ext)\n",
    "\n",
    "\n",
    "df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fbf4a8-155b-44ba-882f-17c40b0a202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_header(text):\n",
    "    try:\n",
    "        from IPython.display import display, Markdown\n",
    "        display(Markdown(f\"# {text}\"))\n",
    "    except ImportError:\n",
    "        print(f\"\\n=== {text} ===\\n\")\n",
    "\n",
    "def display_subheader(text):\n",
    "    try:\n",
    "        from IPython.display import display, Markdown\n",
    "        display(Markdown(f\"## {text}\"))\n",
    "    except ImportError:\n",
    "        print(f\"\\n-- {text} --\\n\")\n",
    "\n",
    "def explore_data(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Perform exploratory data analysis on the input DataFrame with nicely aligned plots.\n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "    \"\"\"\n",
    "    from IPython.display import display\n",
    "\n",
    "    display_header(\"Exploratory Data Analysis\")\n",
    "    \n",
    "    # Data Overview\n",
    "    display_subheader(\"Data Overview\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    if df.shape[0] > 20:\n",
    "        print(\"First 5 rows:\")\n",
    "        display(df.head())\n",
    "        print(\"Last 5 rows:\")\n",
    "        display(df.tail())\n",
    "    else:\n",
    "        display(df)\n",
    "    \n",
    "    # Duplicate Row Checking\n",
    "    display_subheader(\"Duplicate Rows\")\n",
    "    num_duplicates = df.duplicated().sum()\n",
    "    print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "\n",
    "    # Data Types and Memory Usage\n",
    "    display_subheader(\"Data Types and Memory Usage\")\n",
    "    dtype_info = pd.DataFrame({\n",
    "        'Data Type': df.dtypes,\n",
    "        'Memory Usage (MB)': df.memory_usage(deep=True) / 1024 / 1024\n",
    "    })\n",
    "    display(dtype_info)\n",
    "    \n",
    "    # Unique Values Per Column\n",
    "    display_subheader(\"Unique Values Per Column\")\n",
    "    for col in df.columns:\n",
    "        print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "    \n",
    "    # Type Conversion Suggestions\n",
    "    display_subheader(\"Type Conversion Suggestions\")\n",
    "    potential_cat = [\n",
    "        col for col in df.select_dtypes(include=['object']).columns\n",
    "        if df[col].nunique() < max(30, 0.05*df.shape[0])\n",
    "    ]\n",
    "    if potential_cat:\n",
    "        print(\"Consider converting to 'category' dtype for memory/performance:\")\n",
    "        print(potential_cat)\n",
    "    else:\n",
    "        print(\"No obvious candidates for 'category' dtype conversion.\")\n",
    "    \n",
    "    # Summary Statistics\n",
    "    display_subheader(\"Summary Statistics\")\n",
    "    try:\n",
    "        display(df.describe(include='all').T.style.background_gradient(cmap='Blues', axis=1))\n",
    "    except Exception:\n",
    "        display(df.describe(include='all').T)\n",
    "    \n",
    "    # Missing Values\n",
    "    display_subheader(\"Missing Values\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_percent = (missing / len(df)) * 100\n",
    "    missing_info = pd.DataFrame({\n",
    "        'Missing Values': missing,\n",
    "        'Percentage (%)': missing_percent.round(2)\n",
    "    })\n",
    "    if missing.sum() > 0:\n",
    "        display(missing_info[missing_info['Missing Values'] > 0]\n",
    "                .sort_values('Missing Values', ascending=False)\n",
    "                .style.background_gradient(cmap='Reds'))\n",
    "        # Visualize missing values\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        cols_with_missing = missing_info[missing_info['Missing Values'] > 0].index\n",
    "        if len(cols_with_missing) > 0:\n",
    "            sns.heatmap(df[cols_with_missing].isnull(), \n",
    "                        cmap='viridis', \n",
    "                        yticklabels=False, \n",
    "                        cbar_kws={'label': 'Missing Values'})\n",
    "            plt.title('Missing Value Patterns')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No missing values in the dataset.\")\n",
    "    \n",
    "    # Numerical Distributions\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    if len(numerical_cols) > 0:\n",
    "        display_subheader(\"Distribution of Numerical Features\")\n",
    "        sample_cols = numerical_cols[:min(12, len(numerical_cols))]\n",
    "        num_cols = len(sample_cols)\n",
    "        num_rows = (num_cols + 2) // 3  # 3 plots per row, rounded up\n",
    "        fig = plt.figure(figsize=(18, num_rows * 4))\n",
    "        grid = plt.GridSpec(num_rows, 3, figure=fig, hspace=0.4, wspace=0.3)\n",
    "        for i, col in enumerate(sample_cols):\n",
    "            row, col_pos = divmod(i, 3)\n",
    "            ax = fig.add_subplot(grid[row, col_pos])\n",
    "            sns.histplot(df[col].dropna(), kde=True, ax=ax, color='skyblue', alpha=0.7)\n",
    "            mean_val = df[col].mean()\n",
    "            median_val = df[col].median()\n",
    "            ax.axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.2f}')\n",
    "            ax.axvline(median_val, color='green', linestyle=':', label=f'Median: {median_val:.2f}')\n",
    "            stats_text = (f\"Std: {df[col].std():.2f}\\n\"\n",
    "                          f\"Min: {df[col].min():.2f}\\n\"\n",
    "                          f\"Max: {df[col].max():.2f}\")\n",
    "            props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "            ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, fontsize=9,\n",
    "                    verticalalignment='top', bbox=props)\n",
    "            ax.set_title(f'Distribution of {col}')\n",
    "            ax.legend(fontsize='small')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        # Correlation matrix and top correlations\n",
    "        if len(numerical_cols) > 1:\n",
    "            display_subheader(\"Correlation Matrix\")\n",
    "            corr = df[numerical_cols].corr().round(2)\n",
    "            mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            sns.heatmap(corr, mask=mask, annot=True, cmap='coolwarm', \n",
    "                        fmt=\".2f\", linewidths=0.5, vmin=-1, vmax=1, \n",
    "                        annot_kws={\"size\": 10})\n",
    "            plt.title('Correlation Matrix (Lower Triangle Only)', fontsize=14)\n",
    "            plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "            plt.yticks(fontsize=10)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            # Top correlations\n",
    "            if len(numerical_cols) > 5:\n",
    "                corr_unstack = corr.unstack()\n",
    "                corr_abs = corr_unstack.apply(abs)\n",
    "                corr_abs = corr_abs[corr_abs < 1.0]\n",
    "                highest_corrs = corr_abs.sort_values(ascending=False).head(15)\n",
    "                display_subheader(\"Top Correlations\")\n",
    "                for (col1, col2), corr_val in highest_corrs.items():\n",
    "                    actual_val = corr.loc[col1, col2]\n",
    "                    print(f\"{col1} — {col2}: {actual_val:.2f}\")\n",
    "                pairs_to_plot = [(idx[0], idx[1]) for idx in highest_corrs.index][:6]\n",
    "                if pairs_to_plot:\n",
    "                    fig = plt.figure(figsize=(18, 12))\n",
    "                    grid = plt.GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\n",
    "                    for i, (col1, col2) in enumerate(pairs_to_plot):\n",
    "                        row, col_pos = divmod(i, 3)\n",
    "                        ax = fig.add_subplot(grid[row, col_pos])\n",
    "                        sns.regplot(x=df[col1], y=df[col2], ax=ax, scatter_kws={'alpha':0.5})\n",
    "                        r_value = df[col1].corr(df[col2])\n",
    "                        ax.set_title(f'{col1} vs {col2} (r = {r_value:.2f})')\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "    # Categorical columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    if len(categorical_cols) > 0:\n",
    "        display_subheader(\"Categorical Features\")\n",
    "        sample_cat_cols = categorical_cols[:min(6, len(categorical_cols))]\n",
    "        num_cat_cols = len(sample_cat_cols)\n",
    "        num_cat_rows = (num_cat_cols + 1) // 2\n",
    "        fig = plt.figure(figsize=(18, num_cat_rows * 5))\n",
    "        grid = plt.GridSpec(num_cat_rows, 2, figure=fig, hspace=0.4, wspace=0.2)\n",
    "        for i, col in enumerate(sample_cat_cols):\n",
    "            row, col_pos = divmod(i, 2)\n",
    "            ax = fig.add_subplot(grid[row, col_pos])\n",
    "            value_counts = df[col].value_counts().sort_values(ascending=False)\n",
    "            top_n = min(10, len(value_counts))\n",
    "            if len(value_counts) > top_n:\n",
    "                top_values = value_counts.head(top_n-1)\n",
    "                other_count = value_counts.iloc[top_n-1:].sum()\n",
    "                plot_data = pd.concat([top_values, pd.Series({'Other': other_count})])\n",
    "            else:\n",
    "                plot_data = value_counts\n",
    "            sns.barplot(x=plot_data.values, y=plot_data.index, ax=ax, palette='viridis')\n",
    "            ax.set_title(f'Distribution of {col} (Total: {len(value_counts)} unique values)')\n",
    "            ax.set_xlabel('Count')\n",
    "            total = plot_data.sum()\n",
    "            for j, v in enumerate(plot_data.values):\n",
    "                percentage = v / total * 100\n",
    "                ax.text(v + 0.1, j, f'{percentage:.1f}%', va='center')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        # Categorical-numerical boxplots\n",
    "        if numerical_cols and len(categorical_cols) > 0:\n",
    "            display_subheader(\"Categorical-Numerical Relationships\")\n",
    "            numerical_variances = df[numerical_cols].var()\n",
    "            target_numerical = numerical_variances.idxmax()\n",
    "            sample_cat_for_box = [col for col in categorical_cols \n",
    "                                  if df[col].nunique() <= 15][:4]\n",
    "            if sample_cat_for_box:\n",
    "                fig = plt.figure(figsize=(18, 5 * len(sample_cat_for_box)))\n",
    "                for i, cat_col in enumerate(sample_cat_for_box):\n",
    "                    ax = fig.add_subplot(len(sample_cat_for_box), 1, i+1)\n",
    "                    order = df.groupby(cat_col)[target_numerical].median().sort_values().index\n",
    "                    sns.boxplot(x=cat_col, y=target_numerical, data=df, ax=ax, \n",
    "                                order=order, palette='Set3')\n",
    "                    ax.set_title(f'{cat_col} vs {target_numerical}')\n",
    "                    ax.set_xlabel(cat_col)\n",
    "                    ax.set_ylabel(target_numerical)\n",
    "                    plt.xticks(rotation=45, ha='right')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "explore_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325c3beb-a93e-4659-a072-00c25bbbb21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names function\n",
    "def clean_column_names(columns):\n",
    "    cleaned_cols = []\n",
    "    for col in columns:\n",
    "        # First replace ampersands with _&_ to match PyCaret's transformation\n",
    "        col_clean = col.replace(' & ', '_&_')\n",
    "        # Then remove any remaining special chars\n",
    "        col_clean = re.sub(r'[^\\w\\s&]', '', col_clean)\n",
    "        # Finally replace spaces with underscores\n",
    "        col_clean = col_clean.replace(' ', '_')\n",
    "        cleaned_cols.append(col_clean)\n",
    "    return cleaned_cols\n",
    "\n",
    "# Clean column names\n",
    "original_columns = df.columns.tolist()  # Save original column names for reference\n",
    "df.columns = clean_column_names(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce49eec-356d-4d07-b980-529083310368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from original to cleaned column names\n",
    "column_mapping = dict(zip(original_columns, df.columns))\n",
    "print(\"\\nColumn name mapping (original -> cleaned):\")\n",
    "for orig, clean in column_mapping.items():\n",
    "    if orig != clean:  # Only show columns that changed\n",
    "        print(f\"  '{orig}' -> '{clean}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64360b6-c4b3-4237-a553-b67aa75179f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f2be3-8eb2-48be-8014-15d76b7b4497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get comprehensive data summary\n",
    "def get_data_summary(df, n_unique_samples=5):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary of the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas DataFrame\n",
    "        n_unique_samples: Number of unique values to show as sample\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with summary information\n",
    "    \"\"\"\n",
    "    # Summary dataframe with basic info\n",
    "    summary = pd.DataFrame({\n",
    "        'Feature': df.columns,\n",
    "        'data_type': df.dtypes.values,\n",
    "        'Null_number': df.isnull().sum().values,\n",
    "        'Null_pct': (df.isnull().mean() * 100).values,\n",
    "        'Unique_counts': df.nunique().values,\n",
    "        'unique_samples': [list(df[col].dropna().unique()[:n_unique_samples]) for col in df.columns]\n",
    "    })\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate and display data summary\n",
    "summary_df = get_data_summary(df)\n",
    "print(\"\\nData Summary (first 10 columns):\")\n",
    "print(summary_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6992df08-5bd0-46a1-b342-050a8305315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target column\n",
    "target_col = 'project_prf_normalised_work_effort'\n",
    "print(f\"\\nTarget variable: '{target_col}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13dacd-ecd5-4a4a-a559-105f99f9bc7b",
   "metadata": {},
   "source": [
    "[Back to top](#Index:)\n",
    "\n",
    "<a id='part2'></a>\n",
    "\n",
    "# Part 2 - Data Cleaning and Preprocessing\n",
    "\n",
    "Here, data cleaning tasks like handling missing values and providing a detailed summary of each feature, including its type, number of unique values, and a preview of unique values, are performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1478ab15-90a8-47aa-9ed6-831732588629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse missing values\n",
    "print(\"\\nAnalysing missing values...\")\n",
    "missing_pct = df.isnull().mean() * 100\n",
    "missing_sorted = missing_pct.sort_values(ascending=False)\n",
    "print(\"Top 10 columns with highest missing percentages:\")\n",
    "print(missing_sorted.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eed99a-7179-4aae-a7de-6d0cedbff7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with high missing values (>70%)\n",
    "high_missing_cols = missing_pct[missing_pct > 70].index.tolist()\n",
    "print(f\"\\nColumns with >70% missing values ({len(high_missing_cols)} columns):\")\n",
    "for col in high_missing_cols[:5]:  # Show first 5\n",
    "    print(f\"  - {col}: {missing_pct[col]:.2f}% missing\")\n",
    "if len(high_missing_cols) > 5:\n",
    "    print(f\"  - ... and {len(high_missing_cols) - 5} more columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9106b8ff-475a-406a-b1b1-203e0cd4e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clean dataframe by dropping high-missing columns\n",
    "df_clean = df.drop(columns=high_missing_cols)\n",
    "print(f\"\\nData shape after dropping high-missing columns: {df_clean.shape}\")\n",
    "print(f\"\\nHigh missing columns got dropped are: {high_missing_cols}\")\n",
    "\n",
    "# Numerical columns\n",
    "num_cols = df_clean.select_dtypes(include=['number']).columns.tolist()\n",
    "print(\"\\nNumerical columns:\")\n",
    "print(num_cols)\n",
    "\n",
    "# Categorical columns (object or category dtype)\n",
    "cat_cols = df_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(\"\\nCategorical columns:\")\n",
    "print(cat_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d90a2b-18c7-47d4-b5d2-54f5f635ef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle remaining missing values\n",
    "print(\"\\nHandling remaining missing values...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da6431-b898-4bdc-afce-cd700f5b2a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in categorical columns with \"Missing\"\n",
    "cat_cols = df_clean.select_dtypes(include=['object', 'category']).columns\n",
    "for col in cat_cols:\n",
    "    df_clean[col].fillna('Missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc7aeb-1e09-4c8e-8c5b-38511eb644fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check remaining missing values\n",
    "remaining_missing = df_clean.isnull().sum()\n",
    "remaining_missing_count = sum(remaining_missing > 0)\n",
    "print(f\"\\nColumns with remaining missing values: {remaining_missing_count}\")\n",
    "if remaining_missing_count > 0:\n",
    "    print(\"Top columns with missing values:\")\n",
    "    print(remaining_missing[remaining_missing > 0].sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256be495-1710-46ab-8fbd-61d703cf0a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify target variable\n",
    "print(f\"\\nTarget variable '{target_col}' summary:\")\n",
    "print(f\"Unique values: {df_clean[target_col].nunique()}\")\n",
    "print(f\"Missing values: {df_clean[target_col].isnull().sum()}\")\n",
    "print(f\"Top value counts:\")\n",
    "print(df_clean[target_col].value_counts().head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41adf2f-9588-4d04-a0c3-265cb3029645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for infinite values\n",
    "inf_check = np.isinf(df_clean.select_dtypes(include=[np.number])).sum().sum()\n",
    "print(f\"\\nNumber of infinite values: {inf_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e09da5f-54a5-406f-b042-d71f0c52aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "\n",
    "file_name_no_ext\n",
    "\n",
    "df_clean.to_csv(f'../data/{file_name_no_ext}_dropped.csv', index=False)\n",
    "print(f'../data/{file_name_no_ext}_dropped.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28854fb-ea58-4788-9a87-6221a41629b2",
   "metadata": {},
   "source": [
    "[Back to top](#Index:)\n",
    "\n",
    "<a id='part3'></a>\n",
    "\n",
    "# Part 3 - Feature Engineering and Selection\n",
    "\n",
    "Involves creating or selecting specific features for the model based on insights from EDA, including handling categorical variables and reducing dimensionality if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02146b23-08d2-4afe-a4f3-df54babca0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns and check cardinality\n",
    "print(\"\\nCategorical columns and their cardinality:\")\n",
    "cat_cols = df_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "for col in cat_cols[:5]:  # Show first 5\n",
    "    print(f\"  {col}: {df_clean[col].nunique()} unique values\")\n",
    "if len(cat_cols) > 5:\n",
    "    print(f\"  ... and {len(cat_cols) - 5} more columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13827e97-52a7-4706-93e8-56993f34bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical columns with low cardinality (<10 unique values)\n",
    "low_card_cols = [col for col in cat_cols if df_clean[col].nunique() < 10]\n",
    "print(f\"\\nApplying one-hot encoding to {len(low_card_cols)} low-cardinality columns:\")\n",
    "for col in low_card_cols[:5]:  # Show first 5\n",
    "    print(f\"  - {col}\")\n",
    "if len(low_card_cols) > 5:\n",
    "    print(f\"  - ... and {len(low_card_cols) - 5} more columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3b252-b306-42b8-b048-52f5770905cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoded dataframe\n",
    "df_encoded = pd.get_dummies(df_clean, columns=low_card_cols, drop_first=True)\n",
    "print(f\"\\nData shape after one-hot encoding: {df_encoded.shape}\")\n",
    "print(\"\\nAll column names:\")\n",
    "print(df_encoded.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d55a7-732b-4036-a48d-0fcb7ce960e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUALLY fix the problematic column names BEFORE PyCaret setup\n",
    "\n",
    "# Function to fix the column names and prevent duplicates\n",
    "def fix_column_names_no_duplicates(df):\n",
    "    \"\"\"Fix column names that cause issues with PyCaret while preventing duplicates.\"\"\"\n",
    "    original_cols = df.columns.tolist()\n",
    "    fixed_columns = []\n",
    "    \n",
    "    # Track columns to check for duplicates\n",
    "    seen_columns = set()\n",
    "    \n",
    "    for col in original_cols:\n",
    "        # Replace spaces with underscores\n",
    "        fixed_col = col.replace(' ', '_')\n",
    "        # Replace ampersands \n",
    "        fixed_col = fixed_col.replace('&', 'and')\n",
    "        # Remove any other problematic characters\n",
    "        fixed_col = ''.join(c if c.isalnum() or c == '_' else '_' for c in fixed_col)\n",
    "        \n",
    "        # Handle duplicates by appending a suffix\n",
    "        base_col = fixed_col\n",
    "        suffix = 1\n",
    "        while fixed_col in seen_columns:\n",
    "            fixed_col = f\"{base_col}_{suffix}\"\n",
    "            suffix += 1\n",
    "        \n",
    "        seen_columns.add(fixed_col)\n",
    "        fixed_columns.append(fixed_col)\n",
    "    \n",
    "    # Create a new DataFrame with fixed column names\n",
    "    df_fixed = df.copy()\n",
    "    df_fixed.columns = fixed_columns\n",
    "    \n",
    "    # Print statistics about the renaming\n",
    "    n_changed = sum(1 for old, new in zip(original_cols, fixed_columns) if old != new)\n",
    "    print(f\"Changed {n_changed} column names.\")\n",
    "    \n",
    "    # Check for duplicates in the new column names\n",
    "    dup_check = [item for item, count in pd.Series(fixed_columns).value_counts().items() if count > 1]\n",
    "    if dup_check:\n",
    "        print(f\"WARNING: Found {len(dup_check)} duplicate column names after fixing: {dup_check}\")\n",
    "    else:\n",
    "        print(\"No duplicate column names in the fixed DataFrame.\")\n",
    "    \n",
    "    return df_fixed\n",
    "\n",
    "# Show some of the original column names to help diagnose issues\n",
    "print(\"\\nSample of original column names:\")\n",
    "for i, col in enumerate(df_encoded.columns[:15]):  # Show first 15 for diagnosis\n",
    "    print(f\"{i}: {col}\")\n",
    "\n",
    "# Apply the fix to your dataframe\n",
    "print(\"\\nFixing column names for PyCaret compatibility...\")\n",
    "df_fixed = fix_column_names_no_duplicates(df_encoded)\n",
    "\n",
    "# Print some example fixed columns to verify\n",
    "print(\"\\nSample of fixed column names:\")\n",
    "for i, (old, new) in enumerate(zip(df_encoded.columns[:15], df_fixed.columns[:15])):\n",
    "    print(f\"Original: {old} -> Fixed: {new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c537ff5-d690-42ed-8d3c-f9e8bbb8bd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this DataFrame with fixed column names\n",
    "\n",
    "df_fixed.to_csv(f'../data/{file_name_no_ext}_fixed_columns_data.csv', index=False)\n",
    "print(f\"Saved data with fixed column names to '../data/{file_name_no_ext}_fixed_columns_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5242616c-6c05-4fda-9d71-bbc80b0d7e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a diagnostic file with all column transformations\n",
    "with open('../temp/column_transformations.txt', 'w') as f:\n",
    "    f.write(\"Column name transformations:\\n\")\n",
    "    f.write(\"--------------------------\\n\")\n",
    "    for old, new in zip(df_encoded.columns, df_fixed.columns):\n",
    "        f.write(f\"{old} -> {new}\\n\")\n",
    "print(\"Saved complete column transformations to '../temp/column_transformations.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42eca8-d0cf-4068-9f5e-d690f0b93bd6",
   "metadata": {},
   "source": [
    "[Back to top](#Index:)\n",
    "\n",
    "<a id='part4'></a>\n",
    "\n",
    "# Part 4 - Data Profiling\n",
    "\n",
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e217e573-7d82-4f45-8be3-6e3658b397bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Data Profiling (Optional)\n",
    "\n",
    "try:\n",
    "    from ydata_profiling import ProfileReport\n",
    "    \n",
    "    print(\"\\nGenerating data profile report...\")\n",
    "    profile = ProfileReport(df_clean, title=\"ISBSG Dataset Profiling Report\", minimal=True)\n",
    "    profile.to_file(\"data_profile.html\")\n",
    "    print(\"Data profile report saved to 'data_profile.html'\")\n",
    "except ImportError:\n",
    "    print(\"\\nSkipping data profiling (ydata_profiling not installed)\")\n",
    "    print(\"To install: pip install ydata-profiling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f8b277-390c-461b-b520-198b60fb9ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa943057-abe3-4106-bd72-ac7c37aeb57f",
   "metadata": {},
   "source": [
    "[Back to top](#Index:)\n",
    "\n",
    "<a id='part5'></a>\n",
    "\n",
    "# Part 5 - PyCaret setup\n",
    "\n",
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96eb52-e00b-4448-9a60-69e35540abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.__version__)\n",
    "print(pycaret.__version__)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5bcf4b-f351-47c0-8d85-571e1b2c6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.regression import setup, get_config\n",
    "\n",
    "ignore_cols = ['isbsg_project_id', 'external_eef_data_quality_rating',  'project_prf_normalised_level_1_pdr_ufp', 'project_prf_normalised_pdr_ufp', \n",
    "               'project_prf_project_elapsed_time', 'people_prf_ba_team_experience_less_than_1_yr', 'people_prf_ba_team_experience_1_to_3_yr', \n",
    "               'people_prf_ba_team_experience_great_than_3_yr', 'people_prf_it_experience_less_than_1_yr', 'people_prf_it_experience_1_to_3_yr', \n",
    "               'people_prf_it_experience_great_than_3_yr', 'people_prf_it_experience_less_than_3_yr', 'people_prf_it_experience_3_to_9_yr', \n",
    "               'people_prf_it_experience_great_than_9_yr', 'people_prf_project_manage_experience', 'project_prf_total_project_cost', \n",
    "               'project_prf_cost_currency', 'project_prf_currency_multiple', 'project_prf_speed_of_delivery', 'people_prf_project_manage_changes', \n",
    "               'project_prf_defect_density','project_prf_manpower_delivery_rate'\n",
    "            ]\n",
    "setup_results = setup(\n",
    "    data=df_fixed,\n",
    "    target=target_col,\n",
    "    ignore_features=ignore_cols,\n",
    "    session_id=123,\n",
    "    preprocess=True,\n",
    "    # ...other options...\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Get the fitted pipeline from PyCaret\n",
    "preprocessor = get_config('preprocessor')\n",
    "# The scaler is usually inside the pipeline's steps\n",
    "for name, step in preprocessor.named_steps.items():\n",
    "    if 'scaler' in name:\n",
    "        scaler = step\n",
    "        break\n",
    "\n",
    "import pickle\n",
    "with open('../models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc67c7ad-b091-4cb6-8fd1-9a13c05fdac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "557bd817-37cb-49ff-81c2-9c9ca1884c7a",
   "metadata": {},
   "source": [
    "[Back to top](#Index:)\n",
    "\n",
    "<a id='part6'></a>\n",
    "\n",
    "# Part 6 - Feature Correlation Analysis\n",
    "\n",
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93647a03-0bf7-4fa5-8073-b17b72301c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation analysis\n",
    "print(\"\\nAnalyzing feature correlations...\")\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import os\n",
    "    from pycaret.regression import get_config\n",
    "\n",
    "    # Create directory for plots\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "    # Get data from PyCaret\n",
    "    X = get_config('X')\n",
    "\n",
    "    # Ensure we're working with numeric data only\n",
    "    X_numeric = X.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Drop rows with NaN or Inf values before correlation and VIF analysis\n",
    "    X_numeric_clean = X_numeric.replace([np.inf, -np.inf], np.nan).dropna(axis=0, how='any')\n",
    "\n",
    "    # Get number of features\n",
    "    n_features = X_numeric_clean.shape[1]\n",
    "    print(f\"Analyzing correlations among {n_features} numeric features\")\n",
    "\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = X_numeric_clean.corr()\n",
    "\n",
    "    # Determine features with high correlation\n",
    "    correlation_threshold = 0.7\n",
    "    high_corr_pairs = []\n",
    "\n",
    "    # Find highly correlated feature pairs\n",
    "    for i in range(n_features):\n",
    "        for j in range(i+1, n_features):\n",
    "            if abs(corr_matrix.iloc[i, j]) > correlation_threshold:\n",
    "                high_corr_pairs.append((\n",
    "                    X_numeric_clean.columns[i],\n",
    "                    X_numeric_clean.columns[j],\n",
    "                    corr_matrix.iloc[i, j]\n",
    "                ))\n",
    "\n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    mask = np.triu(corr_matrix)\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # If there are too many features, show only the ones with high correlation\n",
    "    if n_features > 20:\n",
    "        print(f\"Large number of features detected ({n_features}). Creating filtered correlation matrix.\")\n",
    "        # Get list of features with high correlation\n",
    "        high_corr_features = set()\n",
    "        for feat1, feat2, _ in high_corr_pairs:\n",
    "            high_corr_features.add(feat1)\n",
    "            high_corr_features.add(feat2)\n",
    "\n",
    "        # If there are high correlations, show only those features\n",
    "        if high_corr_features:\n",
    "            high_corr_features = list(high_corr_features)\n",
    "            filtered_corr = corr_matrix.loc[high_corr_features, high_corr_features]\n",
    "\n",
    "            # Plot filtered heatmap\n",
    "            sns.heatmap(filtered_corr, mask=np.triu(filtered_corr),\n",
    "                        cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "                        square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n",
    "                        annot=True, fmt=\".2f\")\n",
    "            plt.title('Correlation Heatmap (Filtered to Highly Correlated Features)')\n",
    "        else:\n",
    "            # No high correlations, show full matrix\n",
    "            sns.heatmap(corr_matrix, mask=mask,\n",
    "                        cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "                        square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "            plt.title('Correlation Heatmap (All Features)')\n",
    "    else:\n",
    "        # For smaller feature sets, show the full correlation matrix\n",
    "        sns.heatmap(corr_matrix, mask=mask,\n",
    "                    cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "                    square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n",
    "                    annot=True, fmt=\".2f\")\n",
    "        plt.title('Correlation Heatmap (All Features)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/correlation_heatmap.png')\n",
    "    plt.show()      # <-- Show in notebook\n",
    "    plt.close()\n",
    "    print(\"Correlation heatmap saved as plots/correlation_heatmap.png\")\n",
    "\n",
    "    # Calculate Variance Inflation Factor (VIF) if there are enough samples\n",
    "    vif_data = None\n",
    "    if X_numeric_clean.shape[0] > X_numeric_clean.shape[1]:\n",
    "        try:\n",
    "            from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "            # Calculate VIF for each feature\n",
    "            vif_data = pd.DataFrame()\n",
    "            vif_data[\"Feature\"] = X_numeric_clean.columns\n",
    "            vif_data[\"VIF\"] = [variance_inflation_factor(X_numeric_clean.values, i)\n",
    "                               for i in range(X_numeric_clean.shape[1])]\n",
    "\n",
    "            # Sort by VIF value\n",
    "            vif_data = vif_data.sort_values(\"VIF\", ascending=False)\n",
    "\n",
    "            # Plot VIF values\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.barh(vif_data[\"Feature\"], vif_data[\"VIF\"])\n",
    "            plt.axvline(x=5, color='r', linestyle='--', label='VIF=5 (Moderate multicollinearity)')\n",
    "            plt.axvline(x=10, color='darkred', linestyle='--', label='VIF=10 (High multicollinearity)')\n",
    "            plt.xlabel('VIF Value')\n",
    "            plt.title('Variance Inflation Factor (VIF) for Features')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('plots/vif_values.png')\n",
    "            plt.show()      # <-- Show in notebook\n",
    "            plt.close()\n",
    "            print(\"VIF values plot saved as plots/vif_values.png\")\n",
    "        except Exception as vif_err:\n",
    "            print(f\"Could not calculate VIF: {vif_err}\")\n",
    "    else:\n",
    "        print(\"Not enough samples to calculate VIF (need more samples than features)\")\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nFound {len(high_corr_pairs)} feature pairs with correlation > {correlation_threshold}:\")\n",
    "    for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "        print(f\"  • {feat1} and {feat2}: {corr:.4f}\")\n",
    "\n",
    "    # Print VIF results if available\n",
    "    if vif_data is not None:\n",
    "        high_vif_threshold = 10\n",
    "        high_vif_features = vif_data[vif_data[\"VIF\"] > high_vif_threshold]\n",
    "        if not high_vif_features.empty:\n",
    "            print(f\"\\nFeatures with high VIF (> {high_vif_threshold}):\")\n",
    "            for _, row in high_vif_features.iterrows():\n",
    "                print(f\"  • {row['Feature']}: {row['VIF']:.2f}\")\n",
    "        else:\n",
    "            print(f\"\\nNo features have VIF > {high_vif_threshold}\")\n",
    "\n",
    "    # Recommendations based on analysis\n",
    "    print(\"\\n--- Multicollinearity Analysis Recommendations ---\")\n",
    "    if high_corr_pairs:\n",
    "        print(\"Consider addressing multicollinearity by:\")\n",
    "        print(\"1. Removing one feature from each highly correlated pair\")\n",
    "        print(\"2. Creating new features by combining correlated features\")\n",
    "        print(\"3. Applying dimensionality reduction techniques like PCA\")\n",
    "\n",
    "        # Identify top candidates for removal\n",
    "        if len(high_corr_pairs) > 0:\n",
    "            print(\"\\nPotential candidates for removal:\")\n",
    "            # Count frequency of each feature in high correlation pairs\n",
    "            freq = {}\n",
    "            for feat1, feat2, _ in high_corr_pairs:\n",
    "                freq[feat1] = freq.get(feat1, 0) + 1\n",
    "                freq[feat2] = freq.get(feat2, 0) + 1\n",
    "\n",
    "            # Features that appear most frequently in high correlation pairs\n",
    "            freq_df = pd.DataFrame({'Feature': list(freq.keys()),\n",
    "                                    'Frequency in high corr pairs': list(freq.values())})\n",
    "            freq_df = freq_df.sort_values('Frequency in high corr pairs', ascending=False)\n",
    "\n",
    "            for _, row in freq_df.head(5).iterrows():\n",
    "                print(f\"  • {row['Feature']} (appears in {row['Frequency in high corr pairs']} high correlation pairs)\")\n",
    "    else:\n",
    "        print(\"No significant multicollinearity detected based on correlation analysis.\")\n",
    "\n",
    "    if vif_data is not None and not high_vif_features.empty:\n",
    "        print(\"\\nBased on VIF analysis, consider removing or transforming these features with high VIF values.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Feature correlation analysis failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2e34c-4940-4245-b843-74c6d418902b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeef1c6-749b-4100-a390-584707947b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcd0dca-7bc1-47eb-9f4e-e4c5dd4516c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65d552c1-4a33-494b-bdf9-37292f354793",
   "metadata": {},
   "source": [
    "[Back to top](#Index:)\n",
    "\n",
    "<a id='part7'></a>\n",
    "\n",
    "# Part 7 - Model Building with PyCaret\n",
    "\n",
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e57a53-9815-4b73-96e9-802cb2b257a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.regression import get_config, compare_models, pull, tune_model, evaluate_model, save_model\n",
    "\n",
    "import time\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Create output directories if needed\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "\n",
    "# Get preprocessed data for inspection and saving\n",
    "X = get_config(\"X\")\n",
    "y = get_config(\"y\")\n",
    "X.to_csv('data/pycaret_processed_features.csv', index=False)\n",
    "y.to_csv('data/pycaret_processed_target.csv', index=False)\n",
    "print(f\"\\nPreprocessed data shape: {X.shape}\")\n",
    "print(f\"Numeric features: {len(X.select_dtypes(include=[float, int]).columns)}\")\n",
    "print(f\"Categorical features: {len(X.select_dtypes(include=['object', 'category']).columns)}\")\n",
    "print(\"Preprocessed features and target saved.\")\n",
    "\n",
    "\n",
    "# 1. Compare and select top 3 models (returns list of models)\n",
    "print(\"\\nComparing regression models and selecting top 3...\")\n",
    "top_models = compare_models(n_select=3)\n",
    "model_results = pull()\n",
    "model_results.to_csv('logs/model_comparison_results.csv')\n",
    "print(\"\\nModel comparison results:\")\n",
    "print(model_results)\n",
    "\n",
    "# 2. For each top model: tune, evaluate, and save\n",
    "tuned_models = []\n",
    "scores = []\n",
    "\n",
    "for i, model in enumerate(top_models, 1):\n",
    "    model_name = type(model).__name__\n",
    "    print(f\"\\nModel {i}: {model_name}\")\n",
    "    \n",
    "    # Tune\n",
    "    print(\"  Tuning...\")\n",
    "    tuned = tune_model(model, n_iter=10)\n",
    "    tuned_models.append(tuned)\n",
    "\n",
    "    # Pull results after tuning - get the mean values\n",
    "    tuned_results = pull()\n",
    "    tuned_results.to_csv(f'logs/tuned_results_model_{i}_{model_name}.csv')\n",
    "    \n",
    "    # Extract metrics from \"Mean\" column instead of \"Value\"\n",
    "    try:\n",
    "        # First try to access by 'Mean' column which is the typical format\n",
    "        scores.append({\n",
    "            'Model': model_name, \n",
    "            'MAE': tuned_results.loc['MAE', 'Mean'],\n",
    "            'RMSE': tuned_results.loc['RMSE', 'Mean'],\n",
    "            'R2': tuned_results.loc['R2', 'Mean']\n",
    "        })\n",
    "    except KeyError:\n",
    "        # As a fallback, check the structure of tuned_results\n",
    "        print(f\"  Warning: Expected column structure not found in tuned results\")\n",
    "        print(f\"  tuned_results columns: {tuned_results.columns}\")\n",
    "        print(f\"  tuned_results index: {tuned_results.index}\")\n",
    "        \n",
    "        # Try alternative approaches based on the actual structure\n",
    "        if 'Mean' in tuned_results.columns:\n",
    "            scores.append({\n",
    "                'Model': model_name,\n",
    "                'MAE': tuned_results.loc['MAE', 'Mean'] if 'MAE' in tuned_results.index else None,\n",
    "                'RMSE': tuned_results.loc['RMSE', 'Mean'] if 'RMSE' in tuned_results.index else None,\n",
    "                'R2': tuned_results.loc['R2', 'Mean'] if 'R2' in tuned_results.index else None\n",
    "            })\n",
    "        elif len(tuned_results.columns) > 0:\n",
    "            # Get the last column as it might contain mean values\n",
    "            last_col = tuned_results.columns[-1]\n",
    "            scores.append({\n",
    "                'Model': model_name,\n",
    "                'MAE': tuned_results.loc['MAE', last_col] if 'MAE' in tuned_results.index else None,\n",
    "                'RMSE': tuned_results.loc['RMSE', last_col] if 'RMSE' in tuned_results.index else None,\n",
    "                'R2': tuned_results.loc['R2', last_col] if 'R2' in tuned_results.index else None\n",
    "            })\n",
    "        else:\n",
    "            # If we still can't find the right structure, log the issue\n",
    "            scores.append({\n",
    "                'Model': model_name,\n",
    "                'MAE': None,\n",
    "                'RMSE': None,\n",
    "                'R2': None\n",
    "            })\n",
    "            print(f\"  Unable to extract metrics for {model_name}. Check the saved CSV for details.\")\n",
    "    \n",
    "    # Save tuned model\n",
    "    save_model(tuned, f'models/top_model_{i}_{model_name}')\n",
    "    print(f\"  Saved as models/top_model_{i}_{model_name}.pkl\")\n",
    "    print(f\"  Time elapsed: {time.time() - start_time:.1f} seconds\")\n",
    "\n",
    "\n",
    "# Save overall summary of all tuned models\n",
    "score_df = pd.DataFrame(scores)\n",
    "score_df.to_csv('logs/tuned_model_scores.csv', index=False)\n",
    "print(\"\\nTuned models summary:\\n\", score_df)\n",
    "print(\"\\nAll top 3 models have been tuned, evaluated, and saved.\")\n",
    "print(\"\\nAnalysis complete! Proceed with feature importance or SHAP analysis as next steps.\")\n",
    "\n",
    "# 3. Optionally: Pull the best model for additional analysis (feature importance, SHAP, etc.)\n",
    "# You can access the best model as top_models[0] or reload any saved model later\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d4df8-8892-43e5-84e8-89b72ea5acd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c73f82a3-76ea-458d-8fd9-2fbc47e569b5",
   "metadata": {},
   "source": [
    "[Back to top](#Index:)\n",
    "\n",
    "<a id='part8'></a>\n",
    "\n",
    "# Part 8 - Feature Importance\n",
    "\n",
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d688115-661d-4765-add7-89fa6859f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only some models (e.g., tree-based models like Random Forest, XGBoost, LightGBM) have feature_importances_.\n",
    "# Many linear models (like LinearRegression, Lasso), KNN, and some ensemble models do not.\n",
    "\n",
    "# print(type(tuned_model))\n",
    "\n",
    "for i, m in enumerate(tuned_models, 1):\n",
    "    print(f\"Model {i} type: {type(m)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc1e0a-32d0-4292-bc9b-62488b797e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pandas as pd\n",
    "\n",
    "def plot_linear_feature_importance(model, X, y, feature_names=None, method='coefficients'):\n",
    "    \"\"\"\n",
    "    Plot feature importance for Linear models\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : trained linear model\n",
    "    X : feature matrix\n",
    "    y : target vector\n",
    "    feature_names : list of feature names (optional)\n",
    "    method : 'coefficients' or 'permutation'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create directory for plots if it doesn't exist\n",
    "    import os\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    \n",
    "    # Get feature names if not provided\n",
    "    if feature_names is None:\n",
    "        if hasattr(X, 'columns'):  # If X is a DataFrame\n",
    "            feature_names = X.columns.tolist()\n",
    "        else:\n",
    "            feature_names = [f'Feature {i}' for i in range(X.shape[1])]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if method == 'coefficients':\n",
    "        # Use absolute coefficient values as feature importance\n",
    "        importances = np.abs(model.coef_)\n",
    "        indices = np.argsort(importances)\n",
    "        \n",
    "        plt.title('Feature Importance Based on Coefficient Magnitude')\n",
    "        plt.barh(range(len(indices)), importances[indices], align='center')\n",
    "        plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "        plt.xlabel('Absolute Coefficient Magnitude')\n",
    "        \n",
    "    elif method == 'permutation':\n",
    "        # Calculate permutation importance\n",
    "        result = permutation_importance(\n",
    "            model, X, y, n_repeats=10, random_state=42, n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        importances = result.importances_mean\n",
    "        std = result.importances_std\n",
    "        indices = np.argsort(importances)\n",
    "        \n",
    "        plt.title('Feature Importance Based on Permutation Importance')\n",
    "        plt.barh(range(len(indices)), importances[indices], xerr=std[indices], align='center')\n",
    "        plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "        plt.xlabel('Permutation Importance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/linear_feature_importance_{method}.png')\n",
    "    print(f'Feature importance plot saved to plots/linear_feature_importance_{method}.png')\n",
    "    \n",
    "    # Return the importances for potential further analysis\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Example usage:\n",
    "# plot_linear_feature_importance(linear_model, X, y, feature_names=X.columns, method='coefficients')\n",
    "# plot_linear_feature_importance(linear_model, X, y, feature_names=X.columns, method='permutation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48af77a-e9bf-4d32-8185-f24c11e4f496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code modified for top 3 models\n",
    "\n",
    "from pycaret.regression import plot_model\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pycaret_X=X\n",
    "pycaret_y = y\n",
    "\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "feature_names = pycaret_X.columns.tolist()  # Make sure to use the same data as in training\n",
    "\n",
    "for i, tuned_model in enumerate(tuned_models, 1):\n",
    "    model_name = type(tuned_model).__name__\n",
    "    print(f\"\\nModel {i}: {model_name}\")\n",
    "\n",
    "    # First try PyCaret's plot_model\n",
    "    try:\n",
    "        plot_model(tuned_model, plot='feature', save=False)\n",
    "        plt.savefig(f'plots/feature_importance_model_{i}_{model_name}.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        print(f\"  PyCaret feature importance plot saved to plots/feature_importance_model_{i}_{model_name}.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"  PyCaret plot_model failed: {e}\")\n",
    "        # Fallback for linear models with coefficients\n",
    "        try:\n",
    "            # If it's a linear model (like HuberRegressor, LinearRegression, etc.)\n",
    "            if hasattr(tuned_model, 'coef_'):\n",
    "                importance_df = plot_linear_feature_importance(\n",
    "                    tuned_model, pycaret_X, pycaret_y, \n",
    "                    feature_names=feature_names, \n",
    "                    method='coefficients'\n",
    "                )\n",
    "                print(\"  Custom coefficient-based feature importance plot saved.\")\n",
    "                print(\"  Top 5 important features:\")\n",
    "                print(importance_df.head(5))\n",
    "            else:\n",
    "                print(\"  This model does not support .coef_ or is not a linear model.\")\n",
    "        except Exception as e2:\n",
    "            print(f\"  Could not generate feature plot for linear model: {e2}\")\n",
    "\n",
    "    # Optionally: also plot permutation-based feature importance for all linear models\n",
    "    if hasattr(tuned_model, 'coef_'):\n",
    "        print(\"\\n  Generating permutation-based feature importance plot...\")\n",
    "        try:\n",
    "            importance_df_perm = plot_linear_feature_importance(\n",
    "                tuned_model, pycaret_X, pycaret_y, \n",
    "                feature_names=feature_names, \n",
    "                method='permutation'\n",
    "            )\n",
    "            print(\"  Top 5 important features (permutation):\")\n",
    "            print(importance_df_perm.head(5))\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not generate permutation feature plot: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7890176-4a9d-4fb9-97e3-8f1df8830cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb56948-f198-489b-a77b-e6afb332a73f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e1e22a-1dc2-4fb3-ad85-178319eecd06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "257a024c-f292-45d6-974a-e1f9aef94446",
   "metadata": {},
   "source": [
    "[Back to top](#Index:)\n",
    "\n",
    "<a id='part9'></a>\n",
    "\n",
    "# Part 9 - SHAP Analysis\n",
    "\n",
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54da7e7-cd7f-46c6-956d-95752ad6c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if required packages are installed and install if needed\n",
    "def check_and_install_packages():\n",
    "    try:\n",
    "        import shap\n",
    "        from pycaret.regression import get_config\n",
    "        print(\"All required packages are installed.\")\n",
    "        return True\n",
    "    except ImportError as e:\n",
    "        missing_package = str(e).split(\"'\")[1]\n",
    "        print(f\"Missing package: {missing_package}\")\n",
    "        install = input(f\"Would you like to install {missing_package}? (y/n): \")\n",
    "        if install.lower() == 'y':\n",
    "            import sys\n",
    "            import subprocess\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", missing_package])\n",
    "            print(f\"{missing_package} installed successfully.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"⚠️ Please install {missing_package} to proceed.\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673eb80-ee13-4e67-9d7a-a36649d8d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP analysis with proper data type handling and debugging\n",
    "\"\"\"\n",
    "Global Perspective:\n",
    "- Summary Plot: Provides a global overview of feature importance and their \n",
    "  positive or negative impact on the model output across the entire dataset.\n",
    "- Dependence Plot: Illustrates the relationship between a single feature's \n",
    "  value and its SHAP value across all instances to understand its general \n",
    "  effect on the prediction.\n",
    "- Bar Chart: Shows the global importance of each feature based on the average \n",
    "  magnitude of their SHAP values across the entire dataset.\n",
    "\n",
    "Single Instance Perspective:\n",
    "- Force Plot: Explains the prediction for a single instance by showing how each \n",
    "  feature contributes to moving the prediction from the base value for that specific case.\n",
    "- Waterfall Plot: Explains the prediction for a single instance by visualizing the sequential, \n",
    "  additive contribution of each feature's SHAP value for that specific prediction.\n",
    "\"\"\"\n",
    "\n",
    "def run_shap_analysis(tuned_model, model_index=1, debug=True):\n",
    "    \"\"\"\n",
    "    Run SHAP analysis on a tuned model with enhanced debugging and all major SHAP plots.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"SHAP Analysis for Model {model_index}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    try:\n",
    "        import shap\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        import os\n",
    "\n",
    "        os.makedirs('plots', exist_ok=True)\n",
    "        if debug:\n",
    "            print(f\"Output directory created: {os.path.abspath('plots')}\")\n",
    "\n",
    "        try:\n",
    "            X_transformed = get_config('X_transformed')\n",
    "            if debug:\n",
    "                print(f\"Successfully retrieved transformed data\")\n",
    "                print(f\"   Shape: {X_transformed.shape}\")\n",
    "                print(f\"   Data types: {X_transformed.dtypes.value_counts()}\")\n",
    "                print(f\"   Contains NaN: {np.isnan(X_transformed).any()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error getting transformed data: {e}\")\n",
    "            print(\"   Trying alternative approach...\")\n",
    "            try:\n",
    "                X = get_config('X')\n",
    "                X_transformed = X\n",
    "                print(f\"Using raw features instead. Shape: {X_transformed.shape}\")\n",
    "            except Exception as e2:\n",
    "                print(f\"❌ Error getting raw data: {e2}\")\n",
    "                raise ValueError(\"Could not access training data\")\n",
    "        \n",
    "        model_name = type(tuned_model).__name__\n",
    "        if debug:\n",
    "            print(f\"Model identified as: {model_name}\")\n",
    "\n",
    "        # Convert data to float64 for SHAP\n",
    "        try:\n",
    "            X_transformed_float = X_transformed.astype(np.float64)\n",
    "            if debug:\n",
    "                print(\"Data successfully converted to float64 type\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error converting data types: {e}\")\n",
    "            try:\n",
    "                X_transformed_float = X_transformed.copy()\n",
    "                for col in X_transformed.columns:\n",
    "                    X_transformed_float[col] = pd.to_numeric(X_transformed[col], errors='coerce')\n",
    "                X_transformed_float = X_transformed_float.fillna(0)\n",
    "                print(\"Data converted using alternative method\")\n",
    "            except:\n",
    "                print(\"❌ Could not convert data. Proceeding with original data.\")\n",
    "                X_transformed_float = X_transformed\n",
    "\n",
    "        try:\n",
    "            feature_names = get_config('X').columns.tolist()\n",
    "            if debug:\n",
    "                print(f\"Retrieved {len(feature_names)} feature names\")\n",
    "        except:\n",
    "            feature_names = [f\"Feature_{i}\" for i in range(X_transformed.shape[1])]\n",
    "            print(f\"⚠️ Could not get original feature names, using generic names\")\n",
    "\n",
    "        model_type = str(type(tuned_model)).lower()\n",
    "        if debug:\n",
    "            print(f\"📊 Model type details: {model_type}\")\n",
    "\n",
    "        # Sample for efficiency\n",
    "        sample_size = min(100, X_transformed_float.shape[0])\n",
    "        sample_indices = np.random.choice(X_transformed_float.shape[0], sample_size, replace=False)\n",
    "        X_sample = X_transformed_float.iloc[sample_indices] if hasattr(X_transformed_float, 'iloc') else X_transformed_float[sample_indices]\n",
    "\n",
    "        # Choose SHAP explainer\n",
    "        if any(x in model_type for x in ['tree', 'forest', 'xgboost', 'lgbm', 'catboost', 'gradientboosting']):\n",
    "            print(\"🌲 Using TreeExplainer for tree-based model\")\n",
    "            explainer = shap.TreeExplainer(tuned_model)\n",
    "            shap_values = explainer(X_sample)\n",
    "        elif any(x in model_type for x in ['linear', 'logistic', 'ridge', 'lasso', 'huber']):\n",
    "            print(\"📏 Using LinearExplainer for linear model\")\n",
    "            explainer = shap.LinearExplainer(tuned_model, X_sample)\n",
    "            shap_values = explainer(X_sample)\n",
    "        else:\n",
    "            print(\"🔄 Using KernelExplainer as fallback (may be slow)\")\n",
    "            def model_predict(X):\n",
    "                return tuned_model.predict(X)\n",
    "            explainer = shap.KernelExplainer(model_predict, X_sample)\n",
    "            shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "        # Prepare shap_array for plotting\n",
    "        shap_array = shap_values.values if hasattr(shap_values, \"values\") else shap_values\n",
    "\n",
    "        print(f\"shap_array shape: {getattr(shap_array, 'shape', 'N/A')}, X_sample shape: {X_sample.shape}\")\n",
    "\n",
    "        # ========== GLOBAL PERSPECTIVE ==========\n",
    "        # 1. Summary Plot\n",
    "        try:\n",
    "            if isinstance(shap_array, np.ndarray) and shap_array.shape[1] == X_sample.shape[1]:\n",
    "                plt.figure(figsize=(12, 10))\n",
    "                shap.summary_plot(shap_array, X_sample, feature_names=feature_names, show=False)\n",
    "                plt.tight_layout()\n",
    "                summary_path = f'plots/shap_summary_model{model_index}_{model_name}.png'\n",
    "                plt.savefig(summary_path)\n",
    "                plt.show()  # This ensures it's visible inline\n",
    "                plt.close()\n",
    "                print(f\"✅ SHAP summary plot saved to {os.path.abspath(summary_path)}\")\n",
    "            else:\n",
    "                print(f\"❌ SHAP values shape {shap_array.shape} does not match sample features {X_sample.shape}\")\n",
    "                print(\"Skipping summary plot.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error creating summary plot: {e}\")\n",
    "\n",
    "        # 2. Dependence Plot (top global feature)\n",
    "        try:\n",
    "            # Top global feature by mean absolute SHAP value\n",
    "            if isinstance(shap_array, np.ndarray) and shap_array.shape[1] == X_sample.shape[1]:\n",
    "                top_idx = np.argsort(np.abs(shap_array).mean(0))[-1]\n",
    "                top_feat = feature_names[top_idx]\n",
    "                plt.figure(figsize=(10, 7))\n",
    "                shap.dependence_plot(top_feat, shap_array, X_sample, feature_names=feature_names, show=False)\n",
    "                dep_path = f'plots/shap_dependence_{top_feat}_model{model_index}_{model_name}.png'\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(dep_path)\n",
    "                plt.show()  # This ensures it's visible inline\n",
    "                plt.close()\n",
    "                print(f\"✅ SHAP dependence plot for '{top_feat}' saved to {os.path.abspath(dep_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error creating dependence plot: {e}\")\n",
    "\n",
    "        # 3. Bar Chart (mean absolute SHAP values)\n",
    "        try:\n",
    "            if isinstance(shap_array, np.ndarray) and shap_array.shape[1] == X_sample.shape[1]:\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                shap.summary_plot(shap_array, X_sample, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
    "                plt.tight_layout()\n",
    "                bar_path = f'plots/shap_importance_bar_model{model_index}_{model_name}.png'\n",
    "                plt.savefig(bar_path)\n",
    "                plt.show()  # This ensures it's visible inline\n",
    "                plt.close()\n",
    "                print(f\"✅ SHAP feature importance bar plot saved to {os.path.abspath(bar_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error creating bar plot: {e}\")\n",
    "\n",
    "        # ========== SINGLE INSTANCE PERSPECTIVE ==========\n",
    "        # Choose a single instance (first row of sample)\n",
    "        instance = X_sample.iloc[0] if hasattr(X_sample, \"iloc\") else X_sample[0]\n",
    "        shap_value_instance = shap_array[0] if isinstance(shap_array, np.ndarray) else shap_array[0]\n",
    "\n",
    "        # 4. Force Plot\n",
    "        try:\n",
    "            force_path = f'plots/shap_force_model{model_index}_{model_name}.html'\n",
    "            shap.initjs()\n",
    "            force_plot = shap.force_plot(explainer.expected_value, shap_value_instance, instance, feature_names=feature_names, show=False, matplotlib=False)\n",
    "            shap.save_html(force_path, force_plot)\n",
    "            print(f\"✅ SHAP force plot saved to {os.path.abspath(force_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Could not create force plot: {e}\")\n",
    "\n",
    "        # 5. Waterfall Plot\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            shap.plots.waterfall(shap_values[0], show=False)\n",
    "            plt.tight_layout()\n",
    "            waterfall_path = f'plots/shap_waterfall_model{model_index}_{model_name}.png'\n",
    "            plt.savefig(waterfall_path)\n",
    "            plt.show()  # This ensures it's visible inline\n",
    "            plt.close()\n",
    "            print(f\"✅ SHAP waterfall plot saved to {os.path.abspath(waterfall_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Could not create waterfall plot: {e}\")\n",
    "\n",
    "        print(\"\\n✅ SHAP analysis completed successfully!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ SHAP analysis failed: {e}\")\n",
    "        print(\"\\n⚠️ Trying PyCaret's built-in SHAP plot as fallback...\")\n",
    "        try:\n",
    "            from pycaret.regression import plot_model\n",
    "            plot_model(tuned_model, plot='shap', save=True)\n",
    "            print(\"✅ SHAP plot created using PyCaret's built-in functionality\")\n",
    "            return True\n",
    "        except Exception as e2:\n",
    "            print(f\"❌ PyCaret's SHAP plot also failed: {e2}\")\n",
    "            print(\"\\n💡 Recommendations:\")\n",
    "            print(\"   1. Check if your model is compatible with SHAP\")\n",
    "            print(\"   2. Try running just the model importance plot: plot_model(tuned_model, plot='feature')\")\n",
    "            print(\"   3. Make sure your model is properly trained and accessible\")\n",
    "            print(\"   4. Verify that you have the latest versions of SHAP and matplotlib\")\n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "# Use this to check if your plots directory exists and what's in it\n",
    "def check_outputs():\n",
    "    print(\"\\nChecking for output files:\")\n",
    "    try:\n",
    "        if os.path.exists('plots'):\n",
    "            files = os.listdir('plots')\n",
    "            if files:\n",
    "                print(f\"✅ Found {len(files)} files in plots directory:\")\n",
    "                for file in files:\n",
    "                    print(f\"   - {file}\")\n",
    "            else:\n",
    "                print(\"⚠️ 'plots' directory exists but is empty\")\n",
    "        else:\n",
    "            print(\"❌ 'plots' directory does not exist\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking outputs: {e}\")\n",
    "\n",
    "# First check if required packages are installed\n",
    "if check_and_install_packages():\n",
    "    # If you have a trained model from PyCaret, use it like this:\n",
    "    from pycaret.regression import load_model, setup, get_config\n",
    "    \n",
    "    # Load your dataset and set up PyCaret (replace with your actual code)\n",
    "    # df = pd.read_csv('your_dataset.csv')\n",
    "    # setup(df, target='your_target_column')\n",
    "    \n",
    "    # Either load an existing model or use your already trained model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If you have a saved model:\n",
    "        #model = load_model('your_saved_model_path')\n",
    "        print(\"Model loaded successfully!\")\n",
    "    except:\n",
    "        print(\"Please replace 'your_saved_model_path' with your actual model path\")\n",
    "        print(\"Or use your existing trained model variable\")\n",
    "        # model = your_existing_trained_model_variable\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call the SHAP analysis function with your model\n",
    "    # run_shap_analysis(model, model_index=1, debug=True)\n",
    "\n",
    "# -------- RUN SHAP FOR ALL TUNED MODELS --------\n",
    "for idx, tuned_model in enumerate(tuned_models, 1):\n",
    "    run_shap_analysis(tuned_model, model_index=idx, debug=True)\n",
    "\n",
    "# Optionally, check plot outputs\n",
    "check_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f3e0b-a54b-45de-a781-13f827a28485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1849adbd-faec-4101-a227-794e17c2e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance directly (if available)\n",
    "\n",
    "from pycaret.regression import get_config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.makedirs('data', exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "for idx, tuned_model in enumerate(tuned_models, 1):\n",
    "    model_name = type(tuned_model).__name__\n",
    "    print(f\"\\nModel {idx}: {model_name}\")\n",
    "\n",
    "    try:\n",
    "        feature_names = get_config('X_transformed').columns\n",
    "\n",
    "        if hasattr(tuned_model, 'feature_importances_'):\n",
    "            importances = tuned_model.feature_importances_\n",
    "            importance_label = 'feature_importances_'\n",
    "        elif hasattr(tuned_model, 'coef_'):\n",
    "            importances = np.abs(tuned_model.coef_)\n",
    "            importance_label = 'coef_ (abs)'\n",
    "        else:\n",
    "            print(\"Feature importance attribute not available for this model.\")\n",
    "            continue\n",
    "\n",
    "        # Ensure lengths match\n",
    "        if len(feature_names) != len(importances):\n",
    "            print(f\"Warning: Length mismatch - {len(feature_names)} features vs {len(importances)} importance values\")\n",
    "            min_length = min(len(feature_names), len(importances))\n",
    "            feature_names = feature_names[:min_length]\n",
    "            importances = importances[:min_length]\n",
    "\n",
    "        fi = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        })\n",
    "        fi = fi.sort_values('importance', ascending=False)\n",
    "        print(fi.head(15))  # Show top 15 features\n",
    "\n",
    "        # Save to CSV with model index and name\n",
    "        out_path = f\"data/feature_importance_model_{idx}_{model_name}.csv\"\n",
    "        fi.to_csv(out_path, index=False)\n",
    "        print(f\"Feature importance ({importance_label}) saved to '{out_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract feature importance for {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53d1498-04ea-495a-ae5b-db0b584602a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158d4cfd-2f3e-4add-92bd-067944e304c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a380b-4c86-4ac0-a42b-61d1098a27f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3344fbce-ee3b-456d-81c2-ef525011e3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcce337c-4e56-4e0a-b179-19c96d7066e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc162330-9be2-49f3-9022-2b6445b5d29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355208f9-e9d7-49fe-8de2-4f73bc138e28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
