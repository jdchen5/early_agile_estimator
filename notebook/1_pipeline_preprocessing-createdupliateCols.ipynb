{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f790fa4-3694-4fc7-b8d0-b3a480e05053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df43052d-4e2f-497a-9f7d-7884f05342dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComplete Scikit-Learn Preprocessing Pipeline for ISBSG Data\\n===========================================================\\n\\nThis module provides a comprehensive preprocessing pipeline that handles:\\n1. Data loading and initial cleaning\\n2. Column name standardization\\n3. Missing value handling\\n4. Semicolon-separated value processing\\n5. One-hot encoding for categorical variables\\n6. Multi-label binarization for multi-value columns\\n7. Feature selection and filtering\\n8. Data validation and export\\n\\nBased on the preprocessing steps from the provided notebooks.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Scikit-Learn Preprocessing Pipeline for ISBSG Data\n",
    "===========================================================\n",
    "\n",
    "This module provides a comprehensive preprocessing pipeline that handles:\n",
    "1. Data loading and initial cleaning\n",
    "2. Column name standardization\n",
    "3. Missing value handling\n",
    "4. Semicolon-separated value processing\n",
    "5. One-hot encoding for categorical variables\n",
    "6. Multi-label binarization for multi-value columns\n",
    "7. Feature selection and filtering\n",
    "8. Data validation and export\n",
    "\n",
    "Based on the preprocessing steps from the provided notebooks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29e26088-9829-45c3-ad0e-f34c146cf120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb3dd6fc-4a21-4b17-b2cc-9e1d160bfab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp printing activated.\n",
      "Cell executed at: 2025-06-01 20:57:56.920724\n"
     ]
    }
   ],
   "source": [
    "# Sets up an automatic timestamp printout after each Jupyter cell execution \n",
    "# and configures the default visualization style.\n",
    "from IPython import get_ipython\n",
    "\n",
    "def setup_timestamp_callback():\n",
    "    \"\"\"Setup a timestamp callback for Jupyter cells without clearing existing callbacks.\"\"\"\n",
    "    ip = get_ipython()\n",
    "    if ip is not None:\n",
    "        # Define timestamp function\n",
    "        def print_timestamp(*args, **kwargs):\n",
    "            \"\"\"Print timestamp after cell execution.\"\"\"\n",
    "            print(f\"Cell executed at: {datetime.now()}\")\n",
    "        \n",
    "        # Check if our callback is already registered\n",
    "        callbacks = ip.events.callbacks.get('post_run_cell', [])\n",
    "        for cb in callbacks:\n",
    "            if hasattr(cb, '__name__') and cb.__name__ == 'print_timestamp':\n",
    "                # Already registered\n",
    "                return\n",
    "                \n",
    "        # Register new callback if not already present\n",
    "        ip.events.register('post_run_cell', print_timestamp)\n",
    "        print(\"Timestamp printing activated.\")\n",
    "    else:\n",
    "        print(\"Not running in IPython/Jupyter environment.\")\n",
    "\n",
    "# Setup timestamp callback\n",
    "setup_timestamp_callback()\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e705a6-19d4-488a-a6cc-751224ebcccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-06-01 20:57:56.936520\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_FOLDER = \"../data\"\n",
    "SAMPLE_FILE = \"ISBSG2016R1_1_agile_dataset_only.xlsx\"\n",
    "FULL_FILE = \"ISBSG2016R1_1_full_dataset.xlsx\"\n",
    "TARGET_COL = \"project_prf_normalised_work_effort\"  # be careful about case sensitive\n",
    "EXCLUDE_FROM_CATEGORY_AUGMENT = ['project_prf_year_of_project']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c68d9a8a-470a-4f4f-8c3d-1a249d4400af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-06-01 20:57:56.944039\n"
     ]
    }
   ],
   "source": [
    "def convert_object_numeric_cols(df, threshold=0.8):\n",
    "    \"\"\"Convert object columns to numeric if most values can be coerced to numbers.\"\"\"\n",
    "    object_cols = df.select_dtypes(include='object').columns\n",
    "    for col in object_cols:\n",
    "        coerced = pd.to_numeric(df[col], errors='coerce')\n",
    "        if coerced.notnull().mean() > threshold:\n",
    "            df[col] = coerced\n",
    "            print(f\"Column '{col}' auto-converted to numeric (ratio={coerced.notnull().mean():.2f})\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "282c0614-af90-46dd-b1aa-408efab3ab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-06-01 20:57:57.105324\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def analyze_high_cardinality_multivalue(df, column, separator=';'):\n",
    "    \"\"\"\n",
    "    Analyze high-cardinality multi-value columns to choose best strategy\n",
    "    \"\"\"\n",
    "    print(f\"=== ANALYSIS FOR HIGH-CARDINALITY COLUMN: '{column}' ===\\n\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    non_null_data = df[column].dropna().astype(str)\n",
    "    split_values = non_null_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Get all unique values\n",
    "    all_values = []\n",
    "    for values_list in split_values:\n",
    "        all_values.extend(values_list)\n",
    "    \n",
    "    value_counts = Counter(all_values)\n",
    "    unique_values = list(value_counts.keys())\n",
    "    \n",
    "    print(f\"Total unique values: {len(unique_values)}\")\n",
    "    print(f\"Total value occurrences: {len(all_values)}\")\n",
    "    print(f\"Average values per row: {len(all_values) / len(split_values):.2f}\")\n",
    "    \n",
    "    # Show most common values\n",
    "    print(f\"\\nTop 15 most common values:\")\n",
    "    for value, count in value_counts.most_common(15):\n",
    "        percentage = (count / len(non_null_data)) * 100\n",
    "        print(f\"  '{value}': {count} times ({percentage:.1f}% of rows)\")\n",
    "    \n",
    "    # Show distribution of value frequencies\n",
    "    frequency_dist = Counter(value_counts.values())\n",
    "    print(f\"\\nFrequency distribution:\")\n",
    "    for freq, count in sorted(frequency_dist.items(), reverse=True)[:10]:\n",
    "        print(f\"  {count} values appear {freq} time(s)\")\n",
    "    \n",
    "    # Values per row distribution\n",
    "    values_per_row = split_values.apply(len)\n",
    "    print(f\"\\nValues per row:\")\n",
    "    print(f\"  Min: {values_per_row.min()}\")\n",
    "    print(f\"  Max: {values_per_row.max()}\")\n",
    "    print(f\"  Mean: {values_per_row.mean():.2f}\")\n",
    "    print(f\"  Median: {values_per_row.median():.2f}\")\n",
    "    \n",
    "    return value_counts, unique_values\n",
    "\n",
    "\n",
    "def handle_high_cardinality_multivalue(df, multi_value_columns, separator=';', strategy='top_k', **kwargs):\n",
    "    \"\"\"\n",
    "    Handle high-cardinality multi-value columns with various strategies\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    strategy options:\n",
    "    - 'top_k': Keep only top K most frequent values (k=kwargs['k'])\n",
    "    - 'frequency_threshold': Keep values that appear in at least X% of rows (threshold=kwargs['threshold'])\n",
    "    - 'tfidf': Use TF-IDF vectorization with dimensionality reduction (n_components=kwargs['n_components'])\n",
    "    - 'count_features': Simple counting features (count, unique_count, most_common)\n",
    "    - 'embedding': Create category embeddings (requires pre-trained embeddings)\n",
    "    \"\"\"\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    new_columns_mapping = {}\n",
    "    \n",
    "    for col in multi_value_columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing high-cardinality column '{col}' with strategy '{strategy}'...\")\n",
    "        \n",
    "        # Clean and split values\n",
    "        split_values = df[col].fillna('').astype(str).apply(\n",
    "            lambda x: [val.strip() for val in x.split(separator) if val.strip()]\n",
    "        )\n",
    "        \n",
    "        # Get value counts\n",
    "        all_values = []\n",
    "        for values_list in split_values:\n",
    "            all_values.extend(values_list)\n",
    "        value_counts = Counter(all_values)\n",
    "        \n",
    "        if strategy == 'top_k':\n",
    "            k = kwargs.get('k', 20)  # Default to top 20\n",
    "            top_values = [val for val, count in value_counts.most_common(k)]\n",
    "            \n",
    "            new_col_names = []\n",
    "            for value in top_values:\n",
    "                new_col_name = f\"{col}_top_{value}\".replace(' ', '_').replace('-', '_')\n",
    "                df_processed[new_col_name] = split_values.apply(lambda x: 1 if value in x else 0)\n",
    "                new_col_names.append(new_col_name)\n",
    "            \n",
    "            # Add \"other\" category for remaining values\n",
    "            other_col_name = f\"{col}_other\"\n",
    "            df_processed[other_col_name] = split_values.apply(\n",
    "                lambda x: 1 if any(val not in top_values for val in x) else 0\n",
    "            )\n",
    "            new_col_names.append(other_col_name)\n",
    "            \n",
    "            new_columns_mapping[col] = new_col_names\n",
    "            print(f\"  Created {len(new_col_names)} columns (top {k} + other)\")\n",
    "            \n",
    "        elif strategy == 'frequency_threshold':\n",
    "            threshold = kwargs.get('threshold', 0.05)  # Default 5%\n",
    "            min_occurrences = int(len(df) * threshold)\n",
    "            \n",
    "            frequent_values = [val for val, count in value_counts.items() if count >= min_occurrences]\n",
    "            \n",
    "            new_col_names = []\n",
    "            for value in frequent_values:\n",
    "                new_col_name = f\"{col}_freq_{value}\".replace(' ', '_').replace('-', '_')\n",
    "                df_processed[new_col_name] = split_values.apply(lambda x: 1 if value in x else 0)\n",
    "                new_col_names.append(new_col_name)\n",
    "            \n",
    "            # Add rare category\n",
    "            rare_col_name = f\"{col}_rare\"\n",
    "            df_processed[rare_col_name] = split_values.apply(\n",
    "                lambda x: 1 if any(val not in frequent_values for val in x) else 0\n",
    "            )\n",
    "            new_col_names.append(rare_col_name)\n",
    "            \n",
    "            new_columns_mapping[col] = new_col_names\n",
    "            print(f\"  Created {len(new_col_names)} columns ({len(frequent_values)} frequent + rare)\")\n",
    "            \n",
    "        elif strategy == 'count_features':\n",
    "            # Create aggregate features instead of individual columns\n",
    "            new_col_names = []\n",
    "            \n",
    "            # Total count of values\n",
    "            count_col = f\"{col}_count\"\n",
    "            df_processed[count_col] = split_values.apply(len)\n",
    "            new_col_names.append(count_col)\n",
    "            \n",
    "            # Unique count (in case of duplicates)\n",
    "            unique_count_col = f\"{col}_unique_count\"\n",
    "            df_processed[unique_count_col] = split_values.apply(lambda x: len(set(x)))\n",
    "            new_col_names.append(unique_count_col)\n",
    "            \n",
    "            # Most common value in the dataset appears in this row\n",
    "            most_common_value = value_counts.most_common(1)[0][0] if value_counts else None\n",
    "            if most_common_value:\n",
    "                most_common_col = f\"{col}_has_most_common\"\n",
    "                df_processed[most_common_col] = split_values.apply(lambda x: 1 if most_common_value in x else 0)\n",
    "                new_col_names.append(most_common_col)\n",
    "            \n",
    "            # Average frequency of values in this row\n",
    "            avg_freq_col = f\"{col}_avg_frequency\"\n",
    "            df_processed[avg_freq_col] = split_values.apply(\n",
    "                lambda x: np.mean([value_counts[val] for val in x]) if x else 0\n",
    "            )\n",
    "            new_col_names.append(avg_freq_col)\n",
    "            \n",
    "            new_columns_mapping[col] = new_col_names\n",
    "            print(f\"  Created {len(new_col_names)} aggregate feature columns\")\n",
    "            \n",
    "        elif strategy == 'tfidf':\n",
    "            n_components = kwargs.get('n_components', 10)  # Default to 10 components\n",
    "            \n",
    "            # Convert to text format for TF-IDF\n",
    "            text_data = split_values.apply(lambda x: ' '.join(x))\n",
    "            \n",
    "            # Apply TF-IDF\n",
    "            tfidf = TfidfVectorizer(max_features=100, stop_words=None)\n",
    "            tfidf_matrix = tfidf.fit_transform(text_data)\n",
    "            \n",
    "            # Reduce dimensionality\n",
    "            pca = PCA(n_components=n_components)\n",
    "            tfidf_reduced = pca.fit_transform(tfidf_matrix.toarray())\n",
    "            \n",
    "            # Create new columns\n",
    "            new_col_names = []\n",
    "            for i in range(n_components):\n",
    "                new_col_name = f\"{col}_tfidf_comp_{i+1}\"\n",
    "                df_processed[new_col_name] = tfidf_reduced[:, i]\n",
    "                new_col_names.append(new_col_name)\n",
    "            \n",
    "            new_columns_mapping[col] = new_col_names\n",
    "            print(f\"  Created {len(new_col_names)} TF-IDF component columns\")\n",
    "            print(f\"  Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "            \n",
    "        elif strategy == 'hierarchical':\n",
    "            # Group similar values into higher-level categories\n",
    "            # This requires domain knowledge - example implementation\n",
    "            hierarchy = kwargs.get('hierarchy', {})  # Dictionary mapping values to categories\n",
    "            \n",
    "            if not hierarchy:\n",
    "                print(\"  Warning: No hierarchy provided for hierarchical strategy\")\n",
    "                continue\n",
    "            \n",
    "            # Create columns for each high-level category\n",
    "            categories = set(hierarchy.values())\n",
    "            new_col_names = []\n",
    "            \n",
    "            for category in categories:\n",
    "                category_values = [val for val, cat in hierarchy.items() if cat == category]\n",
    "                new_col_name = f\"{col}_category_{category}\".replace(' ', '_')\n",
    "                df_processed[new_col_name] = split_values.apply(\n",
    "                    lambda x: 1 if any(val in category_values for val in x) else 0\n",
    "                )\n",
    "                new_col_names.append(new_col_name)\n",
    "            \n",
    "            new_columns_mapping[col] = new_col_names\n",
    "            print(f\"  Created {len(new_col_names)} hierarchical category columns\")\n",
    "        \n",
    "        # Remove original column\n",
    "        df_processed = df_processed.drop(columns=[col])\n",
    "    \n",
    "    return df_processed, new_columns_mapping\n",
    "\n",
    "\n",
    "def recommend_strategy(df, column, separator=';'):\n",
    "    \"\"\"\n",
    "    Recommend the best strategy based on data characteristics\n",
    "    \"\"\"\n",
    "    value_counts, unique_values = analyze_high_cardinality_multivalue(df, column, separator)\n",
    "    \n",
    "    total_unique = len(unique_values)\n",
    "    total_rows = len(df[column].dropna())\n",
    "    \n",
    "    print(f\"\\n=== STRATEGY RECOMMENDATIONS FOR '{column}' ===\")\n",
    "    \n",
    "    if total_unique > 100:\n",
    "        print(\"🔴 VERY HIGH CARDINALITY (100+ unique values)\")\n",
    "        print(\"Recommended strategies:\")\n",
    "        print(\"1. 'count_features' - Create aggregate features (safest)\")\n",
    "        print(\"2. 'top_k' with k=15-25 - Keep only most important values\")\n",
    "        print(\"3. 'tfidf' with n_components=5-10 - If values have semantic meaning\")\n",
    "        \n",
    "    elif total_unique > 50:\n",
    "        print(\"🟡 HIGH CARDINALITY (50+ unique values)\")\n",
    "        print(\"Recommended strategies:\")\n",
    "        print(\"1. 'top_k' with k=20-30 - Keep most frequent values\")\n",
    "        print(\"2. 'frequency_threshold' with threshold=0.02-0.05\")\n",
    "        print(\"3. 'count_features' - If you want aggregate information\")\n",
    "        \n",
    "    else:\n",
    "        print(\"🟢 MODERATE CARDINALITY (<50 unique values)\")\n",
    "        print(\"Recommended strategies:\")\n",
    "        print(\"1. 'frequency_threshold' with threshold=0.01\")\n",
    "        print(\"2. 'top_k' with k=30-40\")\n",
    "        print(\"3. Binary encoding might be acceptable\")\n",
    "    \n",
    "    # Check frequency distribution\n",
    "    freq_values = list(value_counts.values())\n",
    "    if max(freq_values) / min(freq_values) > 100:\n",
    "        print(\"\\n⚠️  HIGHLY SKEWED DISTRIBUTION detected\")\n",
    "        print(\"   Consider 'frequency_threshold' or 'top_k' strategies\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3fcd3df-7d6b-431d-941f-e775698d7224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-06-01 20:57:57.166568\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def validate_multivalue_processing(df_original, df_processed, original_column, new_columns, separator=';', strategy='top_k'):\n",
    "    \"\"\"\n",
    "    Comprehensive validation of multi-value categorical processing\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_original : pd.DataFrame\n",
    "        Original dataset before processing\n",
    "    df_processed : pd.DataFrame  \n",
    "        Processed dataset after handling multi-value columns\n",
    "    original_column : str\n",
    "        Name of original multi-value column\n",
    "    new_columns : list\n",
    "        List of new column names created from the original column\n",
    "    separator : str\n",
    "        Separator used in original data\n",
    "    strategy : str\n",
    "        Strategy used for processing\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"=== VALIDATION REPORT FOR COLUMN '{original_column}' ===\\n\")\n",
    "    \n",
    "    # 1. BASIC CHECKS\n",
    "    print(\"1. BASIC INTEGRITY CHECKS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check row count consistency\n",
    "    original_rows = len(df_original)\n",
    "    processed_rows = len(df_processed)\n",
    "    print(f\"✓ Row count: {original_rows} → {processed_rows} {'✓ SAME' if original_rows == processed_rows else '⚠️  DIFFERENT'}\")\n",
    "    \n",
    "    # Check if original column was removed\n",
    "    original_removed = original_column not in df_processed.columns\n",
    "    print(f\"✓ Original column removed: {'✓ YES' if original_removed else '⚠️  NO'}\")\n",
    "    \n",
    "    # Check if new columns exist\n",
    "    new_cols_exist = all(col in df_processed.columns for col in new_columns)\n",
    "    print(f\"✓ New columns created: {'✓ YES' if new_cols_exist else '❌ NO'} ({len(new_columns)} columns)\")\n",
    "    \n",
    "    if not new_cols_exist:\n",
    "        missing_cols = [col for col in new_columns if col not in df_processed.columns]\n",
    "        print(f\"  Missing columns: {missing_cols}\")\n",
    "        return False\n",
    "    \n",
    "    # 2. DATA CONSISTENCY CHECKS\n",
    "    print(f\"\\n2. DATA CONSISTENCY CHECKS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Get all unique values from original\n",
    "    all_original_values = set()\n",
    "    for values_list in split_original:\n",
    "        all_original_values.update(values_list)\n",
    "    all_original_values = sorted([v for v in all_original_values if v and v != 'nan'])\n",
    "    \n",
    "    print(f\"Original unique values: {len(all_original_values)}\")\n",
    "    \n",
    "    if strategy == 'top_k':\n",
    "        # Validate top-k strategy\n",
    "        validate_top_k_strategy(df_original, df_processed, original_column, new_columns, separator)\n",
    "    elif strategy == 'count_features':\n",
    "        validate_count_features_strategy(df_original, df_processed, original_column, new_columns, separator)\n",
    "    elif strategy == 'frequency_threshold':\n",
    "        validate_frequency_threshold_strategy(df_original, df_processed, original_column, new_columns, separator)\n",
    "    \n",
    "    # 3. SAMPLE VALIDATION\n",
    "    print(f\"\\n3. SAMPLE-BY-SAMPLE VALIDATION\")\n",
    "    print(\"-\" * 40)\n",
    "    validate_sample_rows(df_original, df_processed, original_column, new_columns, separator, n_samples=5)\n",
    "    \n",
    "    # 4. STATISTICAL VALIDATION\n",
    "    print(f\"\\n4. STATISTICAL VALIDATION\")\n",
    "    print(\"-\" * 40)\n",
    "    validate_statistics(df_original, df_processed, original_column, new_columns, separator)\n",
    "    \n",
    "    # 5. INFORMATION LOSS ASSESSMENT\n",
    "    print(f\"\\n5. INFORMATION LOSS ASSESSMENT\")\n",
    "    print(\"-\" * 40)\n",
    "    assess_information_loss(df_original, df_processed, original_column, new_columns, separator)\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_top_k_strategy(df_original, df_processed, original_column, new_columns, separator, k=None):\n",
    "    \"\"\"Validate top-k strategy specifically\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Get value counts\n",
    "    all_values = []\n",
    "    for values_list in split_original:\n",
    "        all_values.extend(values_list)\n",
    "    value_counts = Counter(all_values)\n",
    "    \n",
    "    # Determine k if not provided\n",
    "    if k is None:\n",
    "        # Exclude \"other\" column to determine k\n",
    "        non_other_cols = [col for col in new_columns if not col.endswith('_other')]\n",
    "        k = len(non_other_cols)\n",
    "    \n",
    "    top_k_values = [val for val, count in value_counts.most_common(k)]\n",
    "    print(f\"Top {k} values: {top_k_values[:5]}{'...' if len(top_k_values) > 5 else ''}\")\n",
    "    \n",
    "    # Check each top-k column\n",
    "    for col in new_columns:\n",
    "        if col.endswith('_other'):\n",
    "            # Validate \"other\" column\n",
    "            validate_other_column(df_original, df_processed, original_column, col, top_k_values, separator)\n",
    "        else:\n",
    "            # Extract the value name from column name\n",
    "            value_name = col.replace(f\"{original_column}_top_\", \"\").replace(f\"{original_column}_\", \"\")\n",
    "            validate_binary_column(df_original, df_processed, original_column, col, value_name, separator)\n",
    "\n",
    "\n",
    "def validate_binary_column(df_original, df_processed, original_column, new_column, value_name, separator):\n",
    "    \"\"\"Validate a single binary column\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Expected values: 1 if value_name in the list, 0 otherwise\n",
    "    expected = split_original.apply(lambda x: 1 if value_name in x else 0)\n",
    "    actual = df_processed[new_column]\n",
    "    \n",
    "    # Compare\n",
    "    matches = (expected == actual).sum()\n",
    "    total = len(expected)\n",
    "    match_rate = matches / total * 100\n",
    "    \n",
    "    print(f\"  '{new_column}': {matches}/{total} matches ({match_rate:.1f}%)\")\n",
    "    \n",
    "    if match_rate < 100:\n",
    "        mismatches = df_original.loc[expected != actual, original_column].head(3)\n",
    "        print(f\"    Sample mismatches: {list(mismatches)}\")\n",
    "\n",
    "\n",
    "def validate_other_column(df_original, df_processed, original_column, other_column, top_values, separator):\n",
    "    \"\"\"Validate the 'other' category column\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Expected: 1 if any value is NOT in top_values, 0 if all values are in top_values\n",
    "    expected = split_original.apply(lambda x: 1 if any(val not in top_values for val in x) else 0)\n",
    "    actual = df_processed[other_column]\n",
    "    \n",
    "    matches = (expected == actual).sum()\n",
    "    total = len(expected)\n",
    "    match_rate = matches / total * 100\n",
    "    \n",
    "    print(f\"  '{other_column}': {matches}/{total} matches ({match_rate:.1f}%)\")\n",
    "\n",
    "\n",
    "def validate_count_features_strategy(df_original, df_processed, original_column, new_columns, separator):\n",
    "    \"\"\"Validate count features strategy\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    for col in new_columns:\n",
    "        if col.endswith('_count'):\n",
    "            # Validate total count\n",
    "            expected = split_original.apply(len)\n",
    "            actual = df_processed[col]\n",
    "            matches = (expected == actual).sum()\n",
    "            print(f\"  '{col}': {matches}/{len(expected)} matches ({matches/len(expected)*100:.1f}%)\")\n",
    "            \n",
    "        elif col.endswith('_unique_count'):\n",
    "            # Validate unique count\n",
    "            expected = split_original.apply(lambda x: len(set(x)))\n",
    "            actual = df_processed[col]\n",
    "            matches = (expected == actual).sum()\n",
    "            print(f\"  '{col}': {matches}/{len(expected)} matches ({matches/len(expected)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "def validate_sample_rows(df_original, df_processed, original_column, new_columns, separator, n_samples=5):\n",
    "    \"\"\"Manually validate a few sample rows\"\"\"\n",
    "    \n",
    "    print(f\"Validating {n_samples} random samples:\")\n",
    "    \n",
    "    # Get random sample indices\n",
    "    sample_indices = np.random.choice(len(df_original), min(n_samples, len(df_original)), replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices, 1):\n",
    "        original_value = df_original.iloc[idx][original_column]\n",
    "        if pd.isna(original_value):\n",
    "            original_values = []\n",
    "        else:\n",
    "            original_values = [v.strip() for v in str(original_value).split(separator) if v.strip()]\n",
    "        \n",
    "        print(f\"\\n  Sample {i} (row {idx}):\")\n",
    "        print(f\"    Original: '{original_value}'\")\n",
    "        print(f\"    Parsed: {original_values}\")\n",
    "        \n",
    "        # Check new columns for this row\n",
    "        for col in new_columns[:5]:  # Show first 5 columns only\n",
    "            processed_value = df_processed.iloc[idx][col]\n",
    "            print(f\"    {col}: {processed_value}\")\n",
    "\n",
    "\n",
    "def validate_statistics(df_original, df_processed, original_column, new_columns, separator):\n",
    "    \"\"\"Validate statistical properties\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Original statistics\n",
    "    values_per_row = split_original.apply(len)\n",
    "    print(f\"Original values per row - Mean: {values_per_row.mean():.2f}, Std: {values_per_row.std():.2f}\")\n",
    "    \n",
    "    # New data statistics\n",
    "    if any('_count' in col for col in new_columns):\n",
    "        count_col = [col for col in new_columns if col.endswith('_count')][0]\n",
    "        new_counts = df_processed[count_col]\n",
    "        print(f\"Processed counts - Mean: {new_counts.mean():.2f}, Std: {new_counts.std():.2f}\")\n",
    "        \n",
    "        # They should match!\n",
    "        correlation = np.corrcoef(values_per_row, new_counts)[0, 1]\n",
    "        print(f\"Correlation between original and processed counts: {correlation:.4f}\")\n",
    "    \n",
    "    # Check for any impossible values\n",
    "    binary_cols = [col for col in new_columns if not col.endswith(('_count', '_frequency', '_avg_frequency'))]\n",
    "    for col in binary_cols:\n",
    "        unique_vals = df_processed[col].unique()\n",
    "        if not set(unique_vals).issubset({0, 1, np.nan}):\n",
    "            print(f\"⚠️  Warning: Non-binary values in '{col}': {unique_vals}\")\n",
    "\n",
    "\n",
    "def assess_information_loss(df_original, df_processed, original_column, new_columns, separator):\n",
    "    \"\"\"Assess how much information was lost in the transformation\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Get all unique values\n",
    "    all_original_values = set()\n",
    "    for values_list in split_original:\n",
    "        all_original_values.update(values_list)\n",
    "    all_original_values = sorted([v for v in all_original_values if v and v != 'nan'])\n",
    "    \n",
    "    # Count how many unique values are captured by new columns\n",
    "    captured_values = set()\n",
    "    for col in new_columns:\n",
    "        if not col.endswith(('_other', '_count', '_unique_count', '_frequency', '_avg_frequency', '_rare')):\n",
    "            # Extract value name from column name\n",
    "            value_parts = col.replace(f\"{original_column}_\", \"\").replace(\"top_\", \"\").replace(\"freq_\", \"\")\n",
    "            captured_values.add(value_parts)\n",
    "    \n",
    "    capture_rate = len(captured_values) / len(all_original_values) * 100 if all_original_values else 0\n",
    "    print(f\"Value capture rate: {len(captured_values)}/{len(all_original_values)} ({capture_rate:.1f}%)\")\n",
    "    \n",
    "    if len(all_original_values) - len(captured_values) > 0:\n",
    "        lost_values = set(all_original_values) - captured_values\n",
    "        print(f\"Lost values (first 10): {list(lost_values)[:10]}\")\n",
    "    \n",
    "    # Estimate row-level information preservation\n",
    "    if any('_other' in col for col in new_columns):\n",
    "        other_col = [col for col in new_columns if col.endswith('_other')][0]\n",
    "        rows_with_other = df_processed[other_col].sum()\n",
    "        print(f\"Rows with 'other' values: {rows_with_other}/{len(df_processed)} ({rows_with_other/len(df_processed)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "def quick_validation_summary(df_original, df_processed, column_mapping):\n",
    "    \"\"\"Quick validation summary for all processed columns\"\"\"\n",
    "    \n",
    "    print(\"=== QUICK VALIDATION SUMMARY ===\\n\")\n",
    "    \n",
    "    for original_col, new_cols in column_mapping.items():\n",
    "        print(f\"✓ {original_col} → {len(new_cols)} new columns\")\n",
    "        \n",
    "        # Check for obvious issues\n",
    "        issues = []\n",
    "        \n",
    "        for col in new_cols:\n",
    "            if col not in df_processed.columns:\n",
    "                issues.append(f\"Missing column: {col}\")\n",
    "            else:\n",
    "                # Check for unexpected values in binary columns\n",
    "                if not col.endswith(('_count', '_frequency', '_avg_frequency')):\n",
    "                    unique_vals = set(df_processed[col].dropna().unique())\n",
    "                    if not unique_vals.issubset({0, 1, 0.0, 1.0}):\n",
    "                        issues.append(f\"Non-binary values in {col}: {unique_vals}\")\n",
    "        \n",
    "        if issues:\n",
    "            print(f\"  ⚠️  Issues: {issues}\")\n",
    "        else:\n",
    "            print(f\"  ✓ Looks good\")\n",
    "    \n",
    "    print(f\"\\nDataset size: {len(df_original)} → {len(df_processed)} rows\")\n",
    "    print(f\"Column count: {len(df_original.columns)} → {len(df_processed.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15658af4-b655-4429-a368-838ee7578410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-06-01 20:57:57.182021\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def add_missing_categories_from_full_dataset(sample_df, full_df, categorical_columns, samples_per_category=2):\n",
    "    \"\"\"\n",
    "    Add missing categorical values to sample dataset by sampling from full dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample_df : pd.DataFrame\n",
    "        Your limited sample dataset\n",
    "    full_df : pd.DataFrame  \n",
    "        Your complete dataset\n",
    "    categorical_columns : list\n",
    "        List of categorical column names\n",
    "    samples_per_category : int\n",
    "        Number of examples to add for each missing category\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Enhanced dataset with missing categories included\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Analyzing missing categories...\")\n",
    "    \n",
    "    # Find missing categories in sample compared to full dataset\n",
    "    missing_categories = {}\n",
    "    category_stats = {}\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col not in sample_df.columns or col not in full_df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in one of the datasets\")\n",
    "            continue\n",
    "            \n",
    "        full_categories = set(full_df[col].dropna().unique())\n",
    "        sample_categories = set(sample_df[col].dropna().unique())\n",
    "        missing = full_categories - sample_categories\n",
    "        \n",
    "        if missing:\n",
    "            missing_categories[col] = missing\n",
    "            category_stats[col] = {\n",
    "                'total_in_full': len(full_categories),\n",
    "                'in_sample': len(sample_categories),\n",
    "                'missing_count': len(missing)\n",
    "            }\n",
    "            print(f\"Column '{col}': Missing {len(missing)} out of {len(full_categories)} categories\")\n",
    "            print(f\"  Missing categories: {list(missing)[:5]}{'...' if len(missing) > 5 else ''}\")\n",
    "        else:\n",
    "            print(f\"Column '{col}': All categories present in sample\")\n",
    "    \n",
    "    if not missing_categories:\n",
    "        print(\"No missing categories found! Your sample already contains all category values.\")\n",
    "        return sample_df.copy()\n",
    "    \n",
    "    # Collect additional rows for missing categories\n",
    "    additional_rows = []\n",
    "    rows_added_by_category = defaultdict(int)\n",
    "    \n",
    "    for col, missing_vals in missing_categories.items():\n",
    "        print(f\"\\nSampling for column '{col}'...\")\n",
    "        \n",
    "        for val in missing_vals:\n",
    "            # Find all rows in full dataset with this category value\n",
    "            matching_rows = full_df[full_df[col] == val]\n",
    "            \n",
    "            if len(matching_rows) == 0:\n",
    "                print(f\"  Warning: No rows found for {col}='{val}' in full dataset\")\n",
    "                continue\n",
    "            \n",
    "            # Sample requested number of rows (or all available if fewer)\n",
    "            n_samples = min(samples_per_category, len(matching_rows))\n",
    "            sampled_rows = matching_rows.sample(n=n_samples, random_state=42)\n",
    "            \n",
    "            additional_rows.append(sampled_rows)\n",
    "            rows_added_by_category[f\"{col}='{val}'\"] = n_samples\n",
    "            print(f\"  Added {n_samples} rows for '{val}' (out of {len(matching_rows)} available)\")\n",
    "    \n",
    "    # Combine all additional rows\n",
    "    if additional_rows:\n",
    "        df_additional = pd.concat(additional_rows, ignore_index=True)\n",
    "        \n",
    "        # Remove potential duplicates (in case same row satisfies multiple missing categories)\n",
    "        initial_additional_count = len(df_additional)\n",
    "        df_additional = df_additional.drop_duplicates()\n",
    "        final_additional_count = len(df_additional)\n",
    "        \n",
    "        if initial_additional_count != final_additional_count:\n",
    "            print(f\"\\nRemoved {initial_additional_count - final_additional_count} duplicate rows\")\n",
    "        \n",
    "        # Combine with original sample\n",
    "        df_enhanced = pd.concat([sample_df, df_additional], ignore_index=True)\n",
    "        \n",
    "        print(f\"\\n=== SUMMARY ===\")\n",
    "        print(f\"Original sample size: {len(sample_df)}\")\n",
    "        print(f\"Additional rows added: {len(df_additional)}\")\n",
    "        print(f\"Final dataset size: {len(df_enhanced)}\")\n",
    "        print(f\"Size increase: {len(df_additional)/len(sample_df)*100:.1f}%\")\n",
    "        \n",
    "        return df_enhanced\n",
    "    \n",
    "    else:\n",
    "        print(\"No additional rows could be sampled\")\n",
    "        return sample_df.copy()\n",
    "\n",
    "\n",
    "def verify_categories_coverage(df_before, df_after, categorical_columns):\n",
    "    \"\"\"\n",
    "    Verify that the enhanced dataset now covers all categories\n",
    "    \"\"\"\n",
    "    print(\"\\n=== CATEGORY COVERAGE VERIFICATION ===\")\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col not in df_before.columns:\n",
    "            continue\n",
    "            \n",
    "        before_cats = set(df_before[col].dropna().unique())\n",
    "        after_cats = set(df_after[col].dropna().unique())\n",
    "        new_cats = after_cats - before_cats\n",
    "        \n",
    "        print(f\"\\nColumn '{col}':\")\n",
    "        print(f\"  Before: {len(before_cats)} categories\")\n",
    "        print(f\"  After:  {len(after_cats)} categories\")\n",
    "        if new_cats:\n",
    "            print(f\"  New categories added: {list(new_cats)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b174ad57-80fd-4e9a-9871-142c98bb371f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-06-01 20:57:57.257538\n"
     ]
    }
   ],
   "source": [
    "# === 1. DataLoader: Load data and check target column ===\n",
    "\n",
    "class DataLoader(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Load and perform initial data validation whether the target col exists:\n",
    "        - Handles both .xlsx and .csv.\n",
    "        - Stores the original shape of the data.\n",
    "        - Raises an error if the target column is missing.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, target_col='project_prf_normalised_work_effort'):\n",
    "        self.file_path = file_path\n",
    "        self.target_col = target_col  # This should be the standardized form\n",
    "        self.original_shape = None\n",
    "        self.original_target_col = None  # Store what we actually found\n",
    "        \n",
    "    def fit(self, X=None, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _standardize_column_name(self, col_name):\n",
    "        \"\"\"Convert column name to standardized format\"\"\"\n",
    "        return col_name.strip().lower().replace(' ', '_')\n",
    "    \n",
    "    def _find_target_column(self, df_columns):\n",
    "        \"\"\"\n",
    "        Smart target column finder - handles various formats\n",
    "        Returns the actual column name from the dataframe\n",
    "        \"\"\"\n",
    "        target_standardized = self.target_col.lower().replace(' ', '_')\n",
    "        \n",
    "        # Try exact match first\n",
    "        if self.target_col in df_columns:\n",
    "            return self.target_col\n",
    "            \n",
    "        # Try standardized versions of all columns\n",
    "        for col in df_columns:\n",
    "            col_standardized = self._standardize_column_name(col)\n",
    "            if col_standardized == target_standardized:\n",
    "                return col\n",
    "                \n",
    "        # If still not found, look for partial matches (for debugging)\n",
    "        similar_cols = []\n",
    "        target_words = set(target_standardized.split('_'))\n",
    "        for col in df_columns:\n",
    "            col_words = set(self._standardize_column_name(col).split('_'))\n",
    "            if len(target_words.intersection(col_words)) >= 2:  # At least 2 words match\n",
    "                similar_cols.append(col)\n",
    "                \n",
    "        return None, similar_cols\n",
    "    \n",
    "    def transform(self, X=None):\n",
    "        \"\"\"Load data from file with smart column handling\"\"\"\n",
    "\n",
    "        print(f\"Loading data from: {self.file_path}\")\n",
    "        \n",
    "        # Determine file type and load accordingly; support for Excel or CSV\n",
    "        if self.file_path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(self.file_path)\n",
    "        elif self.file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(self.file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Use .xlsx or .csv\")\n",
    "        \n",
    "        self.original_shape = df.shape\n",
    "        print(f\"Loaded data with shape: {df.shape}\")\n",
    "\n",
    "        # Standardize ALL object/categorical columns: lowercase and strip\n",
    "        for col in df.select_dtypes(include='object').columns:\n",
    "            df[col] = df[col].str.lower().str.strip()\n",
    "\n",
    "        # Convert object columns to numeric if most values can be coerced to numbers.\n",
    "        #df = convert_object_numeric_cols(df)\n",
    "        \n",
    "\n",
    "        # Smart target column finding\n",
    "        result = self._find_target_column(df.columns)\n",
    "        \n",
    "        if isinstance(result, tuple):  # Not found, got similar columns\n",
    "            actual_col, similar_cols = result\n",
    "            error_msg = f\"Target column '{self.target_col}' not found in data.\"\n",
    "            if similar_cols:\n",
    "                error_msg += f\" Similar columns found: {similar_cols}\"\n",
    "            else:\n",
    "                error_msg += f\" Available columns: {list(df.columns)}\"\n",
    "            raise ValueError(error_msg)\n",
    "        else:\n",
    "            actual_col = result\n",
    "            \n",
    "        # Store the original column name we found\n",
    "        self.original_target_col = actual_col\n",
    "        \n",
    "        if actual_col != self.target_col:\n",
    "            print(f\"Target column found: '{actual_col}' -> will be standardized to '{self.target_col}'\")\n",
    "            \n",
    "        return df\n",
    "\n",
    "# === 2. ColumnNameStandardizer: Clean and standardize column names ===\n",
    "class ColumnNameStandardizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Standardize column names for consistency (lowercase, underscores, removes odd chars):\n",
    "        - Strips spaces, lowercases, replaces & with _&_, removes special chars.\n",
    "        - Useful for later steps and compatibility with modeling libraries.)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_col=None, original_target_col=None):\n",
    "        self.column_mapping = {}\n",
    "        self.target_col = target_col\n",
    "        self.original_target_col = original_target_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _standardize_columns(self, columns):\n",
    "        \"\"\"Standardize column names\"\"\"\n",
    "        return [col.strip().lower().replace(' ', '_') for col in columns]\n",
    "    \n",
    "    def _clean_column_names(self, columns):\n",
    "        \"\"\"Clean column names for compatibility\"\"\"\n",
    "        cleaned_cols = []\n",
    "        for col in columns:\n",
    "            # Replace ampersands with _&_ to match expected transformations\n",
    "            col_clean = col.replace(' & ', '_&_')\n",
    "            # Remove special characters except underscores and ampersands\n",
    "            col_clean = re.sub(r'[^\\w\\s&]', '', col_clean)\n",
    "            # Replace spaces with underscores\n",
    "            col_clean = col_clean.replace(' ', '_')\n",
    "            cleaned_cols.append(col_clean)\n",
    "        return cleaned_cols\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply column name standardization\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Store original column names\n",
    "        original_columns = df.columns.tolist()\n",
    "        \n",
    "        # Apply standardization\n",
    "        standardized_cols = self._standardize_columns(original_columns)\n",
    "        cleaned_cols = self._clean_column_names(standardized_cols)\n",
    "\n",
    "        # Special handling for target column\n",
    "        if self.original_target_col and self.target_col:\n",
    "            target_index = None\n",
    "            try:\n",
    "                target_index = original_columns.index(self.original_target_col)\n",
    "                cleaned_cols[target_index] = self.target_col\n",
    "                print(f\"Target column '{self.original_target_col}' -> '{self.target_col}'\")\n",
    "            except ValueError:\n",
    "                pass  # Original target col not found, proceed normally\n",
    "        \n",
    "        \n",
    "        # Create mapping\n",
    "        self.column_mapping = dict(zip(original_columns, cleaned_cols))\n",
    "        \n",
    "        # Apply new column names\n",
    "        df.columns = cleaned_cols\n",
    "        \n",
    "        # Report changes\n",
    "        changed_cols = sum(1 for orig, new in self.column_mapping.items() if orig != new)\n",
    "        print(f\"Standardized {changed_cols} column names\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 2.1. CategoricalValueStandardizer: Apply a standardization mapping to all values in selected categorical columns ===\n",
    "class CategoricalValueStandardizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Apply a standardization mapping to all values in selected categorical columns.\n",
    "    \"\"\"\n",
    "    def __init__(self, mapping: Dict[str, str], columns: List[str] = None):\n",
    "        self.mapping = mapping or {}\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Optionally auto-detect columns if not provided\n",
    "        if self.columns is None:\n",
    "            # Use all object/category columns\n",
    "            self.columns = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        for col in self.columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = (\n",
    "                    df[col].astype(str)\n",
    "                    .str.strip()\n",
    "                    .str.lower()\n",
    "                    .map(lambda x: self.mapping.get(x, x))\n",
    "                )\n",
    "        return df\n",
    "\n",
    "# === 2.2. CategoricalValueCleaner: replace '-' with '_',  lowercase, strip whitespace ===\n",
    "class CategoricalValueCleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Cleans all categorical (object/category) column values:\n",
    "    - Replace '-' with '_'\n",
    "    - Lowercase\n",
    "    - Strip whitespace\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "        for col in cat_cols:\n",
    "            df[col] = (\n",
    "                df[col].astype(str)\n",
    "                .str.replace('-', '_')\n",
    "                .str.lower()\n",
    "                .str.strip()\n",
    "            )\n",
    "        return df\n",
    "\n",
    "\n",
    "# === 3. MissingValueAnalyzer: Analyze and handle missing values ===\n",
    "class MissingValueAnalyzer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Analyze and handle missing values\n",
    "        - Reports number of columns with >50% and >70% missing.\n",
    "        - Drops columns with a high proportion of missing data, except those you want to keep.\n",
    "        - Fills remaining missing values:\n",
    "            - Categorical: Fills with \"Missing\".\n",
    "            - Numeric: Fills with column median.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, high_missing_threshold=0.7, cols_to_keep=None):\n",
    "        self.high_missing_threshold = high_missing_threshold\n",
    "        self.cols_to_keep = cols_to_keep or []\n",
    "        self.high_missing_cols = []\n",
    "        self.missing_stats = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Analyze and handle missing values\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Calculate missing percentages\n",
    "        missing_pct = df.isnull().mean()\n",
    "        self.missing_stats = missing_pct.sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"\\nMissing value analysis:\")\n",
    "        print(f\"Columns with >50% missing: {sum(missing_pct > 0.5)}\")\n",
    "        print(f\"Columns with >70% missing: {sum(missing_pct > self.high_missing_threshold)}\")\n",
    "        \n",
    "        # Identify high missing columns\n",
    "        self.high_missing_cols = missing_pct[missing_pct > self.high_missing_threshold].index.tolist()\n",
    "        \n",
    "        # Filter out columns we want to keep\n",
    "        final_high_missing_cols = [col for col in self.high_missing_cols if col not in self.cols_to_keep]\n",
    "        \n",
    "        print(f\"Dropping {len(final_high_missing_cols)} columns with >{self.high_missing_threshold*100}% missing values\")\n",
    "        \n",
    "        # Drop high missing columns\n",
    "        df_clean = df.drop(columns=final_high_missing_cols)\n",
    "        \n",
    "        # Fill remaining missing values in categorical columns\n",
    "        cat_cols = df_clean.select_dtypes(include=['object', 'category']).columns\n",
    "        for col in cat_cols:\n",
    "            df_clean[col] = df_clean[col].fillna('Missing')\n",
    "        \n",
    "        # Fill remaining missing values in numerical columns with median\n",
    "        num_cols = df_clean.select_dtypes(include=['number']).columns\n",
    "        for col in num_cols:\n",
    "            if df_clean[col].isnull().sum() > 0:\n",
    "                median_val = df_clean[col].median()\n",
    "                df_clean[col] = df_clean[col].fillna(median_val)\n",
    "                print(f\"Filled {col} missing values with median: {median_val}\")\n",
    "        \n",
    "        print(f\"Data shape after missing value handling: {df_clean.shape}\")\n",
    "        return df_clean\n",
    "\n",
    "# === 4. SemicolonProcessor: Process multi-value columns (semicolon-separated) ===\n",
    "class SemicolonProcessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Process semicolon-separated values in columns (e.g., “Python; Java; SQL”)\n",
    "        - Identifies columns with semicolons.\n",
    "        - Cleans: lowercases, strips, deduplicates, sorts, optionally standardizes values (e.g., \"stand alone\" → \"stand-alone\").\n",
    "        - Useful for multi-value categorical features.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, standardization_mapping=None):\n",
    "        self.semicolon_cols = []\n",
    "        self.standardization_mapping = standardization_mapping or {\n",
    "            \"scrum\": \"agile development\",\n",
    "            \"file &/or print server\": \"file/print server\",\n",
    "        }\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _clean_and_sort_semicolon(self, val, apply_standardization=False, mapping=None):\n",
    "        \"\"\"Clean, deduplicate, sort, and standardize semicolon-separated values\"\"\"\n",
    "        if pd.isnull(val) or val == '':\n",
    "            return val\n",
    "        \n",
    "        parts = [x.strip().lower() for x in str(val).split(';') if x.strip()]\n",
    "        \n",
    "        if apply_standardization and mapping is not None:\n",
    "            parts = [mapping.get(part, part) for part in parts]\n",
    "        \n",
    "        unique_cleaned = sorted(set(parts))\n",
    "        return '; '.join(unique_cleaned)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Process semicolon-separated columns\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Identify columns with semicolons\n",
    "        self.semicolon_cols = [\n",
    "            col for col in df.columns\n",
    "            if df[col].dropna().astype(str).str.contains(';').any()\n",
    "        ]\n",
    "        \n",
    "        print(f\"Found {len(self.semicolon_cols)} columns with semicolons: {self.semicolon_cols}\")\n",
    "        \n",
    "        # Process each semicolon column\n",
    "        for col in self.semicolon_cols:\n",
    "            # Apply mapping for specific columns\n",
    "            apply_mapping = col in ['process_pmf_development_methodologies', 'tech_tf_server_roles']\n",
    "            mapping = self.standardization_mapping if apply_mapping else None\n",
    "            \n",
    "            # Clean the column\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: self._clean_and_sort_semicolon(x, apply_standardization=apply_mapping, mapping=mapping)\n",
    "            )\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 5. MultiValueEncoder: Encode semicolon columns using MultiLabelBinarizer ===\n",
    "class MultiValueEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Handle multi-value columns using MultiLabelBinarizer\n",
    "        - Only processes columns with a manageable number of unique values (max_cardinality).\n",
    "        - Each semicolon column becomes several binary columns (e.g., \"lang__python\", \"lang__java\", ...).     \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_cardinality=10):\n",
    "        # Ensure max_cardinality is always an integer\n",
    "        self.max_cardinality = int(max_cardinality) if max_cardinality is not None else 10\n",
    "        self.multi_value_cols = []\n",
    "        self.mlb_transformers = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Encode multi-value columns\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Identify semicolon columns (multi-value)\n",
    "        semicolon_cols = [\n",
    "            col for col in df.columns\n",
    "            if df[col].dropna().astype(str).str.contains(';').any()\n",
    "        ]\n",
    "        \n",
    "        # Filter for low cardinality multi-value columns\n",
    "        self.multi_value_cols = []\n",
    "        for col in semicolon_cols:\n",
    "            # Get unique values across all entries\n",
    "            all_values = set()\n",
    "            for val in df[col].dropna().astype(str):\n",
    "                values = [v.strip() for v in val.split(';') if v.strip()]\n",
    "                all_values.update(values)\n",
    "            \n",
    "            # Check cardinality (max_cardinality is already an integer from __init__)\n",
    "            if len(all_values) <= self.max_cardinality:\n",
    "                self.multi_value_cols.append(col)\n",
    "        \n",
    "        print(f\"Encoding {len(self.multi_value_cols)} multi-value columns: {self.multi_value_cols}\")\n",
    "        \n",
    "        # Process each multi-value column\n",
    "        for col in self.multi_value_cols:\n",
    "            # Prepare data for MultiLabelBinarizer\n",
    "            values = df[col].dropna().astype(str).apply(\n",
    "                lambda x: [item.strip() for item in x.split(';') if item.strip()]\n",
    "            )\n",
    "            \n",
    "            # Handle empty values - fill with empty list for MultiLabelBinarizer\n",
    "            if len(values) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Fit and transform\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            \n",
    "            # Convert to list of lists, handling NaN/empty cases\n",
    "            values_list = []\n",
    "            for idx in df.index:\n",
    "                if idx in values.index and values[idx]:\n",
    "                    values_list.append(values[idx])\n",
    "                else:\n",
    "                    values_list.append([])  # Empty list for missing values\n",
    "            \n",
    "            onehot = pd.DataFrame(\n",
    "                mlb.fit_transform(values_list),\n",
    "                columns=[f\"{col}__{cat}\" for cat in mlb.classes_],\n",
    "                index=df.index\n",
    "            )\n",
    "            \n",
    "            # Store transformer for later use\n",
    "            self.mlb_transformers[col] = mlb\n",
    "\n",
    "            # Check for duplicate columns BEFORE join\n",
    "            overlap = df.columns.intersection(onehot.columns)\n",
    "            if not overlap.empty:\n",
    "                print(f\"Duplicate columns found and will be dropped from onehot: {list(overlap)}\")\n",
    "                onehot = onehot.drop(columns=overlap, errors='ignore')\n",
    "\n",
    "         \n",
    "            # Join with main dataframe\n",
    "            df = df.join(onehot, how='left')\n",
    "            \n",
    "            print(f\"Encoded {col} into {len(mlb.classes_)} binary columns\")\n",
    "        \n",
    "        # Remove original multi-value columns\n",
    "        df = df.drop(columns=self.multi_value_cols)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 6. CategoricalEncoder: One-hot encode regular categorical columns ===\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Handle single-value categorical columns\n",
    "        - Ignores semicolon columns.\n",
    "        - Only encodes columns with a number of categories ≤ max_cardinality (to avoid high-dimensional explosion).\n",
    "        - Can drop the first category for each variable to avoid multicollinearity.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_cardinality=10, drop_first=True):\n",
    "        self.max_cardinality = max_cardinality\n",
    "        self.drop_first = drop_first\n",
    "        self.categorical_cols = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Encode categorical columns\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Identify categorical columns\n",
    "        cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Identify semicolon columns to exclude\n",
    "        semicolon_cols = [\n",
    "            col for col in df.columns\n",
    "            if df[col].dropna().astype(str).str.contains(';').any()\n",
    "        ]\n",
    "        \n",
    "        # Filter for low cardinality single-value categorical columns\n",
    "        self.categorical_cols = [\n",
    "            col for col in cat_cols \n",
    "            if col not in semicolon_cols and df[col].nunique() <= self.max_cardinality\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nOne-hot encoding {len(self.categorical_cols)} categorical columns: {self.categorical_cols}\")\n",
    "\n",
    "        # Print unique values before encoding\n",
    "        if self.categorical_cols:\n",
    "            print(\"\\nUnique values for each categorical column before one-hot encoding:\")\n",
    "            for col in self.categorical_cols:\n",
    "                print(f\"  - {col}: {sorted(df[col].dropna().unique())}\")\n",
    "\n",
    "        # --- DIAGNOSTIC: Check for collapsing after cleaning ---\n",
    "        # Define your category cleaning function (should match what your pipeline uses)\n",
    "        def clean_func(val):\n",
    "            if isinstance(val, str):\n",
    "                v = val.lower().strip()\n",
    "                v = v.replace('++', 'plus_plus').replace('+', 'plus').replace('#', 'sharp')\n",
    "                v = ''.join(c if c.isalnum() or c == '_' else '_' for c in v)\n",
    "                v = v.replace('__', '_').strip('_')\n",
    "                return v\n",
    "            return str(val)\n",
    "        \n",
    "        for col in self.categorical_cols:\n",
    "            mapping = {}\n",
    "            uniques = df[col].dropna().unique()\n",
    "            for val in uniques:\n",
    "                cleaned = clean_func(val)\n",
    "                if cleaned not in mapping:\n",
    "                    mapping[cleaned] = [val]\n",
    "                else:\n",
    "                    mapping[cleaned].append(val)\n",
    "            for cleaned, raw_vals in mapping.items():\n",
    "                if len(raw_vals) > 1:\n",
    "                    print(f\"WARNING: In column '{col}', raw values {raw_vals} all map to cleaned '{cleaned}'\")\n",
    "                    print(\"         Consider improving your standardization mapping for clarity!\")\n",
    "        \n",
    "        # Apply one-hot encoding\n",
    "        if self.categorical_cols:\n",
    "            df = pd.get_dummies(df, columns=self.categorical_cols, drop_first=self.drop_first)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 7. ColumnNameFixer: Final column name cleanup for PyCaret etc ===\n",
    "class ColumnNameFixer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Fix column names for PyCaret compatibility (removes illegal characters, replaces spaces/ampersands, handles duplicates):\n",
    "        - No duplicate column names after encoding.\n",
    "        - Only alphanumeric and underscores. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.column_transformations = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Fix problematic column names\"\"\"\n",
    "        df = X.copy()\n",
    "        original_cols = df.columns.tolist()\n",
    "        fixed_columns = []\n",
    "        seen_columns = set()\n",
    "        \n",
    "        for col in original_cols:\n",
    "            # Replace spaces with underscores\n",
    "            fixed_col = col.replace(' ', '_')\n",
    "            # Replace ampersands\n",
    "            fixed_col = fixed_col.replace('&', 'and')\n",
    "            # Remove other problematic characters\n",
    "            fixed_col = ''.join(c if c.isalnum() or c == '_' else '_' for c in fixed_col)\n",
    "            # Remove multiple consecutive underscores\n",
    "            fixed_col = re.sub('_+', '_', fixed_col)\n",
    "            # Remove leading/trailing underscores\n",
    "            fixed_col = fixed_col.strip('_')\n",
    "            \n",
    "            # Handle duplicates\n",
    "            base_col = fixed_col\n",
    "            suffix = 1\n",
    "            while fixed_col in seen_columns:\n",
    "                fixed_col = f\"{base_col}_{suffix}\"\n",
    "                suffix += 1\n",
    "            \n",
    "            seen_columns.add(fixed_col)\n",
    "            fixed_columns.append(fixed_col)\n",
    "        \n",
    "        # Store transformations\n",
    "        self.column_transformations = dict(zip(original_cols, fixed_columns))\n",
    "        \n",
    "        # Apply new column names\n",
    "        df.columns = fixed_columns\n",
    "        \n",
    "        # Check for duplicates\n",
    "        dup_check = [item for item, count in pd.Series(fixed_columns).value_counts().items() if count > 1]\n",
    "        if dup_check:\n",
    "            print(f\"WARNING: Found {len(dup_check)} duplicate column names: {dup_check}\")\n",
    "        else:\n",
    "            print(\"No duplicate column names after fixing\")\n",
    "        \n",
    "        n_changed = sum(1 for old, new in self.column_transformations.items() if old != new)\n",
    "        print(f\"Fixed {n_changed} column names for PyCaret compatibility\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 8. DataValidator: Final summary and checks ===\n",
    "class DataValidator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Validate final dataset\n",
    "        - Shape, missing values, infinities.\n",
    "        - Data types (numeric, categorical).\n",
    "        - Stats on the target column (mean, std, min, max, missing).\n",
    "        - Report issues if any.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_col):\n",
    "        self.target_col = target_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Validate the processed dataset\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        print(f\"\\n=== Final Data Validation ===\")\n",
    "        print(f\"Final shape: {df.shape}\")\n",
    "        print(f\"Target column: {self.target_col}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_count = df.isnull().sum().sum()\n",
    "        print(f\"Total missing values: {missing_count}\")\n",
    "        \n",
    "        # Check for infinite values\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        inf_count = np.isinf(df[numeric_cols].values).sum()\n",
    "        print(f\"Total infinite values: {inf_count}\")\n",
    "        \n",
    "        # Data types summary\n",
    "        print(f\"\\nData types:\")\n",
    "        print(f\"  Numeric columns: {len(df.select_dtypes(include=[np.number]).columns)}\")\n",
    "        print(f\"  Categorical columns: {len(df.select_dtypes(include=['object', 'category']).columns)}\")\n",
    "        \n",
    "        # Target variable summary\n",
    "        if self.target_col in df.columns:\n",
    "            target_stats = df[self.target_col].describe()\n",
    "            print(f\"\\nTarget variable '{self.target_col}' statistics:\")\n",
    "            print(f\"  Mean: {target_stats['mean']:.2f}\")\n",
    "            print(f\"  Std: {target_stats['std']:.2f}\")\n",
    "            print(f\"  Min: {target_stats['min']:.2f}\")\n",
    "            print(f\"  Max: {target_stats['max']:.2f}\")\n",
    "            print(f\"  Missing: {df[self.target_col].isnull().sum()}\")\n",
    "        else:\n",
    "            print(f\"WARNING: Target column '{self.target_col}' not found!\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === Pipeline creation function: returns the Scikit-learn pipeline ===\n",
    "def create_isbsg_preprocessing_pipeline(\n",
    "    target_col='project_prf_normalised_work_effort',\n",
    "    original_target_col=None,\n",
    "    high_missing_threshold=0.7,\n",
    "    cols_to_keep=None,\n",
    "    max_categorical_cardinality=10,\n",
    "    standardization_mapping=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create complete preprocessing pipeline with smart target column handling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    target_col : str\n",
    "        Name of target column\n",
    "    original_target_col : str\n",
    "        Original target column name found in data\n",
    "    high_missing_threshold : float\n",
    "        Threshold for dropping columns with high missing values\n",
    "    cols_to_keep : list\n",
    "        Columns to keep even if they have high missing values\n",
    "    max_categorical_cardinality : int\n",
    "        Maximum number of unique values for categorical encoding\n",
    "    standardization_mapping : dict\n",
    "        Custom mapping for standardizing semicolon-separated values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    sklearn.pipeline.Pipeline\n",
    "        Complete preprocessing pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    if cols_to_keep is None:\n",
    "        cols_to_keep = [\n",
    "            'project_prf_case_tool_used', \n",
    "            'process_pmf_prototyping_used',\n",
    "            'tech_tf_client_roles', \n",
    "            'tech_tf_type_of_server', \n",
    "            'tech_tf_clientserver_description'\n",
    "        ]\n",
    "    \n",
    "    # Ensure max_categorical_cardinality is an integer\n",
    "    if not isinstance(max_categorical_cardinality, int):\n",
    "        max_categorical_cardinality = 10\n",
    "        print(f\"Warning: max_categorical_cardinality was not an integer, defaulting to {max_categorical_cardinality}\")\n",
    "\n",
    "    # Determine which columns you want to standardize\n",
    "    columns_to_standardize = [\n",
    "        'tech_tf_primary_programming_language',\n",
    "        'tech_tf_architecture',\n",
    "        'project_prf_application_group',\n",
    "        'project_prf_application_type',\n",
    "        'project_prf_team_size_group',\n",
    "        'tech_tf_architecture',\n",
    "        # Add all columns where you want the mapping applied!\n",
    "    ]\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('column_standardizer', ColumnNameStandardizer(target_col, original_target_col)),\n",
    "        ('missing_handler', MissingValueAnalyzer(\n",
    "            high_missing_threshold=high_missing_threshold,\n",
    "            cols_to_keep=cols_to_keep\n",
    "        )),\n",
    "        ('cat_value_cleaner', CategoricalValueCleaner()),  # <-- value CLEANER\n",
    "        ('semicolon_processor', SemicolonProcessor(standardization_mapping=standardization_mapping)),\n",
    "        ('cat_value_standardizer', CategoricalValueStandardizer(\n",
    "            mapping=standardization_mapping,\n",
    "            columns=columns_to_standardize\n",
    "        )),\n",
    "        ('multi_value_encoder', MultiValueEncoder(max_cardinality=max_categorical_cardinality)),\n",
    "        ('categorical_encoder', CategoricalEncoder(max_cardinality=max_categorical_cardinality)),\n",
    "        ('column_fixer', ColumnNameFixer()),\n",
    "        ('validator', DataValidator(target_col))\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# === Full workflow function: orchestrates loading, pipeline, and saving ===\n",
    "def preprocess_isbsg_data(\n",
    "    file_path,\n",
    "    target_col='project_prf_normalised_work_effort',  # Always use standardized form\n",
    "    output_dir='../data',\n",
    "    save_intermediate=True,\n",
    "    **pipeline_kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete preprocessing workflow for ISBSG data: loads the data, runs \n",
    "      the full preprocessing pipeline, saves processed data, pipeline \n",
    "      object, and a metadata report to disk, and returns the processed \n",
    "      DataFrame and metadata\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to input data file\n",
    "    target_col : str\n",
    "        Name of target column\n",
    "    output_dir : str\n",
    "        Directory to save processed data\n",
    "    save_intermediate : bool\n",
    "        Whether to save intermediate processing steps\n",
    "    **pipeline_kwargs : dict\n",
    "        Additional arguments for pipeline creation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Processed dataframe ready for modeling\n",
    "    dict\n",
    "        Processing metadata and statistics\n",
    "    \"\"\"\n",
    "\n",
    "    # print pipeline header\n",
    "    print(\"=\"*60)\n",
    "    print(\"ISBSG Data Preprocessing Pipeline\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    print(f\"Target column (standardized): {target_col}\")\n",
    "    print(f\"Timestamp: {datetime.now()}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load data with smart column detection\n",
    "    loader = DataLoader(file_path, target_col)\n",
    "    df_raw = loader.transform(X = None)\n",
    "    \n",
    "    # Create and fit preprocessing pipeline\n",
    "    pipeline = create_isbsg_preprocessing_pipeline(\n",
    "        target_col=target_col,\n",
    "        original_target_col=loader.original_target_col,  # Pass the found column name\n",
    "        **pipeline_kwargs\n",
    "    )\n",
    "    \n",
    "    # Apply preprocessing in order of ColumnNameStandardizer=> MissingValueAnalyzer =>\n",
    "    # SemicolonProcessor=> MultiValueEncoder=> CategoricalEncoder => ColumnNameFixer\n",
    "\n",
    "    # Apply preprocessing\n",
    "    df_processed = pipeline.fit_transform(df_raw)\n",
    "    \n",
    "    # Prepare metadata\n",
    "    metadata = {\n",
    "        'original_shape': loader.original_shape,\n",
    "        'processed_shape': df_processed.shape,\n",
    "        'processing_timestamp': datetime.now().isoformat(),\n",
    "        'target_column_standardized': target_col,\n",
    "        'target_column_original': loader.original_target_col,\n",
    "        'pipeline_steps': [step[0] for step in pipeline.steps]\n",
    "    }\n",
    "    \n",
    "    # Save processed data\n",
    "    file_stem = Path(file_path).stem\n",
    "    output_path = os.path.join(output_dir, f\"{file_stem}_preprocessed.csv\")\n",
    "    df_processed.to_csv(output_path, index=False)\n",
    "    print(f\"\\nProcessed data saved to: {output_path}\")\n",
    "    \n",
    "    # Save pipeline\n",
    "    pipeline_path = os.path.join(output_dir, f\"{file_stem}_preprocessing_pipeline.pkl\")\n",
    "    joblib.dump(pipeline, pipeline_path)\n",
    "    print(f\"Pipeline saved to: {pipeline_path}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = os.path.join(output_dir, f\"{file_stem}_preprocessing_metadata.txt\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        f.write(\"ISBSG Data Preprocessing Metadata\\n\")\n",
    "        f.write(\"=\"*40 + \"\\n\")\n",
    "        for key, value in metadata.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    print(f\"Metadata saved to: {metadata_path}\")\n",
    "\n",
    "    # Print completion & return results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Preprocessing completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return df_processed, metadata\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6310e105-c7a4-4df3-adf6-9e4ddaef4fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-06-01 20:57:57.281947\n"
     ]
    }
   ],
   "source": [
    "def integrated_categorical_preprocessing(\n",
    "    sample_file_path: str,\n",
    "    full_file_path: str,\n",
    "    target_col: str,\n",
    "    output_dir: str,\n",
    "    cols_to_keep: List[str] = None,\n",
    "    high_card_columns: List[str] = None,\n",
    "    max_categorical_cardinality: int = 10,\n",
    "    samples_per_category: int = 3,\n",
    "    standardization_mapping: Dict[str, str] = None,\n",
    "    high_missing_threshold: float = 0.7,\n",
    "    separator: str = ';',\n",
    "    strategy: str = 'top_k',\n",
    "    k: int = 20\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Integrated pipeline to:\n",
    "    0. Preprocesses sample and full datasets using preprocess_isbsg_data() first\n",
    "    1. Load sample and full datasets\n",
    "    2. Auto-detect categorical columns\n",
    "    3. Handle high-cardinality multi-value columns\n",
    "    4. Enhance sample with missing categories from full dataset\n",
    "    5. Apply standardization and final preprocessing\n",
    "    \n",
    "    Returns:\n",
    "        - Enhanced and processed DataFrame\n",
    "        - Metadata about the processing steps\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"INTEGRATED CATEGORICAL PREPROCESSING PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # --- STEP 1: Preprocess both sample and full datasets using pipeline ---\n",
    "    print(\"\\n1. Preprocessing sample and full datasets with standard pipeline...\")\n",
    "\n",
    "    # Use preprocess_isbsg_data for both datasets\n",
    "    sample_df, sample_metadata = preprocess_isbsg_data(\n",
    "        file_path=sample_file_path,\n",
    "        target_col=target_col,\n",
    "        output_dir=output_dir,\n",
    "        cols_to_keep=cols_to_keep,\n",
    "        max_categorical_cardinality=max_categorical_cardinality,\n",
    "        standardization_mapping=standardization_mapping,\n",
    "        high_missing_threshold=high_missing_threshold,\n",
    "        save_intermediate=False  # or True if you want\n",
    "    )\n",
    "    print(f\"Sample dataset after pipeline: {sample_df.shape}\")\n",
    "\n",
    "    full_df, full_metadata = preprocess_isbsg_data(\n",
    "        file_path=full_file_path,\n",
    "        target_col=target_col,\n",
    "        output_dir=output_dir,\n",
    "        cols_to_keep=cols_to_keep,\n",
    "        max_categorical_cardinality=max_categorical_cardinality,\n",
    "        standardization_mapping=standardization_mapping,\n",
    "        high_missing_threshold=high_missing_threshold,\n",
    "        save_intermediate=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Sample dataset shape: {sample_df.shape}\")\n",
    "    print(f\"Full dataset shape: {full_df.shape}\")\n",
    "    \n",
    "    # Step 2: Auto-detect categorical columns\n",
    "    print(\"\\n2. Auto-detecting categorical columns...\")\n",
    "    categorical_columns = [\n",
    "        col for col in sample_df.columns\n",
    "        if (sample_df[col].dtype == 'object' or sample_df[col].dtype.name == 'category' and sample_df[col].nunique() < 20)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Detected categorical columns: {categorical_columns}\")\n",
    "    \n",
    "    # Step 3: Identify/process high-cardinality multi-value columns\n",
    "    print(\"\\n3. Processing high-cardinality multi-value columns...\")\n",
    "    if high_card_columns is None:\n",
    "        high_card_columns = ['external_eef_organisation_type', 'project_prf_application_type']\n",
    "    \n",
    "    # Analyze and process high-cardinality columns in full dataset first\n",
    "    full_df_processed = full_df.copy()\n",
    "    col_mapping = {}\n",
    "    \n",
    "    for col in high_card_columns:\n",
    "        if col in full_df_processed.columns:\n",
    "            print(f\"\\nProcessing high-cardinality column: {col}\")\n",
    "            recommend_strategy(full_df_processed, col, separator=separator)\n",
    "            \n",
    "            # Process the column\n",
    "            full_df_processed, temp_mapping = handle_high_cardinality_multivalue(\n",
    "                full_df_processed,\n",
    "                multi_value_columns=[col],\n",
    "                separator=separator,\n",
    "                strategy=strategy,\n",
    "                k=k\n",
    "            )\n",
    "            col_mapping.update(temp_mapping)\n",
    "    \n",
    "    # Step 4: Apply same processing to sample dataset\n",
    "    print(\"\\n4. Applying same processing to sample dataset...\")\n",
    "    sample_df_processed = sample_df.copy()\n",
    "    \n",
    "    for col in high_card_columns:\n",
    "        if col in sample_df_processed.columns:\n",
    "            sample_df_processed, _ = handle_high_cardinality_multivalue(\n",
    "                sample_df_processed,\n",
    "                multi_value_columns=[col],\n",
    "                separator=separator,\n",
    "                strategy=strategy,\n",
    "                k=k\n",
    "            )\n",
    "    \n",
    "    # Step 5: Update categorical columns list after processing\n",
    "    print(\"\\n5. Updating categorical columns after high-cardinality processing...\")\n",
    "    updated_categorical_columns = [\n",
    "        col for col in sample_df_processed.columns\n",
    "            if (\n",
    "                (sample_df_processed[col].dtype == 'object' or\n",
    "                 sample_df_processed[col].dtype.name == 'category')\n",
    "                and sample_df_processed[col].nunique() < max_categorical_cardinality\n",
    "            )\n",
    "    ]\n",
    "    \n",
    "    print(f\"Updated categorical columns: {len(updated_categorical_columns)} columns\")\n",
    "\n",
    "    # Filter out excluded columns from category augmentation\n",
    "    final_categorical_columns = [\n",
    "        col for col in updated_categorical_columns\n",
    "        if col not in EXCLUDE_FROM_CATEGORY_AUGMENT\n",
    "    ]\n",
    "    print(f\"Final updated categorical columns: {len(final_categorical_columns)} columns\")\n",
    "    \n",
    "    # Step 6: Enhance sample with missing categories from full dataset\n",
    "    print(\"\\n6. Enhancing sample with missing categories from full dataset...\")\n",
    "    enhanced_df = add_missing_categories_from_full_dataset(\n",
    "        sample_df=sample_df_processed,\n",
    "        full_df=full_df_processed,\n",
    "        categorical_columns=final_categorical_columns,\n",
    "        samples_per_category=samples_per_category\n",
    "    )\n",
    "    \n",
    "    print(f\"Enhanced dataset shape: {enhanced_df.shape}\")\n",
    "    \n",
    "    # Step 7: Verify categories coverage\n",
    "    print(\"\\n7. Verifying categories coverage...\")\n",
    "    verify_categories_coverage(sample_df_processed, enhanced_df, final_categorical_columns)\n",
    "    \n",
    "    # Step 8: Check for and handle duplicate columns before final preprocessing\n",
    "    print(\"\\n8. Checking for duplicate columns...\")\n",
    "    duplicate_cols = enhanced_df.columns[enhanced_df.columns.duplicated()].tolist()\n",
    "    if duplicate_cols:\n",
    "        print(f\"Warning: Found duplicate columns: {duplicate_cols}\")\n",
    "        # Remove duplicates, keeping the first occurrence\n",
    "        enhanced_df = enhanced_df.loc[:, ~enhanced_df.columns.duplicated()]\n",
    "        print(\"Removed duplicate columns\")\n",
    "    \n",
    "    # Step 9: Apply final preprocessing using safe wrapper\n",
    "    print(\"\\n9. Applying final preprocessing...\")\n",
    "    final_df, preprocessing_metadata = safe_preprocess_with_fallback(\n",
    "        enhanced_df=enhanced_df,\n",
    "        target_col=target_col,\n",
    "        output_dir=output_dir,\n",
    "        cols_to_keep=cols_to_keep,\n",
    "        max_categorical_cardinality=max_categorical_cardinality,\n",
    "        standardization_mapping=standardization_mapping,\n",
    "        high_missing_threshold=high_missing_threshold\n",
    "    )\n",
    "    \n",
    "    # Step 10: Final validation and duplicate check\n",
    "    print(\"\\n10. Final validation and duplicate check...\")\n",
    "    \n",
    "    # Check for any remaining duplicates after all processing\n",
    "    final_duplicate_cols = final_df.columns[final_df.columns.duplicated()].tolist()\n",
    "    if final_duplicate_cols:\n",
    "        print(f\"Warning: Found duplicate columns in final dataset: {final_duplicate_cols}\")\n",
    "        # Remove duplicates, keeping the first occurrence\n",
    "        final_df = final_df.loc[:, ~final_df.columns.duplicated()]\n",
    "        print(\"Removed final duplicate columns\")\n",
    "    \n",
    "    print(f\"Original sample shape: {sample_df.shape}\")\n",
    "    print(f\"Final processed shape: {final_df.shape}\")\n",
    "    print(f\"Columns added: {final_df.shape[1] - sample_df.shape[1]}\")\n",
    "    print(f\"Rows added: {final_df.shape[0] - sample_df.shape[0]}\")\n",
    "    \n",
    "    # Check for columns with similar names (potential duplicates)\n",
    "    similar_cols = []\n",
    "    for col in final_df.columns:\n",
    "        if col.endswith('_1') or col.endswith('_2'):\n",
    "            base_name = col.rsplit('_', 1)[0]\n",
    "            if base_name in final_df.columns:\n",
    "                similar_cols.append((base_name, col))\n",
    "    \n",
    "    if similar_cols:\n",
    "        print(f\"\\nWarning: Found potentially duplicate columns:\")\n",
    "        for base, duplicate in similar_cols:\n",
    "            print(f\"  - '{base}' and '{duplicate}'\")\n",
    "        print(\"Consider reviewing your preprocessing functions to avoid double processing.\")\n",
    "    \n",
    "    # Compile metadata\n",
    "    metadata = {\n",
    "        'sample_pipeline_metadata': sample_metadata,\n",
    "        'full_pipeline_metadata': full_metadata,\n",
    "        'original_sample_shape': sample_df.shape,\n",
    "        'original_full_shape': full_df.shape,\n",
    "        'final_shape': final_df.shape,\n",
    "        'categorical_columns_detected': categorical_columns,\n",
    "        'updated_categorical_columns': updated_categorical_columns,\n",
    "        'high_cardinality_columns_processed': high_card_columns,\n",
    "        'column_mapping': col_mapping,\n",
    "        'preprocessing_metadata': preprocessing_metadata,\n",
    "        'rows_added_from_full_dataset': final_df.shape[0] - sample_df.shape[0]\n",
    "    }\n",
    "    \n",
    "    return final_df, metadata\n",
    "\n",
    "def safe_preprocess_with_fallback(\n",
    "    enhanced_df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    output_dir: str,\n",
    "    cols_to_keep: List[str] = None,\n",
    "    max_categorical_cardinality: int = 10,\n",
    "    standardization_mapping: Dict[str, str] = None,\n",
    "    high_missing_threshold: float = 0.7\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Safe preprocessing function that handles the file_path requirement\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save enhanced dataset to temporary file\n",
    "    temp_enhanced_path = os.path.join(output_dir, 'temp_enhanced_sample.xlsx')\n",
    "    enhanced_df.to_excel(temp_enhanced_path, index=False)\n",
    "    \n",
    "    try:\n",
    "        # Apply preprocessing using existing function\n",
    "        final_df, preprocessing_metadata = preprocess_isbsg_data(\n",
    "            file_path=temp_enhanced_path,\n",
    "            target_col=target_col,\n",
    "            output_dir=output_dir,\n",
    "            cols_to_keep=cols_to_keep,\n",
    "            max_categorical_cardinality=max_categorical_cardinality,\n",
    "            standardization_mapping=standardization_mapping,\n",
    "            high_missing_threshold=high_missing_threshold\n",
    "        )\n",
    "        \n",
    "        return final_df, preprocessing_metadata\n",
    "        \n",
    "    finally:\n",
    "        # Clean up temporary file\n",
    "        try:\n",
    "            os.remove(temp_enhanced_path)\n",
    "        except:\n",
    "            print(f\"Warning: Could not remove temporary file {temp_enhanced_path}\")\n",
    "    \n",
    "    return enhanced_df, {'error': 'Preprocessing failed'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "319619a4-0165-4783-b589-2b49e740c91e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-06-01 20:57:57.298100\n"
     ]
    }
   ],
   "source": [
    "# Main execution function\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the integrated pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    sample_file_path = os.path.join(DATA_FOLDER, SAMPLE_FILE)\n",
    "    full_file_path = os.path.join(DATA_FOLDER, FULL_FILE)\n",
    "    \n",
    "    # Columns to keep (customize as needed)\n",
    "    cols_to_keep = [\n",
    "        'Project_PRF_CASE_Tool_Used', \n",
    "        'Process_PMF_Prototyping_Used',\n",
    "        'Tech_TF_Client_Roles', \n",
    "        'Tech_TF_Type_of_Server', \n",
    "        'Tech_TF_ClientServer_Description'\n",
    "    ]\n",
    "    \n",
    "    # High-cardinality multi-value columns\n",
    "    high_card_columns = [\n",
    "        'external_eef_organisation_type', \n",
    "        'project_prf_application_type'\n",
    "    ]\n",
    "    \n",
    "    # Standardization rules\n",
    "    standardization_map = {\n",
    "        # Programming languages\n",
    "        '.net': 'dotnet',\n",
    "        'c': 'c_lang',         # or simply 'c'\n",
    "        'c++': 'cpp',\n",
    "        'c#': 'csharp',\n",
    "        # Architecture\n",
    "        'stand alone': 'standalone',\n",
    "        'stand-alone': 'standalone',\n",
    "        'client server': 'client-server',\n",
    "        # Application group\n",
    "        'mathematically-intensive application': 'math_intensive_app',\n",
    "        'mathematically intensive': 'math_intensive',\n",
    "        # Web development\n",
    "        \"Web?\": \"Web\",\n",
    "        # Server roles\n",
    "        \"file &/or print server\": \"file/print server\",\n",
    "        # Add others as needed\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Run integrated pipeline\n",
    "        final_df, metadata = integrated_categorical_preprocessing(\n",
    "            sample_file_path=sample_file_path,\n",
    "            full_file_path=full_file_path,\n",
    "            target_col=TARGET_COL,\n",
    "            output_dir=DATA_FOLDER,\n",
    "            cols_to_keep=cols_to_keep,\n",
    "            high_card_columns=high_card_columns,\n",
    "            max_categorical_cardinality=10,\n",
    "            samples_per_category=3,\n",
    "            standardization_mapping=standardization_map,\n",
    "            high_missing_threshold=0.7,\n",
    "            separator=';',\n",
    "            strategy='top_k',\n",
    "            k=20\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        output_path = os.path.join(DATA_FOLDER, 'enhanced_sample_final.csv')\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Final dataset saved to: {output_path}\")\n",
    "        print(f\"Final shape: {final_df.shape}\")\n",
    "        print(f\"Ready for PyCaret setup!\")\n",
    "        \n",
    "        # Print summary of changes\n",
    "        print(f\"\\nSUMMARY:\")\n",
    "        print(f\"- Original sample rows: {metadata['original_sample_shape'][0]}\")\n",
    "        print(f\"- Rows added from full dataset: {metadata['rows_added_from_full_dataset']}\")\n",
    "        print(f\"- Final rows: {metadata['final_shape'][0]}\")\n",
    "        print(f\"- Original columns: {metadata['original_sample_shape'][1]}\")\n",
    "        print(f\"- Final columns: {metadata['final_shape'][1]}\")\n",
    "        \n",
    "        return final_df, metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in integrated pipeline: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1167827a-a72d-4767-b4d0-a525ff81bb96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INTEGRATED CATEGORICAL PREPROCESSING PIPELINE\n",
      "============================================================\n",
      "\n",
      "1. Preprocessing sample and full datasets with standard pipeline...\n",
      "============================================================\n",
      "ISBSG Data Preprocessing Pipeline\n",
      "============================================================\n",
      "Processing file: ../data\\ISBSG2016R1_1_agile_dataset_only.xlsx\n",
      "Target column (standardized): project_prf_normalised_work_effort\n",
      "Timestamp: 2025-06-01 20:57:57.307135\n",
      "Loading data from: ../data\\ISBSG2016R1_1_agile_dataset_only.xlsx\n",
      "Loaded data with shape: (78, 52)\n",
      "Target column found: 'Project_PRF_Normalised Work Effort' -> will be standardized to 'project_prf_normalised_work_effort'\n",
      "Target column 'Project_PRF_Normalised Work Effort' -> 'project_prf_normalised_work_effort'\n",
      "Standardized 52 column names\n",
      "\n",
      "Missing value analysis:\n",
      "Columns with >50% missing: 25\n",
      "Columns with >70% missing: 18\n",
      "Dropping 18 columns with >70.0% missing values\n",
      "Filled project_prf_functional_size missing values with median: 82.0\n",
      "Filled project_prf_normalised_level_1_pdr_ufp missing values with median: 3.5\n",
      "Filled project_prf_normalised_pdr_ufp missing values with median: 3.5\n",
      "Filled project_prf_defect_density missing values with median: 0.0\n",
      "Filled project_prf_speed_of_delivery missing values with median: 28.6\n",
      "Filled project_prf_manpower_delivery_rate missing values with median: 2.1\n",
      "Filled project_prf_project_elapsed_time missing values with median: 5.0\n",
      "Filled project_prf_max_team_size missing values with median: 2.0\n",
      "Filled people_prf_project_manage_changes missing values with median: 0.0\n",
      "Filled people_prf_personnel_changes missing values with median: 0.0\n",
      "Filled project_prf_total_project_cost missing values with median: 60775.0\n",
      "Data shape after missing value handling: (78, 34)\n",
      "Found 3 columns with semicolons: ['external_eef_organisation_type', 'project_prf_application_type', 'process_pmf_development_methodologies']\n",
      "Encoding 0 multi-value columns: []\n",
      "\n",
      "One-hot encoding 13 categorical columns: ['external_eef_data_quality_rating', 'project_prf_application_group', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'tech_tf_primary_programming_language', 'project_prf_relative_size', 'project_prf_team_size_group', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_web_development', 'tech_tf_dbms_used', 'project_prf_cost_currency']\n",
      "\n",
      "Unique values for each categorical column before one-hot encoding:\n",
      "  - external_eef_data_quality_rating: ['a', 'b', 'c', 'd']\n",
      "  - project_prf_application_group: ['business application', 'infrastructure software', 'mathematically_intensive application', 'missing', 'real_time application']\n",
      "  - project_prf_development_type: ['enhancement', 'new development', 're_development']\n",
      "  - tech_tf_development_platform: ['missing', 'mr', 'multi', 'pc', 'proprietary']\n",
      "  - tech_tf_language_type: ['3gl', '4gl', '5gl']\n",
      "  - tech_tf_primary_programming_language: ['abap', 'cpp', 'csharp', 'dotnet', 'java', 'javascript', 'oracle', 'pl/i', 'proprietary agile platform']\n",
      "  - project_prf_relative_size: ['l', 'm1', 'm2', 'missing', 's', 'xl', 'xs', 'xxs']\n",
      "  - project_prf_team_size_group: ['1', '2', '21_30', '3_4', '41_50', '5_8', '61_70', '9_14', 'missing']\n",
      "  - tech_tf_architecture: ['client-server', 'missing', 'multi_tier with web public interface', 'standalone']\n",
      "  - tech_tf_client_server: ['missing', 'no', 'yes']\n",
      "  - tech_tf_web_development: ['missing', 'web']\n",
      "  - tech_tf_dbms_used: ['missing', 'yes']\n",
      "  - project_prf_cost_currency: ['canada, dollar', 'european, euro', 'missing', 'united states, dollar']\n",
      "No duplicate column names after fixing\n",
      "Fixed 9 column names for PyCaret compatibility\n",
      "\n",
      "=== Final Data Validation ===\n",
      "Final shape: (78, 69)\n",
      "Target column: project_prf_normalised_work_effort\n",
      "Total missing values: 0\n",
      "Total infinite values: 0\n",
      "\n",
      "Data types:\n",
      "  Numeric columns: 17\n",
      "  Categorical columns: 4\n",
      "\n",
      "Target variable 'project_prf_normalised_work_effort' statistics:\n",
      "  Mean: 3362.71\n",
      "  Std: 10643.13\n",
      "  Min: 6.00\n",
      "  Max: 60047.00\n",
      "  Missing: 0\n",
      "\n",
      "Processed data saved to: ../data\\ISBSG2016R1_1_agile_dataset_only_preprocessed.csv\n",
      "Pipeline saved to: ../data\\ISBSG2016R1_1_agile_dataset_only_preprocessing_pipeline.pkl\n",
      "Metadata saved to: ../data\\ISBSG2016R1_1_agile_dataset_only_preprocessing_metadata.txt\n",
      "\n",
      "============================================================\n",
      "Preprocessing completed successfully!\n",
      "============================================================\n",
      "Sample dataset after pipeline: (78, 69)\n",
      "============================================================\n",
      "ISBSG Data Preprocessing Pipeline\n",
      "============================================================\n",
      "Processing file: ../data\\ISBSG2016R1_1_full_dataset.xlsx\n",
      "Target column (standardized): project_prf_normalised_work_effort\n",
      "Timestamp: 2025-06-01 20:57:57.732484\n",
      "Loading data from: ../data\\ISBSG2016R1_1_full_dataset.xlsx\n",
      "Loaded data with shape: (7518, 52)\n",
      "Target column found: 'Project_PRF_Normalised Work Effort' -> will be standardized to 'project_prf_normalised_work_effort'\n",
      "Target column 'Project_PRF_Normalised Work Effort' -> 'project_prf_normalised_work_effort'\n",
      "Standardized 52 column names\n",
      "\n",
      "Missing value analysis:\n",
      "Columns with >50% missing: 30\n",
      "Columns with >70% missing: 24\n",
      "Dropping 24 columns with >70.0% missing values\n",
      "Filled project_prf_functional_size missing values with median: 139.0\n",
      "Filled project_prf_normalised_work_effort_level_1 missing values with median: 1593.0\n",
      "Filled project_prf_normalised_work_effort missing values with median: 1699.0\n",
      "Filled project_prf_normalised_level_1_pdr_ufp missing values with median: 11.2\n",
      "Filled project_prf_normalised_pdr_ufp missing values with median: 11.6\n",
      "Filled project_prf_speed_of_delivery missing values with median: 26.8\n",
      "Filled project_prf_project_elapsed_time missing values with median: 6.0\n",
      "Filled project_prf_max_team_size missing values with median: 7.0\n",
      "Data shape after missing value handling: (7518, 28)\n",
      "Found 4 columns with semicolons: ['external_eef_organisation_type', 'project_prf_application_group', 'project_prf_application_type', 'process_pmf_development_methodologies']\n",
      "Encoding 1 multi-value columns: ['project_prf_application_group']\n",
      "Encoded project_prf_application_group into 6 binary columns\n",
      "\n",
      "One-hot encoding 9 categorical columns: ['external_eef_data_quality_rating', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'project_prf_relative_size', 'project_prf_case_tool_used', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_dbms_used']\n",
      "\n",
      "Unique values for each categorical column before one-hot encoding:\n",
      "  - external_eef_data_quality_rating: ['a', 'b', 'c', 'd']\n",
      "  - project_prf_development_type: ['enhancement', 'new development', 'not defined', 'other', 'poc', 'porting', 're_development']\n",
      "  - tech_tf_development_platform: ['hand held', 'mf', 'missing', 'mr', 'multi', 'pc', 'proprietary']\n",
      "  - tech_tf_language_type: ['2gl', '3gl', '4gl', '5gl', 'apg', 'missing']\n",
      "  - project_prf_relative_size: ['l', 'm1', 'm2', 'missing', 's', 'xl', 'xs', 'xxl', 'xxs', 'xxxl']\n",
      "  - project_prf_case_tool_used: [\"don't know\", 'missing', 'no', 'yes']\n",
      "  - tech_tf_architecture: ['client-server', 'missing', 'multi_tier', 'multi_tier / client server', 'multi_tier with web interface', 'multi_tier with web public interface', 'stand_alone', 'standalone']\n",
      "  - tech_tf_client_server: [\"don't know\", 'missing', 'no', 'not applicable', 'yes']\n",
      "  - tech_tf_dbms_used: ['missing', 'no', 'yes']\n",
      "No duplicate column names after fixing\n",
      "Fixed 12 column names for PyCaret compatibility\n",
      "\n",
      "=== Final Data Validation ===\n",
      "Final shape: (7518, 69)\n",
      "Target column: project_prf_normalised_work_effort\n",
      "Total missing values: 0\n",
      "Total infinite values: 0\n",
      "\n",
      "Data types:\n",
      "  Numeric columns: 18\n",
      "  Categorical columns: 6\n",
      "\n",
      "Target variable 'project_prf_normalised_work_effort' statistics:\n",
      "  Mean: 4747.39\n",
      "  Std: 15957.40\n",
      "  Min: 2.00\n",
      "  Max: 645694.00\n",
      "  Missing: 0\n",
      "\n",
      "Processed data saved to: ../data\\ISBSG2016R1_1_full_dataset_preprocessed.csv\n",
      "Pipeline saved to: ../data\\ISBSG2016R1_1_full_dataset_preprocessing_pipeline.pkl\n",
      "Metadata saved to: ../data\\ISBSG2016R1_1_full_dataset_preprocessing_metadata.txt\n",
      "\n",
      "============================================================\n",
      "Preprocessing completed successfully!\n",
      "============================================================\n",
      "Sample dataset shape: (78, 69)\n",
      "Full dataset shape: (7518, 69)\n",
      "\n",
      "2. Auto-detecting categorical columns...\n",
      "Detected categorical columns: ['external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_type', 'process_pmf_development_methodologies']\n",
      "\n",
      "3. Processing high-cardinality multi-value columns...\n",
      "\n",
      "Processing high-cardinality column: external_eef_organisation_type\n",
      "=== ANALYSIS FOR HIGH-CARDINALITY COLUMN: 'external_eef_organisation_type' ===\n",
      "\n",
      "Total unique values: 139\n",
      "Total value occurrences: 8114\n",
      "Average values per row: 1.08\n",
      "\n",
      "Top 15 most common values:\n",
      "  'missing': 1277 times (17.0% of rows)\n",
      "  'insurance': 1152 times (15.3% of rows)\n",
      "  'communications': 746 times (9.9% of rows)\n",
      "  'manufacturing': 730 times (9.7% of rows)\n",
      "  'banking': 540 times (7.2% of rows)\n",
      "  'medical and health care': 539 times (7.2% of rows)\n",
      "  'telecommunications': 516 times (6.9% of rows)\n",
      "  'government': 487 times (6.5% of rows)\n",
      "  'financial, property & business services': 378 times (5.0% of rows)\n",
      "  'public administration': 247 times (3.3% of rows)\n",
      "  'computers & software': 141 times (1.9% of rows)\n",
      "  'telecommunication': 134 times (1.8% of rows)\n",
      "  'community services': 121 times (1.6% of rows)\n",
      "  'transport & storage': 93 times (1.2% of rows)\n",
      "  'wholesale & retail trade': 89 times (1.2% of rows)\n",
      "\n",
      "Frequency distribution:\n",
      "  1 values appear 1277 time(s)\n",
      "  1 values appear 1152 time(s)\n",
      "  1 values appear 746 time(s)\n",
      "  1 values appear 730 time(s)\n",
      "  1 values appear 540 time(s)\n",
      "  1 values appear 539 time(s)\n",
      "  1 values appear 516 time(s)\n",
      "  1 values appear 487 time(s)\n",
      "  1 values appear 378 time(s)\n",
      "  1 values appear 247 time(s)\n",
      "\n",
      "Values per row:\n",
      "  Min: 1\n",
      "  Max: 14\n",
      "  Mean: 1.08\n",
      "  Median: 1.00\n",
      "\n",
      "=== STRATEGY RECOMMENDATIONS FOR 'external_eef_organisation_type' ===\n",
      "🔴 VERY HIGH CARDINALITY (100+ unique values)\n",
      "Recommended strategies:\n",
      "1. 'count_features' - Create aggregate features (safest)\n",
      "2. 'top_k' with k=15-25 - Keep only most important values\n",
      "3. 'tfidf' with n_components=5-10 - If values have semantic meaning\n",
      "\n",
      "⚠️  HIGHLY SKEWED DISTRIBUTION detected\n",
      "   Consider 'frequency_threshold' or 'top_k' strategies\n",
      "\n",
      "Processing high-cardinality column 'external_eef_organisation_type' with strategy 'top_k'...\n",
      "  Created 21 columns (top 20 + other)\n",
      "\n",
      "Processing high-cardinality column: project_prf_application_type\n",
      "=== ANALYSIS FOR HIGH-CARDINALITY COLUMN: 'project_prf_application_type' ===\n",
      "\n",
      "Total unique values: 456\n",
      "Total value occurrences: 8146\n",
      "Average values per row: 1.08\n",
      "\n",
      "Top 15 most common values:\n",
      "  'missing': 1645 times (21.9% of rows)\n",
      "  'financial transaction process/accounting': 1218 times (16.2% of rows)\n",
      "  'transaction/production system': 502 times (6.7% of rows)\n",
      "  'not recorded': 477 times (6.3% of rows)\n",
      "  'management information system': 401 times (5.3% of rows)\n",
      "  'unknown': 272 times (3.6% of rows)\n",
      "  'customer relationship management': 210 times (2.8% of rows)\n",
      "  'workflow support & management': 155 times (2.1% of rows)\n",
      "  'relatively complex application': 154 times (2.0% of rows)\n",
      "  'financial application area': 142 times (1.9% of rows)\n",
      "  'business application': 124 times (1.6% of rows)\n",
      "  'client server': 116 times (1.5% of rows)\n",
      "  'electronic data interchange': 99 times (1.3% of rows)\n",
      "  'online analysis and reporting': 92 times (1.2% of rows)\n",
      "  'stock control & order processing': 91 times (1.2% of rows)\n",
      "\n",
      "Frequency distribution:\n",
      "  1 values appear 1645 time(s)\n",
      "  1 values appear 1218 time(s)\n",
      "  1 values appear 502 time(s)\n",
      "  1 values appear 477 time(s)\n",
      "  1 values appear 401 time(s)\n",
      "  1 values appear 272 time(s)\n",
      "  1 values appear 210 time(s)\n",
      "  1 values appear 155 time(s)\n",
      "  1 values appear 154 time(s)\n",
      "  1 values appear 142 time(s)\n",
      "\n",
      "Values per row:\n",
      "  Min: 1\n",
      "  Max: 8\n",
      "  Mean: 1.08\n",
      "  Median: 1.00\n",
      "\n",
      "=== STRATEGY RECOMMENDATIONS FOR 'project_prf_application_type' ===\n",
      "🔴 VERY HIGH CARDINALITY (100+ unique values)\n",
      "Recommended strategies:\n",
      "1. 'count_features' - Create aggregate features (safest)\n",
      "2. 'top_k' with k=15-25 - Keep only most important values\n",
      "3. 'tfidf' with n_components=5-10 - If values have semantic meaning\n",
      "\n",
      "⚠️  HIGHLY SKEWED DISTRIBUTION detected\n",
      "   Consider 'frequency_threshold' or 'top_k' strategies\n",
      "\n",
      "Processing high-cardinality column 'project_prf_application_type' with strategy 'top_k'...\n",
      "  Created 21 columns (top 20 + other)\n",
      "\n",
      "4. Applying same processing to sample dataset...\n",
      "\n",
      "Processing high-cardinality column 'external_eef_organisation_type' with strategy 'top_k'...\n",
      "  Created 21 columns (top 20 + other)\n",
      "\n",
      "Processing high-cardinality column 'project_prf_application_type' with strategy 'top_k'...\n",
      "  Created 21 columns (top 20 + other)\n",
      "\n",
      "5. Updating categorical columns after high-cardinality processing...\n",
      "Updated categorical columns: 1 columns\n",
      "Final updated categorical columns: 1 columns\n",
      "\n",
      "6. Enhancing sample with missing categories from full dataset...\n",
      "Analyzing missing categories...\n",
      "Column 'process_pmf_development_methodologies': Missing 32 out of 40 categories\n",
      "  Missing categories: ['lean', 'multifunctional teams', 'multifunctional teams; unified process', 'joint application development (jad); multifunctional teams; rapid application development (rad)', 'multifunctional teams; timeboxing']...\n",
      "\n",
      "Sampling for column 'process_pmf_development_methodologies'...\n",
      "  Added 2 rows for 'lean' (out of 2 available)\n",
      "  Added 3 rows for 'multifunctional teams' (out of 74 available)\n",
      "  Added 2 rows for 'multifunctional teams; unified process' (out of 2 available)\n",
      "  Added 3 rows for 'joint application development (jad); multifunctional teams; rapid application development (rad)' (out of 4 available)\n",
      "  Added 3 rows for 'multifunctional teams; timeboxing' (out of 4 available)\n",
      "  Added 3 rows for 'joint application development (jad); multifunctional teams; rapid application development (rad); timeboxing' (out of 6 available)\n",
      "  Added 3 rows for 'incremental' (out of 31 available)\n",
      "  Added 1 rows for 'interactive' (out of 1 available)\n",
      "  Added 3 rows for 'oce' (out of 3 available)\n",
      "  Added 3 rows for 'timeboxing' (out of 20 available)\n",
      "  Added 1 rows for 'joint application development (jad); timeboxing' (out of 1 available)\n",
      "  Added 2 rows for 'rapid application development (rad); timeboxing' (out of 2 available)\n",
      "  Added 1 rows for 'joint application development (jad); multifunctional teams; timeboxing' (out of 1 available)\n",
      "  Added 1 rows for 'personal software process (psp)' (out of 1 available)\n",
      "  Added 3 rows for 'multifunctional teams; rapid application development (rad)' (out of 6 available)\n",
      "  Added 1 rows for 'scrum' (out of 1 available)\n",
      "  Added 3 rows for 'joint application development (jad); rapid application development (rad)' (out of 24 available)\n",
      "  Added 1 rows for 'multifunctional teams; rapid application development (rad); timeboxing' (out of 1 available)\n",
      "  Added 3 rows for 'unified process' (out of 6 available)\n",
      "  Added 1 rows for 'it unified process (itup)' (out of 1 available)\n",
      "  Added 3 rows for 'personal software process (psp); unified process' (out of 5 available)\n",
      "  Added 3 rows for 'missing' (out of 4808 available)\n",
      "  Added 3 rows for 'joint application development (jad)' (out of 79 available)\n",
      "  Added 3 rows for 'spiral' (out of 4 available)\n",
      "  Added 3 rows for 'multifunctional teams; waterfall (incl linear processing & ssadm)' (out of 5 available)\n",
      "  Added 1 rows for 'iterative; unified process' (out of 1 available)\n",
      "  Added 3 rows for 'waterfall (incl linear processing & ssadm)' (out of 2201 available)\n",
      "  Added 3 rows for 'joint application development (jad); multifunctional teams' (out of 33 available)\n",
      "  Added 3 rows for 'rapid application development (rad)' (out of 106 available)\n",
      "  Added 3 rows for 'joint application development (jad); rapid application development (rad); timeboxing' (out of 3 available)\n",
      "  Added 3 rows for 'iterative' (out of 3 available)\n",
      "  Added 1 rows for 'extreme programming (xp)' (out of 1 available)\n",
      "\n",
      "=== SUMMARY ===\n",
      "Original sample size: 78\n",
      "Additional rows added: 75\n",
      "Final dataset size: 153\n",
      "Size increase: 96.2%\n",
      "Enhanced dataset shape: (153, 156)\n",
      "\n",
      "7. Verifying categories coverage...\n",
      "\n",
      "=== CATEGORY COVERAGE VERIFICATION ===\n",
      "\n",
      "Column 'process_pmf_development_methodologies':\n",
      "  Before: 8 categories\n",
      "  After:  40 categories\n",
      "  New categories added: ['lean', 'multifunctional teams', 'multifunctional teams; unified process', 'joint application development (jad); multifunctional teams; rapid application development (rad)', 'multifunctional teams; timeboxing', 'joint application development (jad); multifunctional teams; rapid application development (rad); timeboxing', 'incremental', 'interactive', 'oce', 'timeboxing', 'joint application development (jad); timeboxing', 'rapid application development (rad); timeboxing', 'joint application development (jad); multifunctional teams; timeboxing', 'personal software process (psp)', 'multifunctional teams; rapid application development (rad)', 'scrum', 'joint application development (jad); rapid application development (rad)', 'multifunctional teams; rapid application development (rad); timeboxing', 'unified process', 'it unified process (itup)', 'personal software process (psp); unified process', 'missing', 'joint application development (jad)', 'spiral', 'multifunctional teams; waterfall (incl linear processing & ssadm)', 'iterative; unified process', 'waterfall (incl linear processing & ssadm)', 'joint application development (jad); multifunctional teams', 'rapid application development (rad)', 'joint application development (jad); rapid application development (rad); timeboxing', 'iterative', 'extreme programming (xp)']\n",
      "\n",
      "8. Checking for duplicate columns...\n",
      "\n",
      "9. Applying final preprocessing...\n",
      "============================================================\n",
      "ISBSG Data Preprocessing Pipeline\n",
      "============================================================\n",
      "Processing file: ../data\\temp_enhanced_sample.xlsx\n",
      "Target column (standardized): project_prf_normalised_work_effort\n",
      "Timestamp: 2025-06-01 20:58:01.805894\n",
      "Loading data from: ../data\\temp_enhanced_sample.xlsx\n",
      "Loaded data with shape: (153, 156)\n",
      "Target column 'project_prf_normalised_work_effort' -> 'project_prf_normalised_work_effort'\n",
      "Standardized 11 column names\n",
      "\n",
      "Missing value analysis:\n",
      "Columns with >50% missing: 47\n",
      "Columns with >70% missing: 0\n",
      "Dropping 0 columns with >70.0% missing values\n",
      "Filled project_prf_defect_density missing values with median: 0.0\n",
      "Filled project_prf_manpower_delivery_rate missing values with median: 2.1\n",
      "Filled people_prf_project_manage_changes missing values with median: 0.0\n",
      "Filled people_prf_personnel_changes missing values with median: 0.0\n",
      "Filled project_prf_total_project_cost missing values with median: 60775.0\n",
      "Filled tech_tf_primary_programming_language_cpp missing values with median: 0.0\n",
      "Filled tech_tf_primary_programming_language_csharp missing values with median: 0.0\n",
      "Filled tech_tf_primary_programming_language_dotnet missing values with median: 0.0\n",
      "Filled tech_tf_primary_programming_language_java missing values with median: 0.0\n",
      "Filled tech_tf_primary_programming_language_javascript missing values with median: 0.0\n",
      "Filled tech_tf_primary_programming_language_oracle missing values with median: 0.0\n",
      "Filled tech_tf_primary_programming_language_pl_i missing values with median: 0.0\n",
      "Filled tech_tf_primary_programming_language_proprietary_agile_platform missing values with median: 0.0\n",
      "Filled project_prf_team_size_group_2 missing values with median: 0.0\n",
      "Filled project_prf_team_size_group_21_30 missing values with median: 0.0\n",
      "Filled project_prf_team_size_group_3_4 missing values with median: 0.0\n",
      "Filled project_prf_team_size_group_41_50 missing values with median: 0.0\n",
      "Filled project_prf_team_size_group_5_8 missing values with median: 0.0\n",
      "Filled project_prf_team_size_group_61_70 missing values with median: 0.0\n",
      "Filled project_prf_team_size_group_9_14 missing values with median: 0.0\n",
      "Filled project_prf_team_size_group_missing missing values with median: 0.0\n",
      "Filled tech_tf_web_development_web missing values with median: 0.0\n",
      "Filled project_prf_cost_currency_european_euro missing values with median: 0.0\n",
      "Filled project_prf_cost_currency_missing missing values with median: 1.0\n",
      "Filled project_prf_cost_currency_united_states_dollar missing values with median: 0.0\n",
      "Filled external_eef_organisation_type_top_ieee missing values with median: 0.0\n",
      "Filled external_eef_organisation_type_top_surveillance_&_security missing values with median: 0.0\n",
      "Filled external_eef_organisation_type_top_high_tech missing values with median: 0.0\n",
      "Filled external_eef_organisation_type_top_food_processing missing values with median: 0.0\n",
      "Filled external_eef_organisation_type_top_all_industry_organization_types missing values with median: 0.0\n",
      "Filled external_eef_organisation_type_top_information_technology missing values with median: 0.0\n",
      "Filled external_eef_organisation_type_top_consumer_goods missing values with median: 0.0\n",
      "Filled external_eef_organisation_type_top_public_sector missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_surveillance_and_security missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_content_management_system missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_dynamic_website missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_complex_process_control missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_idm missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_auditing_management missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_management_or_performance_reporting missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_customer_billing missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_data_or_database_management missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_data_warehouse_system missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_mathematical_modelling_finance_or_eng missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_software_development_tool missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_logistic_or_supply_planning_&_control missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_securityauthentication missing values with median: 0.0\n",
      "Filled project_prf_application_group_business_application missing values with median: 1.0\n",
      "Filled project_prf_application_group_mathematically_intensive_application_1 missing values with median: 0.0\n",
      "Filled project_prf_development_type_not_defined missing values with median: 0.0\n",
      "Filled project_prf_development_type_other missing values with median: 0.0\n",
      "Filled project_prf_development_type_poc missing values with median: 0.0\n",
      "Filled project_prf_development_type_porting missing values with median: 0.0\n",
      "Filled tech_tf_development_platform_mf missing values with median: 0.0\n",
      "Filled tech_tf_development_platform_missing missing values with median: 0.0\n",
      "Filled tech_tf_language_type_3gl missing values with median: 1.0\n",
      "Filled tech_tf_language_type_apg missing values with median: 0.0\n",
      "Filled tech_tf_language_type_missing missing values with median: 0.0\n",
      "Filled project_prf_relative_size_xxl missing values with median: 0.0\n",
      "Filled project_prf_relative_size_xxxl missing values with median: 0.0\n",
      "Filled project_prf_case_tool_used_missing missing values with median: 0.0\n",
      "Filled project_prf_case_tool_used_no missing values with median: 0.0\n",
      "Filled project_prf_case_tool_used_yes missing values with median: 0.0\n",
      "Filled tech_tf_architecture_multi_tier missing values with median: 0.0\n",
      "Filled tech_tf_architecture_multi_tier_client_server missing values with median: 0.0\n",
      "Filled tech_tf_architecture_multi_tier_with_web_interface missing values with median: 0.0\n",
      "Filled tech_tf_architecture_stand_alone missing values with median: 0.0\n",
      "Filled tech_tf_client_server_missing missing values with median: 0.0\n",
      "Filled tech_tf_client_server_not_applicable missing values with median: 0.0\n",
      "Filled tech_tf_dbms_used_no missing values with median: 0.0\n",
      "Filled external_eef_organisation_type_top_missing missing values with median: 0.0\n",
      "Filled external_eef_organisation_type_top_insurance missing values with median: 0.0\n",
      "Filled external_eef_organisation_type_top_manufacturing missing values with median: 0.0\n",
      "Filled external_eef_organisation_type_top_telecommunications missing values with median: 0.0\n",
      "Filled external_eef_organisation_type_top_computers_&_software missing values with median: 0.0\n",
      "Filled external_eef_organisation_type_top_telecommunication missing values with median: 0.0\n",
      "Filled external_eef_organisation_type_top_defence missing values with median: 0.0\n",
      "Filled external_eef_organisation_type_top_logistics missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_missing missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_transactionproduction_system missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_not_recorded missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_management_information_system missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_unknown missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_customer_relationship_management missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_relatively_complex_application missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_financial_application_area missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_client_server missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_stock_control_&_order_processing missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_embedded_systemreal_time_application missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_online_esales missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_document_management missing values with median: 0.0\n",
      "Filled project_prf_application_type_top_customer_billingrelationship_management missing values with median: 0.0\n",
      "Data shape after missing value handling: (153, 156)\n",
      "Found 1 columns with semicolons: ['process_pmf_development_methodologies']\n",
      "Encoding 0 multi-value columns: []\n",
      "\n",
      "One-hot encoding 1 categorical columns: ['project_prf_team_size_group']\n",
      "\n",
      "Unique values for each categorical column before one-hot encoding:\n",
      "  - project_prf_team_size_group: ['1', '15_20', '2', '21_30', '31_40', '3_4', '5_8', '61_70', '9_14', 'missing']\n",
      "No duplicate column names after fixing\n",
      "Fixed 16 column names for PyCaret compatibility\n",
      "\n",
      "=== Final Data Validation ===\n",
      "Final shape: (153, 164)\n",
      "Target column: project_prf_normalised_work_effort\n",
      "Total missing values: 0\n",
      "Total infinite values: 0\n",
      "\n",
      "Data types:\n",
      "  Numeric columns: 128\n",
      "  Categorical columns: 3\n",
      "\n",
      "Target variable 'project_prf_normalised_work_effort' statistics:\n",
      "  Mean: 5757.11\n",
      "  Std: 15869.83\n",
      "  Min: 6.00\n",
      "  Max: 138883.00\n",
      "  Missing: 0\n",
      "\n",
      "Processed data saved to: ../data\\temp_enhanced_sample_preprocessed.csv\n",
      "Pipeline saved to: ../data\\temp_enhanced_sample_preprocessing_pipeline.pkl\n",
      "Metadata saved to: ../data\\temp_enhanced_sample_preprocessing_metadata.txt\n",
      "\n",
      "============================================================\n",
      "Preprocessing completed successfully!\n",
      "============================================================\n",
      "\n",
      "10. Final validation and duplicate check...\n",
      "Original sample shape: (78, 69)\n",
      "Final processed shape: (153, 164)\n",
      "Columns added: 95\n",
      "Rows added: 75\n",
      "\n",
      "Warning: Found potentially duplicate columns:\n",
      "  - 'project_prf_application_group_mathematically_intensive_application' and 'project_prf_application_group_mathematically_intensive_application_1'\n",
      "  - 'project_prf_team_size_group_2' and 'project_prf_team_size_group_2_1'\n",
      "  - 'project_prf_team_size_group_21_30' and 'project_prf_team_size_group_21_30_1'\n",
      "  - 'project_prf_team_size_group_3_4' and 'project_prf_team_size_group_3_4_1'\n",
      "  - 'project_prf_team_size_group_5_8' and 'project_prf_team_size_group_5_8_1'\n",
      "  - 'project_prf_team_size_group_61_70' and 'project_prf_team_size_group_61_70_1'\n",
      "  - 'project_prf_team_size_group_9_14' and 'project_prf_team_size_group_9_14_1'\n",
      "  - 'project_prf_team_size_group_missing' and 'project_prf_team_size_group_missing_1'\n",
      "Consider reviewing your preprocessing functions to avoid double processing.\n",
      "\n",
      "============================================================\n",
      "PIPELINE COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "Final dataset saved to: ../data\\enhanced_sample_final.csv\n",
      "Final shape: (153, 164)\n",
      "Ready for PyCaret setup!\n",
      "\n",
      "SUMMARY:\n",
      "- Original sample rows: 78\n",
      "- Rows added from full dataset: 75\n",
      "- Final rows: 153\n",
      "- Original columns: 69\n",
      "- Final columns: 164\n",
      "Cell executed at: 2025-06-01 20:58:02.492876\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    final_df, metadata = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9901f5-b172-4910-9012-1b41f1613c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c188e-6b75-4594-884b-041e49ff40c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5c8df-c459-477a-9eb8-c86337e16a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a1359-5996-453f-a314-22551e0efe97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c40def56-4dad-4aab-aee3-2520a2a5d72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== QUICK VALIDATION SUMMARY ===\n",
      "\n",
      "\n",
      "Dataset size: 7518 → 7518 rows\n",
      "Column count: 52 → 52\n",
      "Original columns: 52\n",
      "Processed columns: 52\n",
      "New columns created: 2\n",
      "Validation functions ready to use!\n"
     ]
    }
   ],
   "source": [
    "# Handling multi-value categorical cols\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load original data\n",
    "    df_original = pd.read_excel(f\"{DATA_FOLDER}/{FULL_FILE}\")\n",
    "    \n",
    "    # Identify your high-cardinality multi-value columns\n",
    "    high_card_columns = ['external_eef_organisation_type', 'project_prf_application_type']  # Updated with actual columns\n",
    "    \n",
    "    # Analyze each column\n",
    "    for col in high_card_columns:\n",
    "        if col in df_original.columns:\n",
    "            recommend_strategy(df_original, col, separator=';')\n",
    "            print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # 2. Process with recommended strategy\n",
    "    df_processed, col_mapping = handle_high_cardinality_multivalue(\n",
    "        df_original,\n",
    "        multi_value_columns=high_card_columns,\n",
    "        separator=';',\n",
    "        strategy='top_k',  # Choose based on recommendations\n",
    "        k=20  # Keep top 20 most frequent values\n",
    "    )\n",
    "\n",
    "    # 3. Quick validation\n",
    "    quick_validation_summary(df_original, df_processed, col_mapping)\n",
    "    \n",
    "    # 4. Detailed validation for specific columns\n",
    "    for original_col, new_cols in col_mapping.items():\n",
    "         validate_multivalue_processing(\n",
    "             df_original, df_processed, original_col, new_cols, \n",
    "             separator=';', strategy='top_k'\n",
    "         )\n",
    "    \n",
    "    print(f\"Original columns: {len(df_original.columns)}\")\n",
    "    print(f\"Processed columns: {len(df_processed.columns)}\")\n",
    "    print(f\"New columns created: {len(df_processed.columns) - len(df_original.columns) + len(high_card_columns)}\")\n",
    "\n",
    "    print(\"Validation functions ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8679ca1-43cd-40fb-87d2-cca7a018f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your datasets from Excel files\n",
    "    sample_df = pd.read_excel(f\"{DATA_FOLDER} / {SAMPLE_FILE}\")  # Your limited sample\n",
    "    full_df = pd.read_excel(f\"{DATA_FOLDER} / {FULL_FILE}\")  # Your complete dataset\n",
    "    \n",
    "   \n",
    "    # Auto-detect categorical columns\n",
    "    categorical_columns = []\n",
    "    for col in sample_df.columns:\n",
    "        if sample_df[col].dtype == 'object' or sample_df[col].nunique() < 20:\n",
    "            categorical_columns.append(col)\n",
    "    \n",
    "    print(\"Categorical columns to process:\", categorical_columns)\n",
    "    \n",
    "    # Enhance the sample with missing categories\n",
    "    enhanced_df = add_missing_categories_from_full_dataset(\n",
    "        sample_df=sample_df,\n",
    "        full_df=full_df,\n",
    "        categorical_columns=categorical_columns,\n",
    "        samples_per_category=3  # Add 3 examples per missing category\n",
    "    )\n",
    "    \n",
    "    # Verify the results\n",
    "    verify_categories_coverage(sample_df, enhanced_df, categorical_columns)\n",
    "    \n",
    "    # Save the enhanced dataset\n",
    "    enhanced_df.to_csv('../data/sample_enhanced_with_missing_categories.csv', index=False)\n",
    "    print(f\"\\nEnhanced dataset saved to: '../data/sample_enhanced_with_missing_categories.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760772b7-c194-4a70-9f14-03303ebb1868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c183a-3940-4b3c-9d71-914a50df3549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22958d39-aeca-4006-8801-d9c03ff40d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f3790-f8ab-4ae6-9dff-7ed5a29fc7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62895741-3d44-4748-982b-811d038539e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ISBSG Data Preprocessing Pipeline\n",
      "============================================================\n",
      "Processing file: ../data\\sample_clean_a_agile_only.xlsx\n",
      "Target column (standardized): Project_PRF_Normalised_Work_Effort\n",
      "Timestamp: 2025-05-31 17:32:41.635778\n",
      "Loading data from: ../data\\sample_clean_a_agile_only.xlsx\n",
      "Loaded data with shape: (78, 52)\n",
      "Target column found: 'Project_PRF_Normalised Work Effort' -> will be standardized to 'Project_PRF_Normalised_Work_Effort'\n",
      "Target column 'Project_PRF_Normalised Work Effort' -> 'Project_PRF_Normalised_Work_Effort'\n",
      "Standardized 52 column names\n",
      "\n",
      "Missing value analysis:\n",
      "Columns with >50% missing: 25\n",
      "Columns with >70% missing: 18\n",
      "Dropping 18 columns with >70.0% missing values\n",
      "Filled project_prf_functional_size missing values with median: 82.0\n",
      "Filled project_prf_normalised_level_1_pdr_ufp missing values with median: 3.5\n",
      "Filled project_prf_normalised_pdr_ufp missing values with median: 3.5\n",
      "Filled project_prf_defect_density missing values with median: 0.0\n",
      "Filled project_prf_speed_of_delivery missing values with median: 28.6\n",
      "Filled project_prf_manpower_delivery_rate missing values with median: 2.1\n",
      "Filled project_prf_project_elapsed_time missing values with median: 5.0\n",
      "Filled project_prf_max_team_size missing values with median: 2.0\n",
      "Filled people_prf_project_manage_changes missing values with median: 0.0\n",
      "Filled people_prf_personnel_changes missing values with median: 0.0\n",
      "Filled project_prf_total_project_cost missing values with median: 60775.0\n",
      "Data shape after missing value handling: (78, 34)\n",
      "Found 3 columns with semicolons: ['external_eef_organisation_type', 'project_prf_application_type', 'process_pmf_development_methodologies']\n",
      "Encoding 0 multi-value columns: []\n",
      "One-hot encoding 13 categorical columns: ['external_eef_data_quality_rating', 'project_prf_application_group', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'tech_tf_primary_programming_language', 'project_prf_relative_size', 'project_prf_team_size_group', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_web_development', 'tech_tf_dbms_used', 'project_prf_cost_currency']\n",
      "No duplicate column names after fixing\n",
      "Fixed 19 column names for PyCaret compatibility\n",
      "\n",
      "=== Final Data Validation ===\n",
      "Final shape: (78, 69)\n",
      "Target column: Project_PRF_Normalised_Work_Effort\n",
      "Total missing values: 0\n",
      "Total infinite values: 0\n",
      "\n",
      "Data types:\n",
      "  Numeric columns: 17\n",
      "  Categorical columns: 4\n",
      "\n",
      "Target variable 'Project_PRF_Normalised_Work_Effort' statistics:\n",
      "  Mean: 3362.71\n",
      "  Std: 10643.13\n",
      "  Min: 6.00\n",
      "  Max: 60047.00\n",
      "  Missing: 0\n",
      "\n",
      "Processed data saved to: ../data\\sample_clean_a_agile_only_preprocessed.csv\n",
      "Pipeline saved to: ../data\\sample_clean_a_agile_only_preprocessing_pipeline.pkl\n",
      "Metadata saved to: ../data\\sample_clean_a_agile_only_preprocessing_metadata.txt\n",
      "\n",
      "============================================================\n",
      "Preprocessing completed successfully!\n",
      "============================================================\n",
      "\n",
      "Final dataset shape: (78, 69)\n",
      "Columns: ['isbsg_project_id', 'project_prf_year_of_project', 'external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_type', 'project_prf_functional_size', 'project_prf_normalised_work_effort_level_1', 'Project_PRF_Normalised_Work_Effort', 'project_prf_normalised_level_1_pdr_ufp', 'project_prf_normalised_pdr_ufp']...\n",
      "\n",
      "Dataset is now ready for PyCaret setup!\n"
     ]
    }
   ],
   "source": [
    "# Execution: usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # File path\n",
    "    file_path = os.path.join(DATA_FOLDER, SAMPLE_FILE)\n",
    "    \n",
    "    # Custom configuration\n",
    "    cols_to_keep = [\n",
    "        'Project_PRF_CASE_Tool_Used', \n",
    "        'Process_PMF_Prototyping_Used',\n",
    "        'Tech_TF_Client_Roles', \n",
    "        'Tech_TF_Type_of_Server', \n",
    "        'Tech_TF_ClientServer_Description'\n",
    "    ]\n",
    "    \n",
    "    # Specific standardization rules for individual components (after cleaning)\n",
    "    standardization_map = {\n",
    "        'stand alone': 'stand-alone',\n",
    "        'client server': 'client-server',\n",
    "        'mathematically intensive': 'mathematically-intensive',\n",
    "        'mathematically intensive application': 'mathematically-intensive application',\n",
    "        \"file &/or print server\": \"file/print server\",\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Run preprocessing\n",
    "        df_processed, metadata = preprocess_isbsg_data(\n",
    "            file_path=file_path,\n",
    "            target_col=TARGET_COL,\n",
    "            output_dir=DATA_FOLDER,\n",
    "            cols_to_keep=cols_to_keep,\n",
    "            max_categorical_cardinality=10,\n",
    "            standardization_mapping=standardization_map,\n",
    "            high_missing_threshold=0.7\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nFinal dataset shape: {df_processed.shape}\")\n",
    "        print(f\"Columns: {list(df_processed.columns[:10])}...\")  # Show first 10 columns\n",
    "        \n",
    "        # Ready for PyCaret setup\n",
    "        print(\"\\nDataset is now ready for PyCaret setup!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47df44c0-f73a-479d-98f1-3e7886cef03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
