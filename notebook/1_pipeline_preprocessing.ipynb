{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f790fa4-3694-4fc7-b8d0-b3a480e05053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df43052d-4e2f-497a-9f7d-7884f05342dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComplete Scikit-Learn Preprocessing Pipeline for ISBSG Data\\n===========================================================\\n\\nThis module provides a comprehensive preprocessing pipeline that handles:\\n1. Data loading and initial cleaning\\n2. Column name standardization\\n3. Missing value handling\\n4. Semicolon-separated value processing\\n5. One-hot encoding for categorical variables\\n6. Multi-label binarization for multi-value columns\\n7. Feature selection and filtering\\n8. Data validation and export\\n\\nBased on the preprocessing steps from the provided notebooks.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Scikit-Learn Preprocessing Pipeline for ISBSG Data\n",
    "===========================================================\n",
    "\n",
    "This module provides a comprehensive preprocessing pipeline that handles:\n",
    "1. Data loading and initial cleaning\n",
    "2. Column name standardization\n",
    "3. Missing value handling\n",
    "4. Semicolon-separated value processing\n",
    "5. One-hot encoding for categorical variables\n",
    "6. Multi-label binarization for multi-value columns\n",
    "7. Feature selection and filtering\n",
    "8. Data validation and export\n",
    "\n",
    "Based on the preprocessing steps from the provided notebooks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29e26088-9829-45c3-ad0e-f34c146cf120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e705a6-19d4-488a-a6cc-751224ebcccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_FOLDER = \"../data\"\n",
    "SAMPLE_FILE = \"sample_clean_a_agile_only.xlsx\"\n",
    "DATA_FILE = \"\"\n",
    "TARGET_COL = \"Project_PRF_Normalised_Work_Effort\"  # be careful about case sensitive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b174ad57-80fd-4e9a-9871-142c98bb371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. DataLoader: Load data and check target column ===\n",
    "\n",
    "class DataLoader(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Load and perform initial data validation whether the target col exists:\n",
    "        - Handles both .xlsx and .csv.\n",
    "        - Stores the original shape of the data.\n",
    "        - Raises an error if the target column is missing.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, target_col='project_prf_normalised_work_effort'):\n",
    "        self.file_path = file_path\n",
    "        self.target_col = target_col  # This should be the standardized form\n",
    "        self.original_shape = None\n",
    "        self.original_target_col = None  # Store what we actually found\n",
    "        \n",
    "    def fit(self, X=None, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _standardize_column_name(self, col_name):\n",
    "        \"\"\"Convert column name to standardized format\"\"\"\n",
    "        return col_name.strip().lower().replace(' ', '_')\n",
    "    \n",
    "    def _find_target_column(self, df_columns):\n",
    "        \"\"\"\n",
    "        Smart target column finder - handles various formats\n",
    "        Returns the actual column name from the dataframe\n",
    "        \"\"\"\n",
    "        target_standardized = self.target_col.lower().replace(' ', '_')\n",
    "        \n",
    "        # Try exact match first\n",
    "        if self.target_col in df_columns:\n",
    "            return self.target_col\n",
    "            \n",
    "        # Try standardized versions of all columns\n",
    "        for col in df_columns:\n",
    "            col_standardized = self._standardize_column_name(col)\n",
    "            if col_standardized == target_standardized:\n",
    "                return col\n",
    "                \n",
    "        # If still not found, look for partial matches (for debugging)\n",
    "        similar_cols = []\n",
    "        target_words = set(target_standardized.split('_'))\n",
    "        for col in df_columns:\n",
    "            col_words = set(self._standardize_column_name(col).split('_'))\n",
    "            if len(target_words.intersection(col_words)) >= 2:  # At least 2 words match\n",
    "                similar_cols.append(col)\n",
    "                \n",
    "        return None, similar_cols\n",
    "    \n",
    "    def transform(self, X=None):\n",
    "        \"\"\"Load data from file with smart column handling\"\"\"\n",
    "\n",
    "        print(f\"Loading data from: {self.file_path}\")\n",
    "        \n",
    "        # Determine file type and load accordingly; support for Excel or CSV\n",
    "        if self.file_path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(self.file_path)\n",
    "        elif self.file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(self.file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Use .xlsx or .csv\")\n",
    "        \n",
    "        self.original_shape = df.shape\n",
    "        print(f\"Loaded data with shape: {df.shape}\")\n",
    "        \n",
    "        # Smart target column finding\n",
    "        result = self._find_target_column(df.columns)\n",
    "        \n",
    "        if isinstance(result, tuple):  # Not found, got similar columns\n",
    "            actual_col, similar_cols = result\n",
    "            error_msg = f\"Target column '{self.target_col}' not found in data.\"\n",
    "            if similar_cols:\n",
    "                error_msg += f\" Similar columns found: {similar_cols}\"\n",
    "            else:\n",
    "                error_msg += f\" Available columns: {list(df.columns)}\"\n",
    "            raise ValueError(error_msg)\n",
    "        else:\n",
    "            actual_col = result\n",
    "            \n",
    "        # Store the original column name we found\n",
    "        self.original_target_col = actual_col\n",
    "        \n",
    "        if actual_col != self.target_col:\n",
    "            print(f\"Target column found: '{actual_col}' -> will be standardized to '{self.target_col}'\")\n",
    "            \n",
    "        return df\n",
    "\n",
    "# === 2. ColumnNameStandardizer: Clean and standardize column names ===\n",
    "class ColumnNameStandardizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Standardize column names for consistency (lowercase, underscores, removes odd chars):\n",
    "        - Strips spaces, lowercases, replaces & with _&_, removes special chars.\n",
    "        - Useful for later steps and compatibility with modeling libraries.)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_col=None, original_target_col=None):\n",
    "        self.column_mapping = {}\n",
    "        self.target_col = target_col\n",
    "        self.original_target_col = original_target_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _standardize_columns(self, columns):\n",
    "        \"\"\"Standardize column names\"\"\"\n",
    "        return [col.strip().lower().replace(' ', '_') for col in columns]\n",
    "    \n",
    "    def _clean_column_names(self, columns):\n",
    "        \"\"\"Clean column names for compatibility\"\"\"\n",
    "        cleaned_cols = []\n",
    "        for col in columns:\n",
    "            # Replace ampersands with _&_ to match expected transformations\n",
    "            col_clean = col.replace(' & ', '_&_')\n",
    "            # Remove special characters except underscores and ampersands\n",
    "            col_clean = re.sub(r'[^\\w\\s&]', '', col_clean)\n",
    "            # Replace spaces with underscores\n",
    "            col_clean = col_clean.replace(' ', '_')\n",
    "            cleaned_cols.append(col_clean)\n",
    "        return cleaned_cols\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply column name standardization\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Store original column names\n",
    "        original_columns = df.columns.tolist()\n",
    "        \n",
    "        # Apply standardization\n",
    "        standardized_cols = self._standardize_columns(original_columns)\n",
    "        cleaned_cols = self._clean_column_names(standardized_cols)\n",
    "\n",
    "        # Special handling for target column\n",
    "        if self.original_target_col and self.target_col:\n",
    "            target_index = None\n",
    "            try:\n",
    "                target_index = original_columns.index(self.original_target_col)\n",
    "                cleaned_cols[target_index] = self.target_col\n",
    "                print(f\"Target column '{self.original_target_col}' -> '{self.target_col}'\")\n",
    "            except ValueError:\n",
    "                pass  # Original target col not found, proceed normally\n",
    "        \n",
    "        \n",
    "        # Create mapping\n",
    "        self.column_mapping = dict(zip(original_columns, cleaned_cols))\n",
    "        \n",
    "        # Apply new column names\n",
    "        df.columns = cleaned_cols\n",
    "        \n",
    "        # Report changes\n",
    "        changed_cols = sum(1 for orig, new in self.column_mapping.items() if orig != new)\n",
    "        print(f\"Standardized {changed_cols} column names\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 3. MissingValueAnalyzer: Analyze and handle missing values ===\n",
    "class MissingValueAnalyzer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Analyze and handle missing values\n",
    "        - Reports number of columns with >50% and >70% missing.\n",
    "        - Drops columns with a high proportion of missing data, except those you want to keep.\n",
    "        - Fills remaining missing values:\n",
    "            - Categorical: Fills with \"Missing\".\n",
    "            - Numeric: Fills with column median.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, high_missing_threshold=0.7, cols_to_keep=None):\n",
    "        self.high_missing_threshold = high_missing_threshold\n",
    "        self.cols_to_keep = cols_to_keep or []\n",
    "        self.high_missing_cols = []\n",
    "        self.missing_stats = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Analyze and handle missing values\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Calculate missing percentages\n",
    "        missing_pct = df.isnull().mean()\n",
    "        self.missing_stats = missing_pct.sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"\\nMissing value analysis:\")\n",
    "        print(f\"Columns with >50% missing: {sum(missing_pct > 0.5)}\")\n",
    "        print(f\"Columns with >70% missing: {sum(missing_pct > self.high_missing_threshold)}\")\n",
    "        \n",
    "        # Identify high missing columns\n",
    "        self.high_missing_cols = missing_pct[missing_pct > self.high_missing_threshold].index.tolist()\n",
    "        \n",
    "        # Filter out columns we want to keep\n",
    "        final_high_missing_cols = [col for col in self.high_missing_cols if col not in self.cols_to_keep]\n",
    "        \n",
    "        print(f\"Dropping {len(final_high_missing_cols)} columns with >{self.high_missing_threshold*100}% missing values\")\n",
    "        \n",
    "        # Drop high missing columns\n",
    "        df_clean = df.drop(columns=final_high_missing_cols)\n",
    "        \n",
    "        # Fill remaining missing values in categorical columns\n",
    "        cat_cols = df_clean.select_dtypes(include=['object', 'category']).columns\n",
    "        for col in cat_cols:\n",
    "            df_clean[col] = df_clean[col].fillna('Missing')\n",
    "        \n",
    "        # Fill remaining missing values in numerical columns with median\n",
    "        num_cols = df_clean.select_dtypes(include=['number']).columns\n",
    "        for col in num_cols:\n",
    "            if df_clean[col].isnull().sum() > 0:\n",
    "                median_val = df_clean[col].median()\n",
    "                df_clean[col] = df_clean[col].fillna(median_val)\n",
    "                print(f\"Filled {col} missing values with median: {median_val}\")\n",
    "        \n",
    "        print(f\"Data shape after missing value handling: {df_clean.shape}\")\n",
    "        return df_clean\n",
    "\n",
    "# === 4. SemicolonProcessor: Process multi-value columns (semicolon-separated) ===\n",
    "class SemicolonProcessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Process semicolon-separated values in columns (e.g., “Python; Java; SQL”)\n",
    "        - Identifies columns with semicolons.\n",
    "        - Cleans: lowercases, strips, deduplicates, sorts, optionally standardizes values (e.g., \"stand alone\" → \"stand-alone\").\n",
    "        - Useful for multi-value categorical features.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, standardization_mapping=None):\n",
    "        self.semicolon_cols = []\n",
    "        self.standardization_mapping = standardization_mapping or {\n",
    "            \"scrum\": \"agile development\",\n",
    "            \"file &/or print server\": \"file/print server\",\n",
    "        }\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _clean_and_sort_semicolon(self, val, apply_standardization=False, mapping=None):\n",
    "        \"\"\"Clean, deduplicate, sort, and standardize semicolon-separated values\"\"\"\n",
    "        if pd.isnull(val) or val == '':\n",
    "            return val\n",
    "        \n",
    "        parts = [x.strip().lower() for x in str(val).split(';') if x.strip()]\n",
    "        \n",
    "        if apply_standardization and mapping is not None:\n",
    "            parts = [mapping.get(part, part) for part in parts]\n",
    "        \n",
    "        unique_cleaned = sorted(set(parts))\n",
    "        return '; '.join(unique_cleaned)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Process semicolon-separated columns\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Identify columns with semicolons\n",
    "        self.semicolon_cols = [\n",
    "            col for col in df.columns\n",
    "            if df[col].dropna().astype(str).str.contains(';').any()\n",
    "        ]\n",
    "        \n",
    "        print(f\"Found {len(self.semicolon_cols)} columns with semicolons: {self.semicolon_cols}\")\n",
    "        \n",
    "        # Process each semicolon column\n",
    "        for col in self.semicolon_cols:\n",
    "            # Apply mapping for specific columns\n",
    "            apply_mapping = col in ['process_pmf_development_methodologies', 'tech_tf_server_roles']\n",
    "            mapping = self.standardization_mapping if apply_mapping else None\n",
    "            \n",
    "            # Clean the column\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: self._clean_and_sort_semicolon(x, apply_standardization=apply_mapping, mapping=mapping)\n",
    "            )\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 5. MultiValueEncoder: Encode semicolon columns using MultiLabelBinarizer ===\n",
    "class MultiValueEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Handle multi-value columns using MultiLabelBinarizer\n",
    "        - Only processes columns with a manageable number of unique values (max_cardinality).\n",
    "        - Each semicolon column becomes several binary columns (e.g., \"lang__python\", \"lang__java\", ...).     \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_cardinality=10):\n",
    "        # Ensure max_cardinality is always an integer\n",
    "        self.max_cardinality = int(max_cardinality) if max_cardinality is not None else 10\n",
    "        self.multi_value_cols = []\n",
    "        self.mlb_transformers = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Encode multi-value columns\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Identify semicolon columns (multi-value)\n",
    "        semicolon_cols = [\n",
    "            col for col in df.columns\n",
    "            if df[col].dropna().astype(str).str.contains(';').any()\n",
    "        ]\n",
    "        \n",
    "        # Filter for low cardinality multi-value columns\n",
    "        self.multi_value_cols = []\n",
    "        for col in semicolon_cols:\n",
    "            # Get unique values across all entries\n",
    "            all_values = set()\n",
    "            for val in df[col].dropna().astype(str):\n",
    "                values = [v.strip() for v in val.split(';') if v.strip()]\n",
    "                all_values.update(values)\n",
    "            \n",
    "            # Check cardinality (max_cardinality is already an integer from __init__)\n",
    "            if len(all_values) <= self.max_cardinality:\n",
    "                self.multi_value_cols.append(col)\n",
    "        \n",
    "        print(f\"Encoding {len(self.multi_value_cols)} multi-value columns: {self.multi_value_cols}\")\n",
    "        \n",
    "        # Process each multi-value column\n",
    "        for col in self.multi_value_cols:\n",
    "            # Prepare data for MultiLabelBinarizer\n",
    "            values = df[col].dropna().astype(str).apply(\n",
    "                lambda x: [item.strip() for item in x.split(';') if item.strip()]\n",
    "            )\n",
    "            \n",
    "            # Handle empty values - fill with empty list for MultiLabelBinarizer\n",
    "            if len(values) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Fit and transform\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            \n",
    "            # Convert to list of lists, handling NaN/empty cases\n",
    "            values_list = []\n",
    "            for idx in df.index:\n",
    "                if idx in values.index and values[idx]:\n",
    "                    values_list.append(values[idx])\n",
    "                else:\n",
    "                    values_list.append([])  # Empty list for missing values\n",
    "            \n",
    "            onehot = pd.DataFrame(\n",
    "                mlb.fit_transform(values_list),\n",
    "                columns=[f\"{col}__{cat}\" for cat in mlb.classes_],\n",
    "                index=df.index\n",
    "            )\n",
    "            \n",
    "            # Store transformer for later use\n",
    "            self.mlb_transformers[col] = mlb\n",
    "            \n",
    "            # Join with main dataframe\n",
    "            df = df.join(onehot, how='left')\n",
    "            \n",
    "            print(f\"Encoded {col} into {len(mlb.classes_)} binary columns\")\n",
    "        \n",
    "        # Remove original multi-value columns\n",
    "        df = df.drop(columns=self.multi_value_cols)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 6. CategoricalEncoder: One-hot encode regular categorical columns ===\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Handle single-value categorical columns\n",
    "        - Ignores semicolon columns.\n",
    "        - Only encodes columns with a number of categories ≤ max_cardinality (to avoid high-dimensional explosion).\n",
    "        - Can drop the first category for each variable to avoid multicollinearity.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_cardinality=10, drop_first=True):\n",
    "        self.max_cardinality = max_cardinality\n",
    "        self.drop_first = drop_first\n",
    "        self.categorical_cols = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Encode categorical columns\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Identify categorical columns\n",
    "        cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Identify semicolon columns to exclude\n",
    "        semicolon_cols = [\n",
    "            col for col in df.columns\n",
    "            if df[col].dropna().astype(str).str.contains(';').any()\n",
    "        ]\n",
    "        \n",
    "        # Filter for low cardinality single-value categorical columns\n",
    "        self.categorical_cols = [\n",
    "            col for col in cat_cols \n",
    "            if col not in semicolon_cols and df[col].nunique() <= self.max_cardinality\n",
    "        ]\n",
    "        \n",
    "        print(f\"One-hot encoding {len(self.categorical_cols)} categorical columns: {self.categorical_cols}\")\n",
    "        \n",
    "        # Apply one-hot encoding\n",
    "        if self.categorical_cols:\n",
    "            df = pd.get_dummies(df, columns=self.categorical_cols, drop_first=self.drop_first)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 7. ColumnNameFixer: Final column name cleanup for PyCaret etc ===\n",
    "class ColumnNameFixer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Fix column names for PyCaret compatibility (removes illegal characters, replaces spaces/ampersands, handles duplicates):\n",
    "        - No duplicate column names after encoding.\n",
    "        - Only alphanumeric and underscores. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.column_transformations = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Fix problematic column names\"\"\"\n",
    "        df = X.copy()\n",
    "        original_cols = df.columns.tolist()\n",
    "        fixed_columns = []\n",
    "        seen_columns = set()\n",
    "        \n",
    "        for col in original_cols:\n",
    "            # Replace spaces with underscores\n",
    "            fixed_col = col.replace(' ', '_')\n",
    "            # Replace ampersands\n",
    "            fixed_col = fixed_col.replace('&', 'and')\n",
    "            # Remove other problematic characters\n",
    "            fixed_col = ''.join(c if c.isalnum() or c == '_' else '_' for c in fixed_col)\n",
    "            # Remove multiple consecutive underscores\n",
    "            fixed_col = re.sub('_+', '_', fixed_col)\n",
    "            # Remove leading/trailing underscores\n",
    "            fixed_col = fixed_col.strip('_')\n",
    "            \n",
    "            # Handle duplicates\n",
    "            base_col = fixed_col\n",
    "            suffix = 1\n",
    "            while fixed_col in seen_columns:\n",
    "                fixed_col = f\"{base_col}_{suffix}\"\n",
    "                suffix += 1\n",
    "            \n",
    "            seen_columns.add(fixed_col)\n",
    "            fixed_columns.append(fixed_col)\n",
    "        \n",
    "        # Store transformations\n",
    "        self.column_transformations = dict(zip(original_cols, fixed_columns))\n",
    "        \n",
    "        # Apply new column names\n",
    "        df.columns = fixed_columns\n",
    "        \n",
    "        # Check for duplicates\n",
    "        dup_check = [item for item, count in pd.Series(fixed_columns).value_counts().items() if count > 1]\n",
    "        if dup_check:\n",
    "            print(f\"WARNING: Found {len(dup_check)} duplicate column names: {dup_check}\")\n",
    "        else:\n",
    "            print(\"No duplicate column names after fixing\")\n",
    "        \n",
    "        n_changed = sum(1 for old, new in self.column_transformations.items() if old != new)\n",
    "        print(f\"Fixed {n_changed} column names for PyCaret compatibility\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 8. DataValidator: Final summary and checks ===\n",
    "class DataValidator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Validate final dataset\n",
    "        - Shape, missing values, infinities.\n",
    "        - Data types (numeric, categorical).\n",
    "        - Stats on the target column (mean, std, min, max, missing).\n",
    "        - Report issues if any.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_col):\n",
    "        self.target_col = target_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Validate the processed dataset\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        print(f\"\\n=== Final Data Validation ===\")\n",
    "        print(f\"Final shape: {df.shape}\")\n",
    "        print(f\"Target column: {self.target_col}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_count = df.isnull().sum().sum()\n",
    "        print(f\"Total missing values: {missing_count}\")\n",
    "        \n",
    "        # Check for infinite values\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        inf_count = np.isinf(df[numeric_cols].values).sum()\n",
    "        print(f\"Total infinite values: {inf_count}\")\n",
    "        \n",
    "        # Data types summary\n",
    "        print(f\"\\nData types:\")\n",
    "        print(f\"  Numeric columns: {len(df.select_dtypes(include=[np.number]).columns)}\")\n",
    "        print(f\"  Categorical columns: {len(df.select_dtypes(include=['object', 'category']).columns)}\")\n",
    "        \n",
    "        # Target variable summary\n",
    "        if self.target_col in df.columns:\n",
    "            target_stats = df[self.target_col].describe()\n",
    "            print(f\"\\nTarget variable '{self.target_col}' statistics:\")\n",
    "            print(f\"  Mean: {target_stats['mean']:.2f}\")\n",
    "            print(f\"  Std: {target_stats['std']:.2f}\")\n",
    "            print(f\"  Min: {target_stats['min']:.2f}\")\n",
    "            print(f\"  Max: {target_stats['max']:.2f}\")\n",
    "            print(f\"  Missing: {df[self.target_col].isnull().sum()}\")\n",
    "        else:\n",
    "            print(f\"WARNING: Target column '{self.target_col}' not found!\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === Pipeline creation function: returns the Scikit-learn pipeline ===\n",
    "def create_isbsg_preprocessing_pipeline(\n",
    "    target_col='project_prf_normalised_work_effort',\n",
    "    original_target_col=None,\n",
    "    high_missing_threshold=0.7,\n",
    "    cols_to_keep=None,\n",
    "    max_categorical_cardinality=10,\n",
    "    standardization_mapping=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create complete preprocessing pipeline with smart target column handling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    target_col : str\n",
    "        Name of target column\n",
    "    original_target_col : str\n",
    "        Original target column name found in data\n",
    "    high_missing_threshold : float\n",
    "        Threshold for dropping columns with high missing values\n",
    "    cols_to_keep : list\n",
    "        Columns to keep even if they have high missing values\n",
    "    max_categorical_cardinality : int\n",
    "        Maximum number of unique values for categorical encoding\n",
    "    standardization_mapping : dict\n",
    "        Custom mapping for standardizing semicolon-separated values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    sklearn.pipeline.Pipeline\n",
    "        Complete preprocessing pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    if cols_to_keep is None:\n",
    "        cols_to_keep = [\n",
    "            'project_prf_case_tool_used', \n",
    "            'process_pmf_prototyping_used',\n",
    "            'tech_tf_client_roles', \n",
    "            'tech_tf_type_of_server', \n",
    "            'tech_tf_clientserver_description'\n",
    "        ]\n",
    "    \n",
    "    # Ensure max_categorical_cardinality is an integer\n",
    "    if not isinstance(max_categorical_cardinality, int):\n",
    "        max_categorical_cardinality = 10\n",
    "        print(f\"Warning: max_categorical_cardinality was not an integer, defaulting to {max_categorical_cardinality}\")\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('column_standardizer', ColumnNameStandardizer(target_col, original_target_col)),\n",
    "        ('missing_handler', MissingValueAnalyzer(\n",
    "            high_missing_threshold=high_missing_threshold,\n",
    "            cols_to_keep=cols_to_keep\n",
    "        )),\n",
    "        ('semicolon_processor', SemicolonProcessor(standardization_mapping=standardization_mapping)),\n",
    "        ('multi_value_encoder', MultiValueEncoder(max_cardinality=max_categorical_cardinality)),\n",
    "        ('categorical_encoder', CategoricalEncoder(max_cardinality=max_categorical_cardinality)),\n",
    "        ('column_fixer', ColumnNameFixer()),\n",
    "        ('validator', DataValidator(target_col))\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# === Full workflow function: orchestrates loading, pipeline, and saving ===\n",
    "def preprocess_isbsg_data(\n",
    "    file_path,\n",
    "    target_col='project_prf_normalised_work_effort',  # Always use standardized form\n",
    "    output_dir='../data',\n",
    "    save_intermediate=True,\n",
    "    **pipeline_kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete preprocessing workflow for ISBSG data: loads the data, runs \n",
    "      the full preprocessing pipeline, saves processed data, pipeline \n",
    "      object, and a metadata report to disk, and returns the processed \n",
    "      DataFrame and metadata\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to input data file\n",
    "    target_col : str\n",
    "        Name of target column\n",
    "    output_dir : str\n",
    "        Directory to save processed data\n",
    "    save_intermediate : bool\n",
    "        Whether to save intermediate processing steps\n",
    "    **pipeline_kwargs : dict\n",
    "        Additional arguments for pipeline creation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Processed dataframe ready for modeling\n",
    "    dict\n",
    "        Processing metadata and statistics\n",
    "    \"\"\"\n",
    "\n",
    "    # print pipeline header\n",
    "    print(\"=\"*60)\n",
    "    print(\"ISBSG Data Preprocessing Pipeline\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    print(f\"Target column (standardized): {target_col}\")\n",
    "    print(f\"Timestamp: {datetime.now()}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load data with smart column detection\n",
    "    loader = DataLoader(file_path, target_col)\n",
    "    df_raw = loader.transform(X = None)\n",
    "    \n",
    "    # Create and fit preprocessing pipeline\n",
    "    pipeline = create_isbsg_preprocessing_pipeline(\n",
    "        target_col=target_col,\n",
    "        original_target_col=loader.original_target_col,  # Pass the found column name\n",
    "        **pipeline_kwargs\n",
    "    )\n",
    "    \n",
    "    # Apply preprocessing in order of ColumnNameStandardizer=> MissingValueAnalyzer =>\n",
    "    # SemicolonProcessor=> MultiValueEncoder=> CategoricalEncoder => ColumnNameFixer\n",
    "\n",
    "    # Apply preprocessing\n",
    "    df_processed = pipeline.fit_transform(df_raw)\n",
    "    \n",
    "    # Prepare metadata\n",
    "    metadata = {\n",
    "        'original_shape': loader.original_shape,\n",
    "        'processed_shape': df_processed.shape,\n",
    "        'processing_timestamp': datetime.now().isoformat(),\n",
    "        'target_column_standardized': target_col,\n",
    "        'target_column_original': loader.original_target_col,\n",
    "        'pipeline_steps': [step[0] for step in pipeline.steps]\n",
    "    }\n",
    "    \n",
    "    # Save processed data\n",
    "    file_stem = Path(file_path).stem\n",
    "    output_path = os.path.join(output_dir, f\"{file_stem}_preprocessed.csv\")\n",
    "    df_processed.to_csv(output_path, index=False)\n",
    "    print(f\"\\nProcessed data saved to: {output_path}\")\n",
    "    \n",
    "    # Save pipeline\n",
    "    pipeline_path = os.path.join(output_dir, f\"{file_stem}_preprocessing_pipeline.pkl\")\n",
    "    joblib.dump(pipeline, pipeline_path)\n",
    "    print(f\"Pipeline saved to: {pipeline_path}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = os.path.join(output_dir, f\"{file_stem}_preprocessing_metadata.txt\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        f.write(\"ISBSG Data Preprocessing Metadata\\n\")\n",
    "        f.write(\"=\"*40 + \"\\n\")\n",
    "        for key, value in metadata.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    print(f\"Metadata saved to: {metadata_path}\")\n",
    "\n",
    "    # Print completion & return results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Preprocessing completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return df_processed, metadata\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62895741-3d44-4748-982b-811d038539e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ISBSG Data Preprocessing Pipeline\n",
      "============================================================\n",
      "Processing file: ../data\\sample_clean_a_agile_only.xlsx\n",
      "Target column (standardized): Project_PRF_Normalised_Work_Effort\n",
      "Timestamp: 2025-05-31 17:32:41.635778\n",
      "Loading data from: ../data\\sample_clean_a_agile_only.xlsx\n",
      "Loaded data with shape: (78, 52)\n",
      "Target column found: 'Project_PRF_Normalised Work Effort' -> will be standardized to 'Project_PRF_Normalised_Work_Effort'\n",
      "Target column 'Project_PRF_Normalised Work Effort' -> 'Project_PRF_Normalised_Work_Effort'\n",
      "Standardized 52 column names\n",
      "\n",
      "Missing value analysis:\n",
      "Columns with >50% missing: 25\n",
      "Columns with >70% missing: 18\n",
      "Dropping 18 columns with >70.0% missing values\n",
      "Filled project_prf_functional_size missing values with median: 82.0\n",
      "Filled project_prf_normalised_level_1_pdr_ufp missing values with median: 3.5\n",
      "Filled project_prf_normalised_pdr_ufp missing values with median: 3.5\n",
      "Filled project_prf_defect_density missing values with median: 0.0\n",
      "Filled project_prf_speed_of_delivery missing values with median: 28.6\n",
      "Filled project_prf_manpower_delivery_rate missing values with median: 2.1\n",
      "Filled project_prf_project_elapsed_time missing values with median: 5.0\n",
      "Filled project_prf_max_team_size missing values with median: 2.0\n",
      "Filled people_prf_project_manage_changes missing values with median: 0.0\n",
      "Filled people_prf_personnel_changes missing values with median: 0.0\n",
      "Filled project_prf_total_project_cost missing values with median: 60775.0\n",
      "Data shape after missing value handling: (78, 34)\n",
      "Found 3 columns with semicolons: ['external_eef_organisation_type', 'project_prf_application_type', 'process_pmf_development_methodologies']\n",
      "Encoding 0 multi-value columns: []\n",
      "One-hot encoding 13 categorical columns: ['external_eef_data_quality_rating', 'project_prf_application_group', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'tech_tf_primary_programming_language', 'project_prf_relative_size', 'project_prf_team_size_group', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_web_development', 'tech_tf_dbms_used', 'project_prf_cost_currency']\n",
      "No duplicate column names after fixing\n",
      "Fixed 19 column names for PyCaret compatibility\n",
      "\n",
      "=== Final Data Validation ===\n",
      "Final shape: (78, 69)\n",
      "Target column: Project_PRF_Normalised_Work_Effort\n",
      "Total missing values: 0\n",
      "Total infinite values: 0\n",
      "\n",
      "Data types:\n",
      "  Numeric columns: 17\n",
      "  Categorical columns: 4\n",
      "\n",
      "Target variable 'Project_PRF_Normalised_Work_Effort' statistics:\n",
      "  Mean: 3362.71\n",
      "  Std: 10643.13\n",
      "  Min: 6.00\n",
      "  Max: 60047.00\n",
      "  Missing: 0\n",
      "\n",
      "Processed data saved to: ../data\\sample_clean_a_agile_only_preprocessed.csv\n",
      "Pipeline saved to: ../data\\sample_clean_a_agile_only_preprocessing_pipeline.pkl\n",
      "Metadata saved to: ../data\\sample_clean_a_agile_only_preprocessing_metadata.txt\n",
      "\n",
      "============================================================\n",
      "Preprocessing completed successfully!\n",
      "============================================================\n",
      "\n",
      "Final dataset shape: (78, 69)\n",
      "Columns: ['isbsg_project_id', 'project_prf_year_of_project', 'external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_type', 'project_prf_functional_size', 'project_prf_normalised_work_effort_level_1', 'Project_PRF_Normalised_Work_Effort', 'project_prf_normalised_level_1_pdr_ufp', 'project_prf_normalised_pdr_ufp']...\n",
      "\n",
      "Dataset is now ready for PyCaret setup!\n"
     ]
    }
   ],
   "source": [
    "# Execution: usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # File path\n",
    "    file_path = os.path.join(DATA_FOLDER, SAMPLE_FILE)\n",
    "    \n",
    "    # Custom configuration\n",
    "    cols_to_keep = [\n",
    "        'Project_PRF_CASE_Tool_Used', \n",
    "        'Process_PMF_Prototyping_Used',\n",
    "        'Tech_TF_Client_Roles', \n",
    "        'Tech_TF_Type_of_Server', \n",
    "        'Tech_TF_ClientServer_Description'\n",
    "    ]\n",
    "    \n",
    "    # Specific standardization rules for individual components (after cleaning)\n",
    "    standardization_map = {\n",
    "        'stand alone': 'stand-alone',\n",
    "        'client server': 'client-server',\n",
    "        'mathematically intensive': 'mathematically-intensive',\n",
    "        'mathematically intensive application': 'mathematically-intensive application',\n",
    "        \"file &/or print server\": \"file/print server\",\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Run preprocessing\n",
    "        df_processed, metadata = preprocess_isbsg_data(\n",
    "            file_path=file_path,\n",
    "            target_col=TARGET_COL,\n",
    "            output_dir=DATA_FOLDER,\n",
    "            cols_to_keep=cols_to_keep,\n",
    "            max_categorical_cardinality=10,\n",
    "            standardization_mapping=standardization_map,\n",
    "            high_missing_threshold=0.7\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nFinal dataset shape: {df_processed.shape}\")\n",
    "        print(f\"Columns: {list(df_processed.columns[:10])}...\")  # Show first 10 columns\n",
    "        \n",
    "        # Ready for PyCaret setup\n",
    "        print(\"\\nDataset is now ready for PyCaret setup!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47df44c0-f73a-479d-98f1-3e7886cef03f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb8ebb-72c8-4b45-856f-64e21740151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick preprocessing\n",
    "df_processed, metadata = preprocess_isbsg_data(\n",
    "    file_path=\"your_data.xlsx\",\n",
    "    target_col=\"Project_PRF_Normalised_Work_Effort\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49625c8-083f-4e98-ae0c-fb5982e600e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With custom settings\n",
    "df_processed, metadata = preprocess_isbsg_data(\n",
    "    file_path=\"your_data.xlsx\",\n",
    "    target_col=\"Project_PRF_Normalised_Work_Effort\",\n",
    "    high_missing_threshold=0.8,  # More lenient with missing values\n",
    "    max_categorical_cardinality=15,  # Allow higher cardinality\n",
    "    cols_to_keep=['specific_column_to_keep']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
