{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f30650-ed19-4abf-bb03-5ef4005a32cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e08f925-be6c-46dc-92b4-da99269eda09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Complete Scikit-Learn Preprocessing Pipeline for ISBSG Data\\n    ===========================================================\\n\\n    This module provides a comprehensive preprocessing pipeline that handles:\\n    1. Data loading and initial cleaning\\n    2. Column name standardization\\n    3. Missing value handling\\n    4. Semicolon-separated value processing\\n    5. One-hot encoding for categorical variables\\n    6. Multi-label binarization for multi-value columns\\n    7. Feature selection and filtering\\n    8. Data validation and export\\n\\n    Based on the preprocessing steps from the provided notebooks.\\n   '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Complete Scikit-Learn Preprocessing Pipeline for ISBSG Data\n",
    "    ===========================================================\n",
    "    \n",
    "    This module provides a comprehensive preprocessing pipeline that handles:\n",
    "    1. Data loading and initial cleaning\n",
    "    2. Column name standardization\n",
    "    3. Missing value handling\n",
    "    4. Semicolon-separated value processing\n",
    "    5. One-hot encoding for categorical variables\n",
    "    6. Multi-label binarization for multi-value columns\n",
    "    7. Feature selection and filtering\n",
    "    8. Data validation and export\n",
    "    \n",
    "    Based on the preprocessing steps from the provided notebooks.\n",
    "   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2848fe58-b966-47fe-9339-6eea3764c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Any, Tuple\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b649e731-d0f9-4bf8-95ed-428c9c818aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_FOLDER = \"../data\"\n",
    "CONFIG_FOLDER = \"../config\"\n",
    "SAMPLE_FILE = \"ISBSG2016R1_1_financial_industry_seed.xlsx\"\n",
    "FULL_FILE = \"ISBSG2016R1_1_full_dataset.xlsx\"\n",
    "TARGET_COL = \"project_prf_normalised_work_effort\"  # be careful about case sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9986b36b-4364-4284-9381-c6ae3d99abd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def analyze_high_cardinality_multivalue(df, column, separator=';'):\n",
    "    \"\"\"\n",
    "    Analyze high-cardinality multi-value columns to choose best strategy\n",
    "    \"\"\"\n",
    "    print(f\"=== ANALYSIS FOR HIGH-CARDINALITY COLUMN: '{column}' ===\\n\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    non_null_data = df[column].dropna().astype(str)\n",
    "    split_values = non_null_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Get all unique values\n",
    "    all_values = []\n",
    "    for values_list in split_values:\n",
    "        all_values.extend(values_list)\n",
    "    \n",
    "    value_counts = Counter(all_values)\n",
    "    unique_values = list(value_counts.keys())\n",
    "    \n",
    "    print(f\"Total unique values: {len(unique_values)}\")\n",
    "    print(f\"Total value occurrences: {len(all_values)}\")\n",
    "    print(f\"Average values per row: {len(all_values) / len(split_values):.2f}\")\n",
    "    \n",
    "    # Show most common values\n",
    "    print(f\"\\nTop 15 most common values:\")\n",
    "    for value, count in value_counts.most_common(15):\n",
    "        percentage = (count / len(non_null_data)) * 100\n",
    "        print(f\"  '{value}': {count} times ({percentage:.1f}% of rows)\")\n",
    "    \n",
    "    # Show distribution of value frequencies\n",
    "    frequency_dist = Counter(value_counts.values())\n",
    "    print(f\"\\nFrequency distribution:\")\n",
    "    for freq, count in sorted(frequency_dist.items(), reverse=True)[:10]:\n",
    "        print(f\"  {count} values appear {freq} time(s)\")\n",
    "    \n",
    "    # Values per row distribution\n",
    "    values_per_row = split_values.apply(len)\n",
    "    print(f\"\\nValues per row:\")\n",
    "    print(f\"  Min: {values_per_row.min()}\")\n",
    "    print(f\"  Max: {values_per_row.max()}\")\n",
    "    print(f\"  Mean: {values_per_row.mean():.2f}\")\n",
    "    print(f\"  Median: {values_per_row.median():.2f}\")\n",
    "    \n",
    "    return value_counts, unique_values\n",
    "\n",
    "\n",
    "def handle_high_cardinality_multivalue(df, multi_value_columns, separator=';', strategy='top_k', preserve_original=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Handle high-cardinality multi-value columns with various strategies\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    strategy options:\n",
    "    - 'top_k': Keep only top K most frequent values (k=kwargs['k'])\n",
    "    - 'frequency_threshold': Keep values that appear in at least X% of rows (threshold=kwargs['threshold'])\n",
    "    - 'tfidf': Use TF-IDF vectorization with dimensionality reduction (n_components=kwargs['n_components'])\n",
    "    - 'count_features': Simple counting features (count, unique_count, most_common)\n",
    "    - 'embedding': Create category embeddings (requires pre-trained embeddings)\n",
    "    \"\"\"\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    new_columns_mapping = {}\n",
    "    preserve_original = preserve_original or []\n",
    "    \n",
    "    for col in multi_value_columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing high-cardinality column '{col}' with strategy '{strategy}'...\")\n",
    "        \n",
    "        # Clean and split values\n",
    "        split_values = df[col].fillna('').astype(str).apply(\n",
    "            lambda x: [val.strip() for val in x.split(separator) if val.strip()]\n",
    "        )\n",
    "        \n",
    "        # Get value counts\n",
    "        all_values = []\n",
    "        for values_list in split_values:\n",
    "            all_values.extend(values_list)\n",
    "        value_counts = Counter(all_values)\n",
    "        \n",
    "        if strategy == 'top_k':\n",
    "            k = kwargs.get('k', 20)  # Default to top 20\n",
    "            top_values = [val for val, count in value_counts.most_common(k)]\n",
    "            \n",
    "            new_col_names = []\n",
    "            for value in top_values:\n",
    "                new_col_name = f\"{col}_top_{value}\".replace(' ', '_').replace('-', '_')\n",
    "                df_processed[new_col_name] = split_values.apply(lambda x: 1 if value in x else 0)\n",
    "                new_col_names.append(new_col_name)\n",
    "            \n",
    "            # Add \"other\" category for remaining values\n",
    "            other_col_name = f\"{col}_other\"\n",
    "            df_processed[other_col_name] = split_values.apply(\n",
    "                lambda x: 1 if any(val not in top_values for val in x) else 0\n",
    "            )\n",
    "            new_col_names.append(other_col_name)\n",
    "            \n",
    "            new_columns_mapping[col] = new_col_names\n",
    "            print(f\"  Created {len(new_col_names)} columns (top {k} + other)\")\n",
    "            \n",
    "        elif strategy == 'frequency_threshold':\n",
    "            threshold = kwargs.get('threshold', 0.05)  # Default 5%\n",
    "            min_occurrences = int(len(df) * threshold)\n",
    "            \n",
    "            frequent_values = [val for val, count in value_counts.items() if count >= min_occurrences]\n",
    "            \n",
    "            new_col_names = []\n",
    "            for value in frequent_values:\n",
    "                new_col_name = f\"{col}_freq_{value}\".replace(' ', '_').replace('-', '_')\n",
    "                df_processed[new_col_name] = split_values.apply(lambda x: 1 if value in x else 0)\n",
    "                new_col_names.append(new_col_name)\n",
    "            \n",
    "            # Add rare category\n",
    "            rare_col_name = f\"{col}_rare\"\n",
    "            df_processed[rare_col_name] = split_values.apply(\n",
    "                lambda x: 1 if any(val not in frequent_values for val in x) else 0\n",
    "            )\n",
    "            new_col_names.append(rare_col_name)\n",
    "            \n",
    "            new_columns_mapping[col] = new_col_names\n",
    "            print(f\"  Created {len(new_col_names)} columns ({len(frequent_values)} frequent + rare)\")\n",
    "            \n",
    "        elif strategy == 'count_features':\n",
    "            # Create aggregate features instead of individual columns\n",
    "            new_col_names = []\n",
    "            \n",
    "            # Total count of values\n",
    "            count_col = f\"{col}_count\"\n",
    "            df_processed[count_col] = split_values.apply(len)\n",
    "            new_col_names.append(count_col)\n",
    "            \n",
    "            # Unique count (in case of duplicates)\n",
    "            unique_count_col = f\"{col}_unique_count\"\n",
    "            df_processed[unique_count_col] = split_values.apply(lambda x: len(set(x)))\n",
    "            new_col_names.append(unique_count_col)\n",
    "            \n",
    "            # Most common value in the dataset appears in this row\n",
    "            most_common_value = value_counts.most_common(1)[0][0] if value_counts else None\n",
    "            if most_common_value:\n",
    "                most_common_col = f\"{col}_has_most_common\"\n",
    "                df_processed[most_common_col] = split_values.apply(lambda x: 1 if most_common_value in x else 0)\n",
    "                new_col_names.append(most_common_col)\n",
    "            \n",
    "            # Average frequency of values in this row\n",
    "            avg_freq_col = f\"{col}_avg_frequency\"\n",
    "            df_processed[avg_freq_col] = split_values.apply(\n",
    "                lambda x: np.mean([value_counts[val] for val in x]) if x else 0\n",
    "            )\n",
    "            new_col_names.append(avg_freq_col)\n",
    "            \n",
    "            new_columns_mapping[col] = new_col_names\n",
    "            print(f\"  Created {len(new_col_names)} aggregate feature columns\")\n",
    "            \n",
    "        elif strategy == 'tfidf':\n",
    "            n_components = kwargs.get('n_components', 10)  # Default to 10 components\n",
    "            \n",
    "            # Convert to text format for TF-IDF\n",
    "            text_data = split_values.apply(lambda x: ' '.join(x))\n",
    "            \n",
    "            # Apply TF-IDF\n",
    "            tfidf = TfidfVectorizer(max_features=100, stop_words=None)\n",
    "            tfidf_matrix = tfidf.fit_transform(text_data)\n",
    "            \n",
    "            # Reduce dimensionality\n",
    "            pca = PCA(n_components=n_components)\n",
    "            tfidf_reduced = pca.fit_transform(tfidf_matrix.toarray())\n",
    "            \n",
    "            # Create new columns\n",
    "            new_col_names = []\n",
    "            for i in range(n_components):\n",
    "                new_col_name = f\"{col}_tfidf_comp_{i+1}\"\n",
    "                df_processed[new_col_name] = tfidf_reduced[:, i]\n",
    "                new_col_names.append(new_col_name)\n",
    "            \n",
    "            new_columns_mapping[col] = new_col_names\n",
    "            print(f\"  Created {len(new_col_names)} TF-IDF component columns\")\n",
    "            print(f\"  Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "            \n",
    "        elif strategy == 'hierarchical':\n",
    "            # Group similar values into higher-level categories\n",
    "            # This requires domain knowledge - example implementation\n",
    "            hierarchy = kwargs.get('hierarchy', {})  # Dictionary mapping values to categories\n",
    "            \n",
    "            if not hierarchy:\n",
    "                print(\"  Warning: No hierarchy provided for hierarchical strategy\")\n",
    "                continue\n",
    "            \n",
    "            # Create columns for each high-level category\n",
    "            categories = set(hierarchy.values())\n",
    "            new_col_names = []\n",
    "            \n",
    "            for category in categories:\n",
    "                category_values = [val for val, cat in hierarchy.items() if cat == category]\n",
    "                new_col_name = f\"{col}_category_{category}\".replace(' ', '_')\n",
    "                df_processed[new_col_name] = split_values.apply(\n",
    "                    lambda x: 1 if any(val in category_values for val in x) else 0\n",
    "                )\n",
    "                new_col_names.append(new_col_name)\n",
    "            \n",
    "            new_columns_mapping[col] = new_col_names\n",
    "            print(f\"  Created {len(new_col_names)} hierarchical category columns\")\n",
    "        \n",
    "        # FIXED: Only remove original column if NOT in preserve list\n",
    "        if col not in preserve_original:\n",
    "            df_processed = df_processed.drop(columns=[col])\n",
    "            print(f\"  Removed original column '{col}'\")\n",
    "        else:\n",
    "            print(f\"  ✅ Preserved original column '{col}' (in exclude list)\")\n",
    "    \n",
    "    return df_processed, new_columns_mapping\n",
    "\n",
    "\n",
    "def recommend_strategy(df, column, separator=';'):\n",
    "    \"\"\"\n",
    "    Recommend the best strategy based on data characteristics\n",
    "    \"\"\"\n",
    "    value_counts, unique_values = analyze_high_cardinality_multivalue(df, column, separator)\n",
    "    \n",
    "    total_unique = len(unique_values)\n",
    "    total_rows = len(df[column].dropna())\n",
    "    \n",
    "    print(f\"\\n=== STRATEGY RECOMMENDATIONS FOR '{column}' ===\")\n",
    "    \n",
    "    if total_unique > 100:\n",
    "        print(\"🔴 VERY HIGH CARDINALITY (100+ unique values)\")\n",
    "        print(\"Recommended strategies:\")\n",
    "        print(\"1. 'count_features' - Create aggregate features (safest)\")\n",
    "        print(\"2. 'top_k' with k=15-25 - Keep only most important values\")\n",
    "        print(\"3. 'tfidf' with n_components=5-10 - If values have semantic meaning\")\n",
    "        \n",
    "    elif total_unique > 50:\n",
    "        print(\"🟡 HIGH CARDINALITY (50+ unique values)\")\n",
    "        print(\"Recommended strategies:\")\n",
    "        print(\"1. 'top_k' with k=20-30 - Keep most frequent values\")\n",
    "        print(\"2. 'frequency_threshold' with threshold=0.02-0.05\")\n",
    "        print(\"3. 'count_features' - If you want aggregate information\")\n",
    "        \n",
    "    else:\n",
    "        print(\"🟢 MODERATE CARDINALITY (<50 unique values)\")\n",
    "        print(\"Recommended strategies:\")\n",
    "        print(\"1. 'frequency_threshold' with threshold=0.01\")\n",
    "        print(\"2. 'top_k' with k=30-40\")\n",
    "        print(\"3. Binary encoding might be acceptable\")\n",
    "    \n",
    "    # Check frequency distribution\n",
    "    freq_values = list(value_counts.values())\n",
    "    if max(freq_values) / min(freq_values) > 100:\n",
    "        print(\"\\n⚠️  HIGHLY SKEWED DISTRIBUTION detected\")\n",
    "        print(\"   Consider 'frequency_threshold' or 'top_k' strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24140d94-4ab5-4228-aa1f-cc0fc2081b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_multivalue_processing(df_original, df_processed, original_column, new_columns, separator=';', strategy='top_k'):\n",
    "    \"\"\"\n",
    "    Comprehensive validation of multi-value categorical processing\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_original : pd.DataFrame\n",
    "        Original dataset before processing\n",
    "    df_processed : pd.DataFrame  \n",
    "        Processed dataset after handling multi-value columns\n",
    "    original_column : str\n",
    "        Name of original multi-value column\n",
    "    new_columns : list\n",
    "        List of new column names created from the original column\n",
    "    separator : str\n",
    "        Separator used in original data\n",
    "    strategy : str\n",
    "        Strategy used for processing\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"=== VALIDATION REPORT FOR COLUMN '{original_column}' ===\\n\")\n",
    "    \n",
    "    # 1. BASIC CHECKS\n",
    "    print(\"1. BASIC INTEGRITY CHECKS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check row count consistency\n",
    "    original_rows = len(df_original)\n",
    "    processed_rows = len(df_processed)\n",
    "    print(f\"✓ Row count: {original_rows} → {processed_rows} {'✓ SAME' if original_rows == processed_rows else '⚠️  DIFFERENT'}\")\n",
    "    \n",
    "    # Check if original column was removed\n",
    "    original_removed = original_column not in df_processed.columns\n",
    "    print(f\"✓ Original column removed: {'✓ YES' if original_removed else '⚠️  NO'}\")\n",
    "    \n",
    "    # Check if new columns exist\n",
    "    new_cols_exist = all(col in df_processed.columns for col in new_columns)\n",
    "    print(f\"✓ New columns created: {'✓ YES' if new_cols_exist else '❌ NO'} ({len(new_columns)} columns)\")\n",
    "    \n",
    "    if not new_cols_exist:\n",
    "        missing_cols = [col for col in new_columns if col not in df_processed.columns]\n",
    "        print(f\"  Missing columns: {missing_cols}\")\n",
    "        return False\n",
    "    \n",
    "    # 2. DATA CONSISTENCY CHECKS\n",
    "    print(f\"\\n2. DATA CONSISTENCY CHECKS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Get all unique values from original\n",
    "    all_original_values = set()\n",
    "    for values_list in split_original:\n",
    "        all_original_values.update(values_list)\n",
    "    all_original_values = sorted([v for v in all_original_values if v and v != 'nan'])\n",
    "    \n",
    "    print(f\"Original unique values: {len(all_original_values)}\")\n",
    "    \n",
    "    if strategy == 'top_k':\n",
    "        # Validate top-k strategy\n",
    "        validate_top_k_strategy(df_original, df_processed, original_column, new_columns, separator)\n",
    "    elif strategy == 'count_features':\n",
    "        validate_count_features_strategy(df_original, df_processed, original_column, new_columns, separator)\n",
    "    elif strategy == 'frequency_threshold':\n",
    "        validate_frequency_threshold_strategy(df_original, df_processed, original_column, new_columns, separator)\n",
    "    \n",
    "    # 3. SAMPLE VALIDATION\n",
    "    print(f\"\\n3. SAMPLE-BY-SAMPLE VALIDATION\")\n",
    "    print(\"-\" * 40)\n",
    "    validate_sample_rows(df_original, df_processed, original_column, new_columns, separator, n_samples=5)\n",
    "    \n",
    "    # 4. STATISTICAL VALIDATION\n",
    "    print(f\"\\n4. STATISTICAL VALIDATION\")\n",
    "    print(\"-\" * 40)\n",
    "    validate_statistics(df_original, df_processed, original_column, new_columns, separator)\n",
    "    \n",
    "    # 5. INFORMATION LOSS ASSESSMENT\n",
    "    print(f\"\\n5. INFORMATION LOSS ASSESSMENT\")\n",
    "    print(\"-\" * 40)\n",
    "    assess_information_loss(df_original, df_processed, original_column, new_columns, separator)\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_top_k_strategy(df_original, df_processed, original_column, new_columns, separator, k=None):\n",
    "    \"\"\"Validate top-k strategy specifically\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Get value counts\n",
    "    all_values = []\n",
    "    for values_list in split_original:\n",
    "        all_values.extend(values_list)\n",
    "    value_counts = Counter(all_values)\n",
    "    \n",
    "    # Determine k if not provided\n",
    "    if k is None:\n",
    "        # Exclude \"other\" column to determine k\n",
    "        non_other_cols = [col for col in new_columns if not col.endswith('_other')]\n",
    "        k = len(non_other_cols)\n",
    "    \n",
    "    top_k_values = [val for val, count in value_counts.most_common(k)]\n",
    "    print(f\"Top {k} values: {top_k_values[:5]}{'...' if len(top_k_values) > 5 else ''}\")\n",
    "    \n",
    "    # Check each top-k column\n",
    "    for col in new_columns:\n",
    "        if col.endswith('_other'):\n",
    "            # Validate \"other\" column\n",
    "            validate_other_column(df_original, df_processed, original_column, col, top_k_values, separator)\n",
    "        else:\n",
    "            # Extract the value name from column name\n",
    "            value_name = col.replace(f\"{original_column}_top_\", \"\").replace(f\"{original_column}_\", \"\")\n",
    "            validate_binary_column(df_original, df_processed, original_column, col, value_name, separator)\n",
    "\n",
    "\n",
    "def validate_binary_column(df_original, df_processed, original_column, new_column, value_name, separator):\n",
    "    \"\"\"Validate a single binary column\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Expected values: 1 if value_name in the list, 0 otherwise\n",
    "    expected = split_original.apply(lambda x: 1 if value_name in x else 0)\n",
    "    actual = df_processed[new_column]\n",
    "    \n",
    "    # Compare\n",
    "    matches = (expected == actual).sum()\n",
    "    total = len(expected)\n",
    "    match_rate = matches / total * 100\n",
    "    \n",
    "    print(f\"  '{new_column}': {matches}/{total} matches ({match_rate:.1f}%)\")\n",
    "    \n",
    "    if match_rate < 100:\n",
    "        mismatches = df_original.loc[expected != actual, original_column].head(3)\n",
    "        print(f\"    Sample mismatches: {list(mismatches)}\")\n",
    "\n",
    "\n",
    "def validate_other_column(df_original, df_processed, original_column, other_column, top_values, separator):\n",
    "    \"\"\"Validate the 'other' category column\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Expected: 1 if any value is NOT in top_values, 0 if all values are in top_values\n",
    "    expected = split_original.apply(lambda x: 1 if any(val not in top_values for val in x) else 0)\n",
    "    actual = df_processed[other_column]\n",
    "    \n",
    "    matches = (expected == actual).sum()\n",
    "    total = len(expected)\n",
    "    match_rate = matches / total * 100\n",
    "    \n",
    "    print(f\"  '{other_column}': {matches}/{total} matches ({match_rate:.1f}%)\")\n",
    "\n",
    "\n",
    "def validate_count_features_strategy(df_original, df_processed, original_column, new_columns, separator):\n",
    "    \"\"\"Validate count features strategy\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    for col in new_columns:\n",
    "        if col.endswith('_count'):\n",
    "            # Validate total count\n",
    "            expected = split_original.apply(len)\n",
    "            actual = df_processed[col]\n",
    "            matches = (expected == actual).sum()\n",
    "            print(f\"  '{col}': {matches}/{len(expected)} matches ({matches/len(expected)*100:.1f}%)\")\n",
    "            \n",
    "        elif col.endswith('_unique_count'):\n",
    "            # Validate unique count\n",
    "            expected = split_original.apply(lambda x: len(set(x)))\n",
    "            actual = df_processed[col]\n",
    "            matches = (expected == actual).sum()\n",
    "            print(f\"  '{col}': {matches}/{len(expected)} matches ({matches/len(expected)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "def validate_frequency_threshold_strategy(df_original, df_processed, original_column, new_columns, separator):\n",
    "    \"\"\"Validate frequency threshold strategy\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Get value counts\n",
    "    all_values = []\n",
    "    for values_list in split_original:\n",
    "        all_values.extend(values_list)\n",
    "    value_counts = Counter(all_values)\n",
    "    \n",
    "    for col in new_columns:\n",
    "        if col.endswith('_rare'):\n",
    "            # Validate rare column - similar to other column validation\n",
    "            continue\n",
    "        else:\n",
    "            # Extract the value name from column name\n",
    "            value_name = col.replace(f\"{original_column}_freq_\", \"\").replace(f\"{original_column}_\", \"\")\n",
    "            validate_binary_column(df_original, df_processed, original_column, col, value_name, separator)\n",
    "\n",
    "\n",
    "def validate_sample_rows(df_original, df_processed, original_column, new_columns, separator, n_samples=5):\n",
    "    \"\"\"Manually validate a few sample rows\"\"\"\n",
    "    \n",
    "    print(f\"Validating {n_samples} random samples:\")\n",
    "    \n",
    "    # Get random sample indices\n",
    "    sample_indices = np.random.choice(len(df_original), min(n_samples, len(df_original)), replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices, 1):\n",
    "        original_value = df_original.iloc[idx][original_column]\n",
    "        if pd.isna(original_value):\n",
    "            original_values = []\n",
    "        else:\n",
    "            original_values = [v.strip() for v in str(original_value).split(separator) if v.strip()]\n",
    "        \n",
    "        print(f\"\\n  Sample {i} (row {idx}):\")\n",
    "        print(f\"    Original: '{original_value}'\")\n",
    "        print(f\"    Parsed: {original_values}\")\n",
    "        \n",
    "        # Check new columns for this row\n",
    "        for col in new_columns[:5]:  # Show first 5 columns only\n",
    "            processed_value = df_processed.iloc[idx][col]\n",
    "            print(f\"    {col}: {processed_value}\")\n",
    "\n",
    "\n",
    "def validate_statistics(df_original, df_processed, original_column, new_columns, separator):\n",
    "    \"\"\"Validate statistical properties\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Original statistics\n",
    "    values_per_row = split_original.apply(len)\n",
    "    print(f\"Original values per row - Mean: {values_per_row.mean():.2f}, Std: {values_per_row.std():.2f}\")\n",
    "    \n",
    "    # New data statistics\n",
    "    if any('_count' in col for col in new_columns):\n",
    "        count_col = [col for col in new_columns if col.endswith('_count')][0]\n",
    "        new_counts = df_processed[count_col]\n",
    "        print(f\"Processed counts - Mean: {new_counts.mean():.2f}, Std: {new_counts.std():.2f}\")\n",
    "        \n",
    "        # They should match!\n",
    "        correlation = np.corrcoef(values_per_row, new_counts)[0, 1]\n",
    "        print(f\"Correlation between original and processed counts: {correlation:.4f}\")\n",
    "    \n",
    "    # Check for any impossible values\n",
    "    binary_cols = [col for col in new_columns if not col.endswith(('_count', '_frequency', '_avg_frequency'))]\n",
    "    for col in binary_cols:\n",
    "        unique_vals = df_processed[col].unique()\n",
    "        if not set(unique_vals).issubset({0, 1, np.nan}):\n",
    "            print(f\"⚠️  Warning: Non-binary values in '{col}': {unique_vals}\")\n",
    "\n",
    "\n",
    "def assess_information_loss(df_original, df_processed, original_column, new_columns, separator):\n",
    "    \"\"\"Assess how much information was lost in the transformation\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Get all unique values\n",
    "    all_original_values = set()\n",
    "    for values_list in split_original:\n",
    "        all_original_values.update(values_list)\n",
    "    all_original_values = sorted([v for v in all_original_values if v and v != 'nan'])\n",
    "    \n",
    "    # Count how many unique values are captured by new columns\n",
    "    captured_values = set()\n",
    "    for col in new_columns:\n",
    "        if not col.endswith(('_other', '_count', '_unique_count', '_frequency', '_avg_frequency', '_rare')):\n",
    "            # Extract value name from column name\n",
    "            value_parts = col.replace(f\"{original_column}_\", \"\").replace(\"top_\", \"\").replace(\"freq_\", \"\")\n",
    "            captured_values.add(value_parts)\n",
    "    \n",
    "    capture_rate = len(captured_values) / len(all_original_values) * 100 if all_original_values else 0\n",
    "    print(f\"Value capture rate: {len(captured_values)}/{len(all_original_values)} ({capture_rate:.1f}%)\")\n",
    "    \n",
    "    if len(all_original_values) - len(captured_values) > 0:\n",
    "        lost_values = set(all_original_values) - captured_values\n",
    "        print(f\"Lost values (first 10): {list(lost_values)[:10]}\")\n",
    "    \n",
    "    # Estimate row-level information preservation\n",
    "    if any('_other' in col for col in new_columns):\n",
    "        other_col = [col for col in new_columns if col.endswith('_other')][0]\n",
    "        rows_with_other = df_processed[other_col].sum()\n",
    "        print(f\"Rows with 'other' values: {rows_with_other}/{len(df_processed)} ({rows_with_other/len(df_processed)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "def quick_validation_summary(df_original, df_processed, column_mapping):\n",
    "    \"\"\"Quick validation summary for all processed columns\"\"\"\n",
    "    \n",
    "    print(\"=== QUICK VALIDATION SUMMARY ===\\n\")\n",
    "    \n",
    "    for original_col, new_cols in column_mapping.items():\n",
    "        print(f\"✓ {original_col} → {len(new_cols)} new columns\")\n",
    "        \n",
    "        # Check for obvious issues\n",
    "        issues = []\n",
    "        \n",
    "        for col in new_cols:\n",
    "            if col not in df_processed.columns:\n",
    "                issues.append(f\"Missing column: {col}\")\n",
    "            else:\n",
    "                # Check for unexpected values in binary columns\n",
    "                if not col.endswith(('_count', '_frequency', '_avg_frequency')):\n",
    "                    unique_vals = set(df_processed[col].dropna().unique())\n",
    "                    if not unique_vals.issubset({0, 1, 0.0, 1.0}):\n",
    "                        issues.append(f\"Non-binary values in {col}: {unique_vals}\")\n",
    "        \n",
    "        if issues:\n",
    "            print(f\"  ⚠️  Issues: {issues}\")\n",
    "        else:\n",
    "            print(f\"  ✓ Looks good\")\n",
    "    \n",
    "    print(f\"\\nDataset size: {len(df_original)} → {len(df_processed)} rows\")\n",
    "    print(f\"Column count: {len(df_original.columns)} → {len(df_processed.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c7dfe5f-5eb6-4191-bcac-6e5370f5cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_categories_from_full_dataset(\n",
    "    sample_df, \n",
    "    full_df, \n",
    "    categorical_columns, \n",
    "    samples_per_category=2,\n",
    "    exclude_columns=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Add missing categorical values to sample dataset by sampling from full dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample_df : pd.DataFrame\n",
    "        Your limited sample dataset\n",
    "    full_df : pd.DataFrame  \n",
    "        Your complete dataset\n",
    "    categorical_columns : list\n",
    "        List of categorical column names\n",
    "    samples_per_category : int\n",
    "        Number of examples to add for each missing category\n",
    "    exclude_columns : list\n",
    "        Columns that should not get ANY new categories (even indirectly)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Enhanced dataset with missing categories included\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Analyzing missing categories...\")\n",
    "\n",
    "    # Apply exclusions if provided at this level\n",
    "    if exclude_columns:\n",
    "        categorical_columns = [col for col in categorical_columns \n",
    "                              if col not in exclude_columns]\n",
    "        print(f\"Excluded columns: {exclude_columns}\")\n",
    "    \n",
    "    # Find missing categories in sample compared to full dataset\n",
    "    missing_categories = {}\n",
    "    category_stats = {}\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col not in sample_df.columns or col not in full_df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in one of the datasets\")\n",
    "            continue\n",
    "            \n",
    "        full_categories = set(full_df[col].dropna().unique())\n",
    "        sample_categories = set(sample_df[col].dropna().unique())\n",
    "        missing = full_categories - sample_categories\n",
    "        \n",
    "        if missing:\n",
    "            missing_categories[col] = missing\n",
    "            category_stats[col] = {\n",
    "                'total_in_full': len(full_categories),\n",
    "                'in_sample': len(sample_categories),\n",
    "                'missing_count': len(missing)\n",
    "            }\n",
    "            print(f\"Column '{col}': Missing {len(missing)} out of {len(full_categories)} categories\")\n",
    "            print(f\"  Missing categories: {list(missing)[:5]}{'...' if len(missing) > 5 else ''}\")\n",
    "        else:\n",
    "            print(f\"Column '{col}': All categories present in sample\")\n",
    "    \n",
    "    if not missing_categories:\n",
    "        print(\"No missing categories found! Your sample already contains all category values.\")\n",
    "        return sample_df.copy()\n",
    "    \n",
    "    # Collect additional rows for missing categories\n",
    "    additional_rows = []\n",
    "    rows_added_by_category = defaultdict(int)\n",
    "    \n",
    "    for col, missing_vals in missing_categories.items():\n",
    "        print(f\"\\nSampling for column '{col}'...\")\n",
    "        \n",
    "        for val in missing_vals:\n",
    "            # Find all rows in full dataset with this category value\n",
    "            matching_rows = full_df[full_df[col] == val]\n",
    "            \n",
    "            if len(matching_rows) == 0:\n",
    "                print(f\"  Warning: No rows found for {col}='{val}' in full dataset\")\n",
    "                continue\n",
    "            \n",
    "            # Sample requested number of rows (or all available if fewer)\n",
    "            n_samples = min(samples_per_category, len(matching_rows))\n",
    "            sampled_rows = matching_rows.sample(n=n_samples, random_state=42)\n",
    "            \n",
    "            # Filter out rows that would introduce new categories in excluded columns\n",
    "            if exclude_columns:\n",
    "                original_sample_size = len(sampled_rows)\n",
    "                \n",
    "                for exclude_col in exclude_columns:\n",
    "                    if exclude_col in sampled_rows.columns and exclude_col in sample_df.columns:\n",
    "                        # Get existing categories in sample\n",
    "                        existing_categories = set(sample_df[exclude_col].dropna().unique())\n",
    "                        \n",
    "                        # Only keep rows where excluded column has existing values or is null\n",
    "                        mask = (sampled_rows[exclude_col].isin(existing_categories) | \n",
    "                               sampled_rows[exclude_col].isna())\n",
    "                        sampled_rows = sampled_rows[mask]\n",
    "                \n",
    "                if len(sampled_rows) == 0:\n",
    "                    print(f\"  Skipped '{val}': Would violate exclusion constraints\")\n",
    "                    continue\n",
    "                elif len(sampled_rows) < original_sample_size:\n",
    "                    print(f\"  Filtered {original_sample_size - len(sampled_rows)} rows to respect exclusions\")\n",
    "            \n",
    "            additional_rows.append(sampled_rows)\n",
    "            rows_added_by_category[f\"{col}='{val}'\"] = len(sampled_rows)\n",
    "            print(f\"  Added {len(sampled_rows)} rows for '{val}' (out of {len(matching_rows)} available)\")\n",
    "    \n",
    "    # Combine all additional rows\n",
    "    if additional_rows:\n",
    "        df_additional = pd.concat(additional_rows, ignore_index=True)\n",
    "        \n",
    "        # Remove potential duplicates (in case same row satisfies multiple missing categories)\n",
    "        initial_additional_count = len(df_additional)\n",
    "        df_additional = df_additional.drop_duplicates()\n",
    "        final_additional_count = len(df_additional)\n",
    "        \n",
    "        if initial_additional_count != final_additional_count:\n",
    "            print(f\"\\nRemoved {initial_additional_count - final_additional_count} duplicate rows\")\n",
    "        \n",
    "        # Combine with original sample\n",
    "        df_enhanced = pd.concat([sample_df, df_additional], ignore_index=True)\n",
    "        \n",
    "        print(f\"\\n=== SUMMARY ===\")\n",
    "        print(f\"Original sample size: {len(sample_df)}\")\n",
    "        print(f\"Additional rows added: {len(df_additional)}\")\n",
    "        print(f\"Final dataset size: {len(df_enhanced)}\")\n",
    "        print(f\"Size increase: {len(df_additional)/len(sample_df)*100:.1f}%\")\n",
    "        \n",
    "        return df_enhanced\n",
    "    \n",
    "    else:\n",
    "        print(\"No additional rows could be sampled\")\n",
    "        return sample_df.copy()\n",
    "\n",
    "def verify_categories_coverage(df_before, df_after, categorical_columns):\n",
    "    \"\"\"\n",
    "    Verify that the enhanced dataset now covers all categories\n",
    "    \"\"\"\n",
    "    print(\"\\n=== CATEGORY COVERAGE VERIFICATION ===\")\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col not in df_before.columns:\n",
    "            continue\n",
    "            \n",
    "        before_cats = set(df_before[col].dropna().unique())\n",
    "        after_cats = set(df_after[col].dropna().unique())\n",
    "        new_cats = after_cats - before_cats\n",
    "        \n",
    "        print(f\"\\nColumn '{col}':\")\n",
    "        print(f\"  Before: {len(before_cats)} categories\")\n",
    "        print(f\"  After:  {len(after_cats)} categories\")\n",
    "        if new_cats:\n",
    "            print(f\"  New categories added: {list(new_cats)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74cc766b-65b7-4d9a-8502-478c3969901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preserving_pipeline(target_col, max_categorical_cardinality, excluded_columns=None):\n",
    "    \"\"\"\n",
    "    Create a pipeline that preserves excluded columns\n",
    "    \"\"\"\n",
    "    excluded_columns = excluded_columns or []\n",
    "    \n",
    "    return Pipeline([\n",
    "        ('multi_value_encoder', PreservingMultiValueEncoder(\n",
    "            max_cardinality=max_categorical_cardinality,\n",
    "            preserve_columns=excluded_columns\n",
    "        )),\n",
    "        ('categorical_encoder', PreservingCategoricalEncoder(\n",
    "            max_cardinality=max_categorical_cardinality,\n",
    "            preserve_columns=excluded_columns\n",
    "        )),\n",
    "        ('column_fixer', ColumnNameFixer()),\n",
    "        ('validator', DataValidator(target_col))\n",
    "    ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "502733c3-ec2e-41a2-ab19-1f449f514473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# === 1. DataLoader: Load data and check target column ===\n",
    "\n",
    "class DataLoader(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Load and perform initial data validation whether the target col exists:\n",
    "        - Handles both .xlsx and .csv.\n",
    "        - Stores the original shape of the data.\n",
    "        - Raises an error if the target column is missing.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, target_col='project_prf_normalised_work_effort'):\n",
    "        self.file_path = file_path\n",
    "        self.target_col = target_col  # This should be the standardized form\n",
    "        self.original_shape = None\n",
    "        self.original_target_col = None  # Store what we actually found\n",
    "        \n",
    "    def fit(self, X=None, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _standardize_column_name(self, col_name):\n",
    "        \"\"\"Convert column name to standardized format\"\"\"\n",
    "        return col_name.strip().lower().replace(' ', '_')\n",
    "    \n",
    "    def _find_target_column(self, df_columns):\n",
    "        \"\"\"\n",
    "        Smart target column finder - handles various formats\n",
    "        Returns the actual column name from the dataframe\n",
    "        \"\"\"\n",
    "        target_standardized = self.target_col.lower().replace(' ', '_')\n",
    "        \n",
    "        # Try exact match first\n",
    "        if self.target_col in df_columns:\n",
    "            return self.target_col\n",
    "            \n",
    "        # Try standardized versions of all columns\n",
    "        for col in df_columns:\n",
    "            col_standardized = self._standardize_column_name(col)\n",
    "            if col_standardized == target_standardized:\n",
    "                return col\n",
    "                \n",
    "        # If still not found, look for partial matches (for debugging)\n",
    "        similar_cols = []\n",
    "        target_words = set(target_standardized.split('_'))\n",
    "        for col in df_columns:\n",
    "            col_words = set(self._standardize_column_name(col).split('_'))\n",
    "            if len(target_words.intersection(col_words)) >= 2:  # At least 2 words match\n",
    "                similar_cols.append(col)\n",
    "                \n",
    "        return None, similar_cols\n",
    "    \n",
    "    def transform(self, X=None):\n",
    "        \"\"\"Load data from file with smart column handling\"\"\"\n",
    "\n",
    "        print(f\"Loading data from: {self.file_path}\")\n",
    "        \n",
    "        # Determine file type and load accordingly; support for Excel or CSV\n",
    "        if self.file_path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(self.file_path)\n",
    "        elif self.file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(self.file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Use .xlsx or .csv\")\n",
    "        \n",
    "        self.original_shape = df.shape\n",
    "        print(f\"Loaded data with shape: {df.shape}\")\n",
    "        \n",
    "        # Smart target column finding\n",
    "        result = self._find_target_column(df.columns)\n",
    "        \n",
    "        if isinstance(result, tuple):  # Not found, got similar columns\n",
    "            actual_col, similar_cols = result\n",
    "            error_msg = f\"Target column '{self.target_col}' not found in data.\"\n",
    "            if similar_cols:\n",
    "                error_msg += f\" Similar columns found: {similar_cols}\"\n",
    "            else:\n",
    "                error_msg += f\" Available columns: {list(df.columns)}\"\n",
    "            raise ValueError(error_msg)\n",
    "        else:\n",
    "            actual_col = result\n",
    "            \n",
    "        # Store the original column name we found\n",
    "        self.original_target_col = actual_col\n",
    "        \n",
    "        if actual_col != self.target_col:\n",
    "            print(f\"Target column found: '{actual_col}' -> will be standardized to '{self.target_col}'\")\n",
    "            \n",
    "        return df\n",
    "\n",
    "\n",
    "# === 2. ColumnNameStandardizer: Clean and standardize column names ===\n",
    "class ColumnNameStandardizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Standardize column names for consistency (lowercase, underscores, removes odd chars):\n",
    "        - Strips spaces, lowercases, replaces & with _&_, removes special chars.\n",
    "        - Useful for later steps and compatibility with modeling libraries.)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_col=None, original_target_col=None):\n",
    "        self.column_mapping = {}\n",
    "        self.target_col = target_col\n",
    "        self.original_target_col = original_target_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _standardize_columns(self, columns):\n",
    "        \"\"\"Standardize column names\"\"\"\n",
    "        return [col.strip().lower().replace(' ', '_') for col in columns]\n",
    "    \n",
    "    def _clean_column_names(self, columns):\n",
    "        \"\"\"Clean column names for compatibility\"\"\"\n",
    "        cleaned_cols = []\n",
    "        for col in columns:\n",
    "            # Replace ampersands with _&_ to match expected transformations\n",
    "            col_clean = col.replace(' & ', '_&_')\n",
    "            # Remove special characters except underscores and ampersands\n",
    "            col_clean = re.sub(r'[^\\w\\s&]', '', col_clean)\n",
    "            # Replace spaces with underscores\n",
    "            col_clean = col_clean.replace(' ', '_')\n",
    "            cleaned_cols.append(col_clean)\n",
    "        return cleaned_cols\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply column name standardization\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Store original column names\n",
    "        original_columns = df.columns.tolist()\n",
    "        \n",
    "        # Apply standardization\n",
    "        standardized_cols = self._standardize_columns(original_columns)\n",
    "        cleaned_cols = self._clean_column_names(standardized_cols)\n",
    "\n",
    "        # Special handling for target column\n",
    "        if self.original_target_col and self.target_col:\n",
    "            target_index = None\n",
    "            try:\n",
    "                target_index = original_columns.index(self.original_target_col)\n",
    "                cleaned_cols[target_index] = self.target_col\n",
    "                print(f\"Target column '{self.original_target_col}' -> '{self.target_col}'\")\n",
    "            except ValueError:\n",
    "                pass  # Original target col not found, proceed normally\n",
    "        \n",
    "        \n",
    "        # Create mapping\n",
    "        self.column_mapping = dict(zip(original_columns, cleaned_cols))\n",
    "        \n",
    "        # Apply new column names\n",
    "        df.columns = cleaned_cols\n",
    "        \n",
    "        # Report changes\n",
    "        changed_cols = sum(1 for orig, new in self.column_mapping.items() if orig != new)\n",
    "        print(f\"Standardized {changed_cols} column names\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 3. MissingValueAnalyzer: Analyze and handle missing values ===\n",
    "class MissingValueAnalyzer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Analyze and handle missing values\n",
    "        - Reports number of columns with >50% and >70% missing.\n",
    "        - Drops columns with a high proportion of missing data, except those you want to keep.\n",
    "        - Fills remaining missing values:\n",
    "            - Categorical: Fills with \"Missing\".\n",
    "            - Numeric: Fills with column median.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, high_missing_threshold=0.7, cols_to_keep=None):\n",
    "        self.high_missing_threshold = high_missing_threshold\n",
    "        self.cols_to_keep = cols_to_keep or []\n",
    "        self.high_missing_cols = []\n",
    "        self.missing_stats = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Analyze and handle missing values\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Calculate missing percentages\n",
    "        missing_pct = df.isnull().mean()\n",
    "        self.missing_stats = missing_pct.sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"\\nMissing value analysis:\")\n",
    "        print(f\"Columns with >50% missing: {sum(missing_pct > 0.5)}\")\n",
    "        print(f\"Columns with >70% missing: {sum(missing_pct > self.high_missing_threshold)}\")\n",
    "        \n",
    "        # Identify high missing columns\n",
    "        self.high_missing_cols = missing_pct[missing_pct > self.high_missing_threshold].index.tolist()\n",
    "        \n",
    "        # Filter out columns we want to keep\n",
    "        final_high_missing_cols = [col for col in self.high_missing_cols if col not in self.cols_to_keep]\n",
    "        \n",
    "        print(f\"Dropping {len(final_high_missing_cols)} columns with >{self.high_missing_threshold*100}% missing values\")\n",
    "        \n",
    "        # Drop high missing columns\n",
    "        df_clean = df.drop(columns=final_high_missing_cols)\n",
    "        \n",
    "        # Fill remaining missing values in categorical columns\n",
    "        cat_cols = df_clean.select_dtypes(include=['object', 'category']).columns\n",
    "        for col in cat_cols:\n",
    "            df_clean[col] = df_clean[col].fillna('Missing')\n",
    "        \n",
    "        # Fill remaining missing values in numerical columns with median\n",
    "        num_cols = df_clean.select_dtypes(include=['number']).columns\n",
    "        for col in num_cols:\n",
    "            if df_clean[col].isnull().sum() > 0:\n",
    "                median_val = df_clean[col].median()\n",
    "                df_clean[col] = df_clean[col].fillna(median_val)\n",
    "                print(f\"Filled {col} missing values with median: {median_val}\")\n",
    "        \n",
    "        print(f\"Data shape after missing value handling: {df_clean.shape}\")\n",
    "        return df_clean\n",
    "\n",
    "# === 4. SemicolonProcessor: Process multi-value columns (semicolon-separated) ===\n",
    "class SemicolonProcessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Process semicolon-separated values in columns (e.g., \"Python; Java; SQL\")\n",
    "        - Identifies columns with semicolons.\n",
    "        - Cleans: lowercases, strips, deduplicates, sorts, optionally standardizes values (e.g., \"stand alone\" → \"stand-alone\").\n",
    "        - Useful for multi-value categorical features.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, standardization_mapping=None):\n",
    "        self.semicolon_cols = []\n",
    "        self.standardization_mapping = standardization_mapping or {\n",
    "            \"scrum\": \"agile development\",\n",
    "            \"file &/or print server\": \"file/print server\",\n",
    "        }\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _clean_and_sort_semicolon(self, val, apply_standardization=False, mapping=None):\n",
    "        \"\"\"Clean, deduplicate, sort, and standardize semicolon-separated values\"\"\"\n",
    "        if pd.isnull(val) or val == '':\n",
    "            return val\n",
    "        \n",
    "        parts = [x.strip().lower() for x in str(val).split(';') if x.strip()]\n",
    "        \n",
    "        if apply_standardization and mapping is not None:\n",
    "            parts = [mapping.get(part, part) for part in parts]\n",
    "        \n",
    "        unique_cleaned = sorted(set(parts))\n",
    "        return '; '.join(unique_cleaned)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Process semicolon-separated columns\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Identify columns with semicolons\n",
    "        self.semicolon_cols = [\n",
    "            col for col in df.columns\n",
    "            if df[col].dropna().astype(str).str.contains(';').any()\n",
    "        ]\n",
    "        \n",
    "        print(f\"Found {len(self.semicolon_cols)} columns with semicolons: {self.semicolon_cols}\")\n",
    "        \n",
    "        # Process each semicolon column\n",
    "        for col in self.semicolon_cols:\n",
    "            # Apply mapping for specific columns\n",
    "            apply_mapping = col in ['process_pmf_development_methodologies', 'tech_tf_server_roles']\n",
    "            mapping = self.standardization_mapping if apply_mapping else None\n",
    "            \n",
    "            # Clean the column\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: self._clean_and_sort_semicolon(x, apply_standardization=apply_mapping, mapping=mapping)\n",
    "            )\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 5. MultiValueEncoder: Encode semicolon columns using MultiLabelBinarizer ===\n",
    "class MultiValueEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Handle multi-value columns using MultiLabelBinarizer\n",
    "        - Only processes columns with a manageable number of unique values (max_cardinality).\n",
    "        - Each semicolon column becomes several binary columns (e.g., \"lang__python\", \"lang__java\", ...).     \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_cardinality=10):\n",
    "        # Ensure max_cardinality is always an integer\n",
    "        self.max_cardinality = int(max_cardinality) if max_cardinality is not None else 10\n",
    "        self.multi_value_cols = []\n",
    "        self.mlb_transformers = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Encode multi-value columns\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Identify semicolon columns (multi-value)\n",
    "        semicolon_cols = [\n",
    "            col for col in df.columns\n",
    "            if df[col].dropna().astype(str).str.contains(';').any()\n",
    "        ]\n",
    "        \n",
    "        # Filter for low cardinality multi-value columns\n",
    "        self.multi_value_cols = []\n",
    "        for col in semicolon_cols:\n",
    "            # Get unique values across all entries\n",
    "            all_values = set()\n",
    "            for val in df[col].dropna().astype(str):\n",
    "                values = [v.strip() for v in val.split(';') if v.strip()]\n",
    "                all_values.update(values)\n",
    "            \n",
    "            # Check cardinality (max_cardinality is already an integer from __init__)\n",
    "            if len(all_values) <= self.max_cardinality:\n",
    "                self.multi_value_cols.append(col)\n",
    "        \n",
    "        print(f\"Encoding {len(self.multi_value_cols)} multi-value columns: {self.multi_value_cols}\")\n",
    "        \n",
    "        # Process each multi-value column\n",
    "        for col in self.multi_value_cols:\n",
    "            # Prepare data for MultiLabelBinarizer\n",
    "            values = df[col].dropna().astype(str).apply(\n",
    "                lambda x: [item.strip() for item in x.split(';') if item.strip()]\n",
    "            )\n",
    "            \n",
    "            # Handle empty values - fill with empty list for MultiLabelBinarizer\n",
    "            if len(values) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Fit and transform\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            \n",
    "            # Convert to list of lists, handling NaN/empty cases\n",
    "            values_list = []\n",
    "            for idx in df.index:\n",
    "                if idx in values.index and values[idx]:\n",
    "                    values_list.append(values[idx])\n",
    "                else:\n",
    "                    values_list.append([])  # Empty list for missing values\n",
    "            \n",
    "            onehot = pd.DataFrame(\n",
    "                mlb.fit_transform(values_list),\n",
    "                columns=[f\"{col}__{cat}\" for cat in mlb.classes_],\n",
    "                index=df.index\n",
    "            )\n",
    "            \n",
    "            # Store transformer for later use\n",
    "            self.mlb_transformers[col] = mlb\n",
    "            \n",
    "            # Join with main dataframe\n",
    "            df = df.join(onehot, how='left')\n",
    "            \n",
    "            print(f\"Encoded {col} into {len(mlb.classes_)} binary columns\")\n",
    "        \n",
    "        # Remove original multi-value columns\n",
    "        df = df.drop(columns=self.multi_value_cols)\n",
    "        \n",
    "        return df\n",
    "\n",
    "class PreservingMultiValueEncoder(MultiValueEncoder):\n",
    "    \"\"\"\n",
    "    Modified MultiValueEncoder that preserves specific columns\n",
    "    \"\"\"\n",
    "    def __init__(self, max_cardinality=10, preserve_columns=None):\n",
    "        super().__init__(max_cardinality)\n",
    "        self.preserve_columns = preserve_columns or []\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"Encode multi-value columns but preserve specified columns\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Store preserved columns before processing\n",
    "        preserved_data = {}\n",
    "        for col in self.preserve_columns:\n",
    "            if col in df.columns:\n",
    "                preserved_data[col] = df[col].copy()\n",
    "                print(f\"[PreservingMultiValueEncoder] Preserving column '{col}'\")\n",
    "        \n",
    "        # Apply normal multi-value encoding\n",
    "        df_processed = super().transform(df)\n",
    "        \n",
    "        # Add back preserved columns (they might have been processed/removed)\n",
    "        for col, data in preserved_data.items():\n",
    "            if col not in df_processed.columns:\n",
    "                df_processed[col] = data\n",
    "                print(f\"[PreservingMultiValueEncoder] Restored column '{col}'\")\n",
    "        \n",
    "        return df_processed\n",
    "\n",
    "\n",
    "\n",
    "# === 6. CategoricalEncoder: One-hot encode regular categorical columns ===\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Handle single-value categorical columns\n",
    "        - Ignores semicolon columns.\n",
    "        - Only encodes columns with a number of categories ≤ max_cardinality (to avoid high-dimensional explosion).\n",
    "        - Can drop the first category for each variable to avoid multicollinearity.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_cardinality=10, drop_first=True):\n",
    "        self.max_cardinality = max_cardinality\n",
    "        self.drop_first = drop_first\n",
    "        self.categorical_cols = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Encode categorical columns\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Identify categorical columns\n",
    "        cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Identify semicolon columns to exclude\n",
    "        semicolon_cols = [\n",
    "            col for col in df.columns\n",
    "            if df[col].dropna().astype(str).str.contains(';').any()\n",
    "        ]\n",
    "        \n",
    "        # Filter for low cardinality single-value categorical columns\n",
    "        excluded_columns = getattr(self, 'preserve_columns', [])  # Add this attribute\n",
    "        self.categorical_cols = [\n",
    "            col for col in cat_cols \n",
    "            if (col not in semicolon_cols and \n",
    "                df[col].nunique() <= self.max_cardinality and\n",
    "                col not in excluded_columns)  # Skip excluded columns\n",
    "        ]\n",
    "        \n",
    "        print(f\"One-hot encoding {len(self.categorical_cols)} categorical columns: {self.categorical_cols}\")\n",
    "        \n",
    "        # Apply one-hot encoding\n",
    "        if self.categorical_cols:\n",
    "            df = pd.get_dummies(df, columns=self.categorical_cols, drop_first=self.drop_first)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Before creating the final pipeline, modify the encoder classes:\n",
    "class PreservingCategoricalEncoder(CategoricalEncoder):\n",
    "    \"\"\"\n",
    "    Modified CategoricalEncoder that preserves specific columns\n",
    "    \"\"\"\n",
    "    def __init__(self, max_cardinality=10, drop_first=True, preserve_columns=None):\n",
    "        super().__init__(max_cardinality, drop_first)\n",
    "        self.preserve_columns = preserve_columns or []\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"Encode categorical columns but preserve specified columns\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Store preserved columns before processing\n",
    "        preserved_data = {}\n",
    "        for col in self.preserve_columns:\n",
    "            if col in df.columns:\n",
    "                preserved_data[col] = df[col].copy()\n",
    "                print(f\"[PreservingCategoricalEncoder] Preserving column '{col}'\")\n",
    "        \n",
    "        # Remove preserved columns from categorical processing\n",
    "        original_cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        semicolon_cols = [\n",
    "            col for col in df.columns\n",
    "            if df[col].dropna().astype(str).str.contains(';').any()\n",
    "        ]\n",
    "        \n",
    "        # Filter out preserved columns from processing\n",
    "        self.categorical_cols = [\n",
    "            col for col in original_cat_cols \n",
    "            if (col not in semicolon_cols and \n",
    "                df[col].nunique() <= self.max_cardinality and \n",
    "                col not in self.preserve_columns)  # Don't process preserved columns\n",
    "        ]\n",
    "        \n",
    "        print(f\"[PreservingCategoricalEncoder] Will encode {len(self.categorical_cols)} columns (excluding {len(self.preserve_columns)} preserved)\")\n",
    "        \n",
    "        # Apply one-hot encoding to non-preserved columns only\n",
    "        if self.categorical_cols:\n",
    "            df_processed = pd.get_dummies(df, columns=self.categorical_cols, drop_first=self.drop_first)\n",
    "        else:\n",
    "            df_processed = df.copy()\n",
    "        \n",
    "        # Ensure preserved columns are still there\n",
    "        for col, data in preserved_data.items():\n",
    "            if col not in df_processed.columns:\n",
    "                df_processed[col] = data\n",
    "                print(f\"[PreservingCategoricalEncoder] Restored column '{col}'\")\n",
    "        \n",
    "        return df_processed\n",
    "\n",
    "\n",
    "\n",
    "# === 7. ColumnNameFixer: Final column name cleanup for PyCaret etc ===\n",
    "class ColumnNameFixer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Fix column names for PyCaret compatibility (removes illegal characters, replaces spaces/ampersands, handles duplicates):\n",
    "        - No duplicate column names after encoding.\n",
    "        - Only alphanumeric and underscores. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.column_transformations = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Fix problematic column names\"\"\"\n",
    "        df = X.copy()\n",
    "        original_cols = df.columns.tolist()\n",
    "        fixed_columns = []\n",
    "        seen_columns = set()\n",
    "        \n",
    "        for col in original_cols:\n",
    "            # Replace spaces with underscores\n",
    "            fixed_col = col.replace(' ', '_')\n",
    "            # Replace ampersands\n",
    "            fixed_col = fixed_col.replace('&', 'and')\n",
    "            # Remove other problematic characters\n",
    "            fixed_col = ''.join(c if c.isalnum() or c == '_' else '_' for c in fixed_col)\n",
    "            # Remove multiple consecutive underscores\n",
    "            fixed_col = re.sub('_+', '_', fixed_col)\n",
    "            # Remove leading/trailing underscores\n",
    "            fixed_col = fixed_col.strip('_')\n",
    "            \n",
    "            # Handle duplicates\n",
    "            base_col = fixed_col\n",
    "            suffix = 1\n",
    "            while fixed_col in seen_columns:\n",
    "                fixed_col = f\"{base_col}_{suffix}\"\n",
    "                suffix += 1\n",
    "            \n",
    "            seen_columns.add(fixed_col)\n",
    "            fixed_columns.append(fixed_col)\n",
    "        \n",
    "        # Store transformations\n",
    "        self.column_transformations = dict(zip(original_cols, fixed_columns))\n",
    "        \n",
    "        # Apply new column names\n",
    "        df.columns = fixed_columns\n",
    "        \n",
    "        # Check for duplicates\n",
    "        dup_check = [item for item, count in pd.Series(fixed_columns).value_counts().items() if count > 1]\n",
    "        if dup_check:\n",
    "            print(f\"WARNING: Found {len(dup_check)} duplicate column names: {dup_check}\")\n",
    "        else:\n",
    "            print(\"No duplicate column names after fixing\")\n",
    "        \n",
    "        n_changed = sum(1 for old, new in self.column_transformations.items() if old != new)\n",
    "        print(f\"Fixed {n_changed} column names for PyCaret compatibility\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 8. DataValidator: Final summary and checks ===\n",
    "class DataValidator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Validate final dataset\n",
    "        - Shape, missing values, infinities.\n",
    "        - Data types (numeric, categorical).\n",
    "        - Stats on the target column (mean, std, min, max, missing).\n",
    "        - Report issues if any.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_col):\n",
    "        self.target_col = target_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Validate the processed dataset\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        print(f\"\\n=== Final Data Validation ===\")\n",
    "        print(f\"Final shape: {df.shape}\")\n",
    "        print(f\"Target column: {self.target_col}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_count = df.isnull().sum().sum()\n",
    "        print(f\"Total missing values: {missing_count}\")\n",
    "        \n",
    "        # Check for infinite values\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        inf_count = np.isinf(df[numeric_cols].values).sum()\n",
    "        print(f\"Total infinite values: {inf_count}\")\n",
    "        \n",
    "        # Data types summary\n",
    "        print(f\"\\nData types:\")\n",
    "        print(f\"  Numeric columns: {len(df.select_dtypes(include=[np.number]).columns)}\")\n",
    "        print(f\"  Categorical columns: {len(df.select_dtypes(include=['object', 'category']).columns)}\")\n",
    "        \n",
    "        # Target variable summary\n",
    "        if self.target_col in df.columns:\n",
    "            target_stats = df[self.target_col].describe()\n",
    "            print(f\"\\nTarget variable '{self.target_col}' statistics:\")\n",
    "            print(f\"  Mean: {target_stats['mean']:.2f}\")\n",
    "            print(f\"  Std: {target_stats['std']:.2f}\")\n",
    "            print(f\"  Min: {target_stats['min']:.2f}\")\n",
    "            print(f\"  Max: {target_stats['max']:.2f}\")\n",
    "            print(f\"  Missing: {df[self.target_col].isnull().sum()}\")\n",
    "        else:\n",
    "            print(f\"WARNING: Target column '{self.target_col}' not found!\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === Pipeline creation function: returns the Scikit-learn pipeline ===\n",
    "def create_isbsg_preprocessing_pipeline(\n",
    "    target_col='project_prf_normalised_work_effort',\n",
    "    original_target_col=None,\n",
    "    high_missing_threshold=0.7,\n",
    "    cols_to_keep=None,\n",
    "    max_categorical_cardinality=10,\n",
    "    standardization_mapping=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create complete preprocessing pipeline with smart target column handling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    target_col : str\n",
    "        Name of target column\n",
    "    original_target_col : str\n",
    "        Original target column name found in data\n",
    "    high_missing_threshold : float\n",
    "        Threshold for dropping columns with high missing values\n",
    "    cols_to_keep : list\n",
    "        Columns to keep even if they have high missing values\n",
    "    max_categorical_cardinality : int\n",
    "        Maximum number of unique values for categorical encoding\n",
    "    standardization_mapping : dict\n",
    "        Custom mapping for standardizing semicolon-separated values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    sklearn.pipeline.Pipeline\n",
    "        Complete preprocessing pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    if cols_to_keep is None:\n",
    "        cols_to_keep = [\n",
    "            'project_prf_case_tool_used', \n",
    "            'process_pmf_prototyping_used',\n",
    "            'tech_tf_client_roles', \n",
    "            'tech_tf_type_of_server', \n",
    "            'tech_tf_clientserver_description'\n",
    "        ]\n",
    "    \n",
    "    # Ensure max_categorical_cardinality is an integer\n",
    "    if not isinstance(max_categorical_cardinality, int):\n",
    "        max_categorical_cardinality = 10\n",
    "        print(f\"Warning: max_categorical_cardinality was not an integer, defaulting to {max_categorical_cardinality}\")\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('column_standardizer', ColumnNameStandardizer(target_col, original_target_col)),\n",
    "        ('missing_handler', MissingValueAnalyzer(\n",
    "            high_missing_threshold=high_missing_threshold,\n",
    "            cols_to_keep=cols_to_keep\n",
    "        )),\n",
    "        ('semicolon_processor', SemicolonProcessor(standardization_mapping=standardization_mapping)),\n",
    "        ('multi_value_encoder', MultiValueEncoder(max_cardinality=max_categorical_cardinality)),\n",
    "        ('categorical_encoder', CategoricalEncoder(max_cardinality=max_categorical_cardinality)),\n",
    "        ('column_fixer', ColumnNameFixer()),\n",
    "        ('validator', DataValidator(target_col))\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# === Full workflow function: orchestrates loading, pipeline, and saving ===\n",
    "def preprocess_isbsg_data(\n",
    "    file_path,\n",
    "    target_col='project_prf_normalised_work_effort',  # Always use standardized form\n",
    "    output_dir='../data',\n",
    "    save_intermediate=True,\n",
    "    **pipeline_kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete preprocessing workflow for ISBSG data: loads the data, runs \n",
    "      the full preprocessing pipeline, saves processed data, pipeline \n",
    "      object, and a metadata report to disk, and returns the processed \n",
    "      DataFrame and metadata\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to input data file\n",
    "    target_col : str\n",
    "        Name of target column\n",
    "    output_dir : str\n",
    "        Directory to save processed data\n",
    "    save_intermediate : bool\n",
    "        Whether to save intermediate processing steps\n",
    "    **pipeline_kwargs : dict\n",
    "        Additional arguments for pipeline creation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Processed dataframe ready for modeling\n",
    "    dict\n",
    "        Processing metadata and statistics\n",
    "    \"\"\"\n",
    "\n",
    "    # print pipeline header\n",
    "    print(\"=\"*60)\n",
    "    print(\"ISBSG Data Preprocessing Pipeline\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    print(f\"Target column (standardized): {target_col}\")\n",
    "    print(f\"Timestamp: {datetime.now()}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load data with smart column detection\n",
    "    loader = DataLoader(file_path, target_col)\n",
    "    df_raw = loader.transform(X = None)\n",
    "    \n",
    "    # Create and fit preprocessing pipeline\n",
    "    pipeline = create_isbsg_preprocessing_pipeline(\n",
    "        target_col=target_col,\n",
    "        original_target_col=loader.original_target_col,  # Pass the found column name\n",
    "        **pipeline_kwargs\n",
    "    )\n",
    "    \n",
    "    # Apply preprocessing in order of ColumnNameStandardizer=> MissingValueAnalyzer =>\n",
    "    # SemicolonProcessor=> MultiValueEncoder=> CategoricalEncoder => ColumnNameFixer\n",
    "\n",
    "    # Apply preprocessing\n",
    "    df_processed = pipeline.fit_transform(df_raw)\n",
    "    \n",
    "    # Prepare metadata\n",
    "    metadata = {\n",
    "        'original_shape': loader.original_shape,\n",
    "        'processed_shape': df_processed.shape,\n",
    "        'processing_timestamp': datetime.now().isoformat(),\n",
    "        'target_column_standardized': target_col,\n",
    "        'target_column_original': loader.original_target_col,\n",
    "        'pipeline_steps': [step[0] for step in pipeline.steps]\n",
    "    }\n",
    "    \n",
    "    # Save processed data\n",
    "    file_stem = Path(file_path).stem\n",
    "    output_path = os.path.join(output_dir, f\"{file_stem}_preprocessed.csv\")\n",
    "    df_processed.to_csv(output_path, index=False)\n",
    "    print(f\"\\nProcessed data saved to: {output_path}\")\n",
    "    \n",
    "    # Save pipeline\n",
    "    pipeline_path = os.path.join(output_dir, f\"{file_stem}_preprocessing_pipeline.pkl\")\n",
    "    joblib.dump(pipeline, pipeline_path)\n",
    "    print(f\"Pipeline saved to: {pipeline_path}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = os.path.join(output_dir, f\"{file_stem}_preprocessing_metadata.txt\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        f.write(\"ISBSG Data Preprocessing Metadata\\n\")\n",
    "        f.write(\"=\"*40 + \"\\n\")\n",
    "        for key, value in metadata.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    print(f\"Metadata saved to: {metadata_path}\")\n",
    "\n",
    "    # Print completion & return results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Preprocessing completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return df_processed, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a67d417-a269-414e-9944-fc954d659ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_categorical_preprocessing(\n",
    "    sample_file_path: str,\n",
    "    full_file_path: str,\n",
    "    target_col: str,\n",
    "    output_dir: str,\n",
    "    cols_to_keep: List[str] = None,\n",
    "    high_card_columns: List[str] = None,\n",
    "    process_high_cardinality: bool = True,\n",
    "    max_categorical_cardinality: int = 10,\n",
    "    samples_per_category: int = 3,\n",
    "    standardization_mapping: Dict[str, str] = None,\n",
    "    high_missing_threshold: float = 0.7,\n",
    "    separator: str = ';',\n",
    "    strategy: str = 'top_k',\n",
    "    k: int = 20,\n",
    "    exclude_from_enhancement: List[str] = None \n",
    ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Integrated pipeline to:\n",
    "    1. Load sample and full datasets\n",
    "    2. Apply consistent preprocessing to both datasets before comparison\n",
    "    2. Auto-detect categorical columns\n",
    "    3. Handle high-cardinality multi-value columns\n",
    "    4. Enhance sample with missing categories from full dataset\n",
    "    5. Apply standardization and final preprocessing\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    exclude_from_enhancement : List[str]\n",
    "        List of column names to exclude from getting additional categories from full dataset\n",
    "    \n",
    "    Returns:\n",
    "        - Enhanced and processed DataFrame\n",
    "        - Metadata about the processing steps\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"INTEGRATED CATEGORICAL PREPROCESSING PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Initialize exclude list if not provided\n",
    "    if exclude_from_enhancement is None:\n",
    "        exclude_from_enhancement = []\n",
    "    \n",
    "    # Step 1: Load datasets\n",
    "    print(\"\\n1. Loading datasets...\")\n",
    "    sample_df = pd.read_excel(sample_file_path)\n",
    "    full_df = pd.read_excel(full_file_path)\n",
    "\n",
    "    # Lowercase all column names in both DataFrames independently\n",
    "    sample_df.columns = [col1.lower() for col1 in sample_df.columns]\n",
    "    full_df.columns   = [col2.lower() for col2 in full_df.columns]\n",
    "\n",
    "    \n",
    "    print(f\"Sample dataset shape: {sample_df.shape}\")\n",
    "    print(f\"Full dataset shape: {full_df.shape}\")\n",
    "    \n",
    "    # Step 2: Create preprocessing pipeline (WITHOUT final validation)\n",
    "    print(\"\\n2. Creating preprocessing pipeline...\")\n",
    "    \n",
    "    # Create a pipeline that stops before final validation\n",
    "    initial_pipeline = Pipeline([\n",
    "        ('column_standardizer', ColumnNameStandardizer(target_col, target_col.lower())),\n",
    "        ('missing_handler', MissingValueAnalyzer(\n",
    "            high_missing_threshold=high_missing_threshold,\n",
    "            cols_to_keep=cols_to_keep or []\n",
    "        )),\n",
    "        ('semicolon_processor', SemicolonProcessor(standardization_mapping=standardization_mapping)),\n",
    "    ])\n",
    "    \n",
    "    # Step 3: Apply initial preprocessing to BOTH datasets\n",
    "    print(\"\\n3. Applying initial preprocessing to both datasets...\")\n",
    "    \n",
    "    # Process sample dataset\n",
    "    sample_df_preprocessed = initial_pipeline.fit_transform(sample_df)\n",
    "    print(f\"Sample after initial preprocessing: {sample_df_preprocessed.shape}\")\n",
    "    \n",
    "    # Process full dataset with same pipeline\n",
    "    full_df_preprocessed = initial_pipeline.transform(full_df)  # Use transform, not fit_transform\n",
    "    print(f\"Full dataset after initial preprocessing: {full_df_preprocessed.shape}\")\n",
    "    \n",
    "    # Step 4: Handle high-cardinality columns on PREPROCESSED datasets\n",
    "    # Initialize col_mapping regardless of whether we process high-cardinality columns\n",
    "    col_mapping = {}\n",
    "    if process_high_cardinality and high_card_columns:\n",
    "        print(\"\\n4. Processing high-cardinality multi-value columns...\")\n",
    "        \n",
    "        if high_card_columns is None:\n",
    "            high_card_columns = ['external_eef_organisation_type', 'project_prf_application_type']\n",
    "        \n",
    "        # Filter out excluded columns from high-cardinality processing\n",
    "        high_card_columns_to_process = [col for col in high_card_columns \n",
    "                             if col not in (exclude_from_enhancement or [])]\n",
    "        \n",
    "        if not high_card_columns_to_process:\n",
    "            print(\"All high-cardinality columns are in exclusion list - skipping processing\")\n",
    "        else:\n",
    "            # Process high-cardinality columns in both datasets\n",
    "            for col in high_card_columns_to_process:\n",
    "                if col in full_df_preprocessed.columns:\n",
    "                    print(f\"\\nProcessing high-cardinality column: {col}\")\n",
    "                    \n",
    "                    # Process full dataset\n",
    "                    full_df_preprocessed, temp_mapping = handle_high_cardinality_multivalue(\n",
    "                        full_df_preprocessed,\n",
    "                        multi_value_columns=[col],\n",
    "                        separator=separator,\n",
    "                        strategy=strategy,\n",
    "                        k=k,\n",
    "                        preserve_original=exclude_from_enhancement\n",
    "                    )\n",
    "                    \n",
    "                    # Process sample dataset with same strategy\n",
    "                    sample_df_preprocessed, _ = handle_high_cardinality_multivalue(\n",
    "                        sample_df_preprocessed,\n",
    "                        multi_value_columns=[col],\n",
    "                        separator=separator,\n",
    "                        strategy=strategy,\n",
    "                        k=k,\n",
    "                        preserve_original=exclude_from_enhancement\n",
    "                    )\n",
    "                    \n",
    "                    col_mapping.update(temp_mapping)\n",
    "    else:\n",
    "        print(\"\\n4. Skipping high-cardinality processing (disabled or no columns specified)\")                    \n",
    "\n",
    "    # Step 4.5: Expand exclude list to include all derived binary columns\n",
    "    print(\"\\n4.5. Expanding exclude list to include derived columns...\")\n",
    "    expanded_exclude_list = exclude_from_enhancement.copy()\n",
    "    \n",
    "    for pattern in exclude_from_enhancement:\n",
    "        # Find all columns that start with the excluded pattern\n",
    "        matching_cols = [col for col in sample_df_preprocessed.columns \n",
    "                        if col.startswith(pattern)]\n",
    "        expanded_exclude_list.extend(matching_cols)\n",
    "        if matching_cols:\n",
    "            print(f\"  Added derived columns for '{pattern}': {matching_cols}\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    expanded_exclude_list = list(set(expanded_exclude_list))\n",
    "    print(f\"  Final expanded exclude list: {len(expanded_exclude_list)} columns\")   \n",
    "    \n",
    "    # Step 5: NOW identify categorical columns from preprocessed datasets\n",
    "    print(\"\\n5. Identifying categorical columns from preprocessed datasets...\")\n",
    "    categorical_columns = []\n",
    "    for col in sample_df_preprocessed.columns:\n",
    "        if (sample_df_preprocessed[col].dtype == 'object' or \n",
    "            sample_df_preprocessed[col].nunique() < max_categorical_cardinality):\n",
    "            categorical_columns.append(col)\n",
    "\n",
    "    categorical_columns = [col.lower() for col in categorical_columns]\n",
    "    print(f\"Detected categorical columns: {len(categorical_columns)} columns\")\n",
    "    \n",
    "    # Step 6: Enhanced category sampling with PROPER exclusions\n",
    "    print(\"\\n6. Enhancing sample with missing categories from preprocessed full dataset...\")\n",
    "    print(f\"Excluding columns from enhancement: {exclude_from_enhancement}\")\n",
    "    \n",
    "    # SEPARATE enhancement exclusion from processing exclusion\n",
    "    columns_to_enhance = [col for col in categorical_columns \n",
    "                         if col not in (exclude_from_enhancement or [])]\n",
    "    \n",
    "    print(f\"Columns that will be enhanced: {len(columns_to_enhance)} out of {len(categorical_columns)}\")\n",
    "    \n",
    "    # Add missing categories ONLY for non-excluded columns\n",
    "    enhanced_df = add_missing_categories_from_full_dataset(\n",
    "        sample_df=sample_df_preprocessed,\n",
    "        full_df=full_df_preprocessed,\n",
    "        categorical_columns=columns_to_enhance,  # Only enhance these\n",
    "        samples_per_category=samples_per_category,\n",
    "        exclude_columns=exclude_from_enhancement  # But respect exclusions for side effects\n",
    "    )\n",
    "    \n",
    "    print(f\"Enhanced dataset shape: {enhanced_df.shape}\")\n",
    "\n",
    "    # CRITICAL FIX: Ensure excluded columns are still in enhanced_df\n",
    "    excluded_columns = exclude_from_enhancement or []\n",
    "    missing_excluded = []\n",
    "    \n",
    "    for exclude_col in excluded_columns:\n",
    "        if exclude_col not in enhanced_df.columns:\n",
    "            missing_excluded.append(exclude_col)\n",
    "            print(f\"⚠️  WARNING: Excluded column '{exclude_col}' missing from enhanced_df\")\n",
    "    \n",
    "    if missing_excluded:\n",
    "        print(f\"\\nERROR: {len(missing_excluded)} excluded columns are missing!\")\n",
    "        print(\"This should not happen. Checking original preprocessed data...\")\n",
    "        \n",
    "        for col in missing_excluded:\n",
    "            if col in sample_df_preprocessed.columns:\n",
    "                print(f\"  '{col}' exists in sample_df_preprocessed - copying over\")\n",
    "                enhanced_df[col] = sample_df_preprocessed[col]\n",
    "            else:\n",
    "                print(f\"  '{col}' not found anywhere!\")\n",
    "\n",
    "    # Step 6.5: Restore excluded columns if they got lost during high-cardinality processing\n",
    "    print(\"\\n6.5. Restoring excluded columns that were lost during high-cardinality processing...\")\n",
    "    \n",
    "    excluded_columns = exclude_from_enhancement or []\n",
    "    missing_excluded = []\n",
    "    \n",
    "    for exclude_col in excluded_columns:\n",
    "        if exclude_col not in enhanced_df.columns:\n",
    "            missing_excluded.append(exclude_col)\n",
    "            print(f\"⚠️  WARNING: Excluded column '{exclude_col}' missing from enhanced_df\")\n",
    "    \n",
    "    if missing_excluded:\n",
    "        print(f\"\\nRestoring {len(missing_excluded)} excluded columns from original data...\")\n",
    "        \n",
    "        for col in missing_excluded:\n",
    "            # Try to find the column in the original sample data before high-cardinality processing\n",
    "            if col in sample_df.columns:  # Use the very original sample_df\n",
    "                print(f\"  Restoring '{col}' from original sample_df\")\n",
    "                enhanced_df[col] = sample_df[col].iloc[:len(enhanced_df)]  # Match the length\n",
    "            elif col in sample_df_preprocessed.columns:\n",
    "                print(f\"  Restoring '{col}' from sample_df_preprocessed\")\n",
    "                enhanced_df[col] = sample_df_preprocessed[col].iloc[:len(enhanced_df)]\n",
    "            else:\n",
    "                print(f\"  ERROR: '{col}' not found in any source data!\")\n",
    "        \n",
    "        print(f\"Enhanced dataset shape after restoration: {enhanced_df.shape}\")\n",
    "    else:\n",
    "        print(\"✅ All excluded columns are already present\")\n",
    "    \n",
    "    # Step 7: Verify categories coverage (ALL columns, but note which were excluded)\n",
    "    print(\"\\n7. Verifying categories coverage...\")\n",
    "    print(\"\\n=== CATEGORY COVERAGE VERIFICATION ===\")\n",
    "    excluded_columns = exclude_from_enhancement or []\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col not in sample_df_preprocessed.columns:\n",
    "            continue\n",
    "            \n",
    "        before_cats = set(sample_df_preprocessed[col].dropna().unique())\n",
    "        after_cats = set(enhanced_df[col].dropna().unique())\n",
    "        new_cats = after_cats - before_cats\n",
    "        \n",
    "        print(f\"\\nColumn '{col}':\")\n",
    "        print(f\"  Before: {len(before_cats)} categories\")\n",
    "        print(f\"  After:  {len(after_cats)} categories\")\n",
    "        \n",
    "        if col in excluded_columns:\n",
    "            if new_cats:\n",
    "                print(f\"  ⚠️  EXCLUDED column gained {len(new_cats)} categories (side effect): {list(new_cats)[:5]}\")\n",
    "            else:\n",
    "                print(f\"  ✅ EXCLUDED column preserved (no new categories)\")\n",
    "        else:\n",
    "            if new_cats:\n",
    "                print(f\"  ✅ Enhanced with {len(new_cats)} new categories: {list(new_cats)[:5]}\")\n",
    "            else:\n",
    "                print(f\"  ➖ No enhancement needed\")\n",
    "    \n",
    "    # Step 8: Apply final preprocessing stages to ALL columns\n",
    "    print(\"\\n8. Applying final preprocessing stages...\")\n",
    "    \n",
    "    # Create modified pipeline that preserves excluded columns\n",
    "    final_pipeline = create_preserving_pipeline(\n",
    "        target_col=target_col,\n",
    "        max_categorical_cardinality=max_categorical_cardinality,\n",
    "        excluded_columns=exclude_from_enhancement\n",
    "    )\n",
    "    \n",
    "    final_df = final_pipeline.fit_transform(enhanced_df)\n",
    "    \n",
    "    # Step 9: Final validation and duplicate check\n",
    "    print(\"\\n9. Final validation and duplicate check...\")\n",
    "    \n",
    "    # Check for any remaining duplicates after all processing\n",
    "    final_duplicate_cols = final_df.columns[final_df.columns.duplicated()].tolist()\n",
    "    if final_duplicate_cols:\n",
    "        print(f\"Warning: Found duplicate columns in final dataset: {final_duplicate_cols}\")\n",
    "        final_df = final_df.loc[:, ~final_df.columns.duplicated()]\n",
    "        print(\"Removed final duplicate columns\")\n",
    "    \n",
    "    print(f\"Original sample shape: {sample_df.shape}\")\n",
    "    print(f\"Final processed shape: {final_df.shape}\")\n",
    "    print(f\"Columns added: {final_df.shape[1] - sample_df.shape[1]}\")\n",
    "    print(f\"Rows added: {final_df.shape[0] - sample_df.shape[0]}\")\n",
    "    \n",
    "    # Compile metadata\n",
    "    metadata = {\n",
    "        'original_sample_shape': sample_df.shape,\n",
    "        'original_full_shape': full_df.shape,\n",
    "        'final_shape': final_df.shape,\n",
    "        'categorical_columns_detected': categorical_columns,\n",
    "        'high_cardinality_columns_processed': high_card_columns,\n",
    "        'column_mapping': col_mapping,\n",
    "        'rows_added_from_full_dataset': final_df.shape[0] - sample_df.shape[0]\n",
    "    }\n",
    "    \n",
    "    return final_df, metadata\n",
    "\n",
    "def safe_preprocess_with_fallback(\n",
    "    enhanced_df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    output_dir: str,\n",
    "    cols_to_keep: List[str] = None,\n",
    "    max_categorical_cardinality: int = 10,\n",
    "    standardization_mapping: Dict[str, str] = None,\n",
    "    high_missing_threshold: float = 0.7\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Safe preprocessing function that handles the file_path requirement\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save enhanced dataset to temporary file\n",
    "    temp_enhanced_path = os.path.join(output_dir, 'temp_enhanced_sample.xlsx')\n",
    "    enhanced_df.to_excel(temp_enhanced_path, index=False)\n",
    "    \n",
    "    try:\n",
    "        # Apply preprocessing using existing function\n",
    "        final_df, preprocessing_metadata = preprocess_isbsg_data(\n",
    "            file_path=temp_enhanced_path,\n",
    "            target_col=target_col,\n",
    "            output_dir=output_dir,\n",
    "            cols_to_keep=cols_to_keep,\n",
    "            max_categorical_cardinality=max_categorical_cardinality,\n",
    "            standardization_mapping=standardization_mapping,\n",
    "            high_missing_threshold=high_missing_threshold\n",
    "        )\n",
    "        \n",
    "        return final_df, preprocessing_metadata\n",
    "        \n",
    "    finally:\n",
    "        # Clean up temporary file\n",
    "        try:\n",
    "            os.remove(temp_enhanced_path)\n",
    "        except:\n",
    "            print(f\"Warning: Could not remove temporary file {temp_enhanced_path}\")\n",
    "    \n",
    "    return enhanced_df, {'error': 'Preprocessing failed'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54df81cd-5503-4931-a2b8-2666049d5f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATA_FOLDER = ../data, SAMPLE_FILE = ISBSG2016R1_1_financial_industry_seed.xlsx, FULL_FILE = ISBSG2016R1_1_full_dataset.xlsx, TARGET_COL = project_prf_normalised_work_effort\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Configuration constants (define these at module level)\n",
    "#DATA_FOLDER = \"../data\"  # Update this path as needed\n",
    "#SAMPLE_FILE = \"sample_data.xlsx\"  # Update this filename as needed\n",
    "#FULL_FILE = \"full_data.xlsx\"  # Update this filename as needed\n",
    "#TARGET_COL = \"project_prf_normalised_work_effort\"\n",
    "\n",
    "print(f\"\\nDATA_FOLDER = {DATA_FOLDER}, SAMPLE_FILE = {SAMPLE_FILE}, FULL_FILE = {FULL_FILE}, TARGET_COL = {TARGET_COL}\")\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the integrated pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    sample_file_path = os.path.join(CONFIG_FOLDER, SAMPLE_FILE)\n",
    "    full_file_path = os.path.join(DATA_FOLDER, FULL_FILE)\n",
    "    FINANCE = \"finance\"\n",
    "    \n",
    "\n",
    "        # Columns to exclude (customize as needed)\n",
    "    cols_to_exclude_add_category = [\n",
    "        'external_eef_industry_sector', \n",
    "        'external_eef_organisation_type',\n",
    "        'project_prf_application_type', \n",
    "     ]\n",
    "    \n",
    "    # Columns to keep (customize as needed)\n",
    "    cols_to_keep = [\n",
    "        'Project_PRF_CASE_Tool_Used', \n",
    "        'Process_PMF_Prototyping_Used',\n",
    "        'Tech_TF_Client_Roles', \n",
    "        'Tech_TF_Type_of_Server', \n",
    "        'Tech_TF_ClientServer_Description'\n",
    "    ]\n",
    "    \n",
    "    # High-cardinality multi-value columns: will top_k strategy - Keep only top K most frequent categorical values)\n",
    "    high_card_columns = [\n",
    "        'external_eef_organisation_type', \n",
    "        'project_prf_application_type'\n",
    "    ]\n",
    "    \n",
    "    # Standardization rules\n",
    "    standardization_map = {\n",
    "        'stand alone': 'stand-alone',\n",
    "        'client server': 'client-server',\n",
    "        'mathematically intensive': 'mathematically-intensive',\n",
    "        #'mathematically intensive application': 'mathematically-intensive application',\n",
    "        \"file &/or print server\": \"file/print server\",\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Run integrated pipeline\n",
    "        final_df, metadata = integrated_categorical_preprocessing(\n",
    "            sample_file_path=sample_file_path,\n",
    "            full_file_path=full_file_path,\n",
    "            target_col=TARGET_COL,\n",
    "            output_dir=DATA_FOLDER,\n",
    "            cols_to_keep=cols_to_keep,\n",
    "            high_card_columns=high_card_columns,\n",
    "            max_categorical_cardinality=10,\n",
    "            samples_per_category=3,\n",
    "            standardization_mapping=standardization_map,\n",
    "            high_missing_threshold=0.7,\n",
    "            separator=';',\n",
    "            strategy='top_k',\n",
    "            k=20,\n",
    "            process_high_cardinality=False,\n",
    "            exclude_from_enhancement=cols_to_exclude_add_category\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        if FINANCE in sample_file_path:\n",
    "            output_path = os.path.join(DATA_FOLDER, f\"{FINANCE}_enhanced_sample_final.csv\")\n",
    "        else:\n",
    "            output_path = os.path.join(DATA_FOLDER, 'enhanced_sample_final.csv')\n",
    "            \n",
    "        final_df.to_csv(output_path, index=False)\n",
    "\n",
    "        # Check what columns are actually in final_df\n",
    "        print(f\"\\\\n=== FINAL COLUMN CHECK ===\")\n",
    "        print(f\"Total columns in final CSV: {len(final_df.columns)}\")\n",
    "        print(f\"All columns: {list(final_df.columns)}\")\n",
    "        \n",
    "   \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Final dataset saved to: {output_path}\")\n",
    "        print(f\"Final shape: {final_df.shape}\")\n",
    "        print(f\"Ready for PyCaret setup!\")\n",
    "        \n",
    "        # Print summary of changes\n",
    "        print(f\"\\nSUMMARY:\")\n",
    "        print(f\"- Original sample rows: {metadata['original_sample_shape'][0]}\")\n",
    "        print(f\"- Rows added from full dataset: {metadata['rows_added_from_full_dataset']}\")\n",
    "        print(f\"- Final rows: {metadata['final_shape'][0]}\")\n",
    "        print(f\"- Original columns: {metadata['original_sample_shape'][1]}\")\n",
    "        print(f\"- Final columns: {metadata['final_shape'][1]}\")\n",
    "        \n",
    "        return final_df, metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in integrated pipeline: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bf03c5-31b5-4093-916e-9ca5efc4702b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fa1ff42-b05a-4697-b88a-390c4fac38f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INTEGRATED CATEGORICAL PREPROCESSING PIPELINE\n",
      "============================================================\n",
      "\n",
      "1. Loading datasets...\n",
      "Sample dataset shape: (939, 51)\n",
      "Full dataset shape: (7518, 52)\n",
      "\n",
      "2. Creating preprocessing pipeline...\n",
      "\n",
      "3. Applying initial preprocessing to both datasets...\n",
      "Standardized 26 column names\n",
      "\n",
      "Missing value analysis:\n",
      "Columns with >50% missing: 29\n",
      "Columns with >70% missing: 26\n",
      "Dropping 26 columns with >70.0% missing values\n",
      "Filled project_prf_functional_size missing values with median: 154.5\n",
      "Filled project_prf_normalised_work_effort_level_1 missing values with median: 1550.0\n",
      "Filled project_prf_normalised_work_effort missing values with median: 1652.0\n",
      "Filled project_prf_normalised_level_1_pdr_ufp missing values with median: 11.45\n",
      "Filled project_prf_normalised_pdr_ufp missing values with median: 11.9\n",
      "Filled project_prf_speed_of_delivery missing values with median: 27.05\n",
      "Filled project_prf_project_elapsed_time missing values with median: 6.0\n",
      "Data shape after missing value handling: (939, 25)\n",
      "Found 2 columns with semicolons: ['external_eef_organisation_type', 'project_prf_application_type']\n",
      "Sample after initial preprocessing: (939, 25)\n",
      "Standardized 27 column names\n",
      "\n",
      "Missing value analysis:\n",
      "Columns with >50% missing: 30\n",
      "Columns with >70% missing: 24\n",
      "Dropping 24 columns with >70.0% missing values\n",
      "Filled project_prf_functional_size missing values with median: 139.0\n",
      "Filled project_prf_normalised_work_effort_level_1 missing values with median: 1593.0\n",
      "Filled project_prf_normalised_work_effort missing values with median: 1699.0\n",
      "Filled project_prf_normalised_level_1_pdr_ufp missing values with median: 11.2\n",
      "Filled project_prf_normalised_pdr_ufp missing values with median: 11.6\n",
      "Filled project_prf_speed_of_delivery missing values with median: 26.8\n",
      "Filled project_prf_project_elapsed_time missing values with median: 6.0\n",
      "Filled project_prf_max_team_size missing values with median: 7.0\n",
      "Data shape after missing value handling: (7518, 28)\n",
      "Found 4 columns with semicolons: ['external_eef_organisation_type', 'project_prf_application_group', 'project_prf_application_type', 'process_pmf_development_methodologies']\n",
      "Full dataset after initial preprocessing: (7518, 28)\n",
      "\n",
      "4. Skipping high-cardinality processing (disabled or no columns specified)\n",
      "\n",
      "4.5. Expanding exclude list to include derived columns...\n",
      "  Added derived columns for 'external_eef_industry_sector': ['external_eef_industry_sector']\n",
      "  Added derived columns for 'external_eef_organisation_type': ['external_eef_organisation_type']\n",
      "  Added derived columns for 'project_prf_application_type': ['project_prf_application_type']\n",
      "  Final expanded exclude list: 3 columns\n",
      "\n",
      "5. Identifying categorical columns from preprocessed datasets...\n",
      "Detected categorical columns: 14 columns\n",
      "\n",
      "6. Enhancing sample with missing categories from preprocessed full dataset...\n",
      "Excluding columns from enhancement: ['external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_type']\n",
      "Columns that will be enhanced: 11 out of 14\n",
      "Analyzing missing categories...\n",
      "Excluded columns: ['external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_type']\n",
      "Column 'external_eef_data_quality_rating': All categories present in sample\n",
      "Column 'project_prf_application_group': Missing 7 out of 7 categories\n",
      "  Missing categories: ['real-time application', 'infrastructure software', 'mathematically-intensive application', 'mathematically intensive application', 'business application']...\n",
      "Column 'project_prf_development_type': Missing 2 out of 7 categories\n",
      "  Missing categories: ['Porting', 'Other']\n",
      "Column 'tech_tf_development_platform': Missing 1 out of 7 categories\n",
      "  Missing categories: ['Hand Held']\n",
      "Column 'tech_tf_language_type': Missing 1 out of 7 categories\n",
      "  Missing categories: ['APG']\n",
      "Column 'tech_tf_primary_programming_language': Missing 80 out of 129 categories\n",
      "  Missing categories: ['ColdFusion', 'BPM', 'Azure', 'gcc', 'Adobe Flex']...\n",
      "Column 'project_prf_relative_size': Missing 1 out of 10 categories\n",
      "  Missing categories: ['XXXL']\n",
      "Column 'project_prf_case_tool_used': All categories present in sample\n",
      "Column 'tech_tf_architecture': Missing 2 out of 8 categories\n",
      "  Missing categories: ['Multi-tier with web interface', 'Stand-alone']\n",
      "Column 'tech_tf_client_server': All categories present in sample\n",
      "Column 'tech_tf_dbms_used': All categories present in sample\n",
      "\n",
      "Sampling for column 'project_prf_application_group'...\n",
      "  Skipped 'real-time application': Would violate exclusion constraints\n",
      "  Skipped 'infrastructure software': Would violate exclusion constraints\n",
      "  Skipped 'mathematically-intensive application': Would violate exclusion constraints\n",
      "  Skipped 'mathematically intensive application': Would violate exclusion constraints\n",
      "  Filtered 2 rows to respect exclusions\n",
      "  Added 1 rows for 'business application' (out of 4655 available)\n",
      "  Filtered 2 rows to respect exclusions\n",
      "  Added 1 rows for 'missing' (out of 2311 available)\n",
      "  Skipped 'business application; infrastructure software': Would violate exclusion constraints\n",
      "\n",
      "Sampling for column 'project_prf_development_type'...\n",
      "  Skipped 'Porting': Would violate exclusion constraints\n",
      "  Skipped 'Other': Would violate exclusion constraints\n",
      "\n",
      "Sampling for column 'tech_tf_development_platform'...\n",
      "  Skipped 'Hand Held': Would violate exclusion constraints\n",
      "\n",
      "Sampling for column 'tech_tf_language_type'...\n",
      "  Skipped 'APG': Would violate exclusion constraints\n",
      "\n",
      "Sampling for column 'tech_tf_primary_programming_language'...\n",
      "  Skipped 'ColdFusion': Would violate exclusion constraints\n",
      "  Skipped 'BPM': Would violate exclusion constraints\n",
      "  Skipped 'Azure': Would violate exclusion constraints\n",
      "  Skipped 'gcc': Would violate exclusion constraints\n",
      "  Skipped 'Adobe Flex': Would violate exclusion constraints\n",
      "  Skipped 'Pega Workflows': Would violate exclusion constraints\n",
      "  Skipped 'STAFFWARE': Would violate exclusion constraints\n",
      "  Skipped 'IEF': Would violate exclusion constraints\n",
      "  Skipped 'Huron/Object Star': Would violate exclusion constraints\n",
      "  Skipped 'OutlookVBA': Would violate exclusion constraints\n",
      "  Skipped 'VisualFoxPro': Would violate exclusion constraints\n",
      "  Skipped 'PYTHON': Would violate exclusion constraints\n",
      "  Skipped 'MS-Navision Properitory Language': Would violate exclusion constraints\n",
      "  Skipped 'Siebel': Would violate exclusion constraints\n",
      "  Skipped 'ADO.Net': Would violate exclusion constraints\n",
      "  Skipped 'Delphi': Would violate exclusion constraints\n",
      "  Skipped 'A:G': Would violate exclusion constraints\n",
      "  Skipped 'RPL': Would violate exclusion constraints\n",
      "  Skipped 'COGNOS': Would violate exclusion constraints\n",
      "  Skipped 'BASIC': Would violate exclusion constraints\n",
      "  Skipped 'APPS': Would violate exclusion constraints\n",
      "  Skipped 'INGRES': Would violate exclusion constraints\n",
      "  Skipped 'BEA Weblogic': Would violate exclusion constraints\n",
      "  Skipped 'FORTRAN': Would violate exclusion constraints\n",
      "  Skipped 'ARBOR/BP': Would violate exclusion constraints\n",
      "  Skipped 'UNIFACE': Would violate exclusion constraints\n",
      "  Skipped 'Data base language': Would violate exclusion constraints\n",
      "  Skipped 'Periproducer': Would violate exclusion constraints\n",
      "  Skipped 'COOL:Gen': Would violate exclusion constraints\n",
      "  Skipped 'Centura': Would violate exclusion constraints\n",
      "  Skipped 'Datastage': Would violate exclusion constraints\n",
      "  Skipped 'Ada': Would violate exclusion constraints\n",
      "  Skipped 'Upfront': Would violate exclusion constraints\n",
      "  Skipped 'SAS': Would violate exclusion constraints\n",
      "  Skipped 'Doc1 Designer (Entorno visual)': Would violate exclusion constraints\n",
      "  Skipped 'Informatica PowerCenter': Would violate exclusion constraints\n",
      "  Skipped 'ACCEL': Would violate exclusion constraints\n",
      "  Skipped 'Visual Studio .Net': Would violate exclusion constraints\n",
      "  Skipped 'SLOGAN': Would violate exclusion constraints\n",
      "  Skipped 'Enablon': Would violate exclusion constraints\n",
      "  Skipped 'Caa': Would violate exclusion constraints\n",
      "  Skipped 'HPS': Would violate exclusion constraints\n",
      "  Skipped 'J2EE': Would violate exclusion constraints\n",
      "  Skipped 'MATLAB': Would violate exclusion constraints\n",
      "  Skipped 'LISP': Would violate exclusion constraints\n",
      "  Skipped 'BRE': Would violate exclusion constraints\n",
      "  Skipped 'XGML': Would violate exclusion constraints\n",
      "  Skipped 'BO': Would violate exclusion constraints\n",
      "  Skipped 'Express': Would violate exclusion constraints\n",
      "  Skipped 'AB INITIO': Would violate exclusion constraints\n",
      "  Skipped 'iPlanet Netscape Application Server': Would violate exclusion constraints\n",
      "  Skipped 'PERIPHONICS': Would violate exclusion constraints\n",
      "  Skipped 'EJB': Would violate exclusion constraints\n",
      "  Skipped 'PHP': Would violate exclusion constraints\n",
      "  Skipped 'TNSDL': Would violate exclusion constraints\n",
      "  Skipped 'IBM WTX': Would violate exclusion constraints\n",
      "  Skipped 'ADS/Online': Would violate exclusion constraints\n",
      "  Skipped 'Magic': Would violate exclusion constraints\n",
      "  Skipped 'Object oriented language': Would violate exclusion constraints\n",
      "  Skipped 'REXX': Would violate exclusion constraints\n",
      "  Skipped 'ABF': Would violate exclusion constraints\n",
      "  Skipped 'MANTIS': Would violate exclusion constraints\n",
      "  Skipped 'ASAP': Would violate exclusion constraints\n",
      "  Skipped 'NCR teradata scripting': Would violate exclusion constraints\n",
      "  Skipped 'Jdeveloper': Would violate exclusion constraints\n",
      "  Skipped 'C/AL': Would violate exclusion constraints\n",
      "  Skipped 'IIS': Would violate exclusion constraints\n",
      "  Skipped 'LEX': Would violate exclusion constraints\n",
      "  Skipped 'Perl': Would violate exclusion constraints\n",
      "  Skipped 'IDEAL': Would violate exclusion constraints\n",
      "  Skipped 'Must Modeller': Would violate exclusion constraints\n",
      "  Skipped 'Brightware proprietary': Would violate exclusion constraints\n",
      "  Skipped 'SLEL': Would violate exclusion constraints\n",
      "  Skipped 'PASCAL': Would violate exclusion constraints\n",
      "  Skipped 'Spreadsheet': Would violate exclusion constraints\n",
      "  Skipped 'Formspath': Would violate exclusion constraints\n",
      "  Skipped 'DRIFT': Would violate exclusion constraints\n",
      "  Skipped 'PowerPlay': Would violate exclusion constraints\n",
      "  Skipped 'Mendix': Would violate exclusion constraints\n",
      "  Skipped 'CICS': Would violate exclusion constraints\n",
      "\n",
      "Sampling for column 'project_prf_relative_size'...\n",
      "  Skipped 'XXXL': Would violate exclusion constraints\n",
      "\n",
      "Sampling for column 'tech_tf_architecture'...\n",
      "  Skipped 'Multi-tier with web interface': Would violate exclusion constraints\n",
      "  Skipped 'Stand-alone': Would violate exclusion constraints\n",
      "\n",
      "=== SUMMARY ===\n",
      "Original sample size: 939\n",
      "Additional rows added: 2\n",
      "Final dataset size: 941\n",
      "Size increase: 0.2%\n",
      "Enhanced dataset shape: (941, 28)\n",
      "\n",
      "6.5. Restoring excluded columns that were lost during high-cardinality processing...\n",
      "✅ All excluded columns are already present\n",
      "\n",
      "7. Verifying categories coverage...\n",
      "\n",
      "=== CATEGORY COVERAGE VERIFICATION ===\n",
      "\n",
      "Column 'external_eef_data_quality_rating':\n",
      "  Before: 4 categories\n",
      "  After:  4 categories\n",
      "  ➖ No enhancement needed\n",
      "\n",
      "Column 'external_eef_industry_sector':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "  ✅ EXCLUDED column preserved (no new categories)\n",
      "\n",
      "Column 'external_eef_organisation_type':\n",
      "  Before: 16 categories\n",
      "  After:  16 categories\n",
      "  ✅ EXCLUDED column preserved (no new categories)\n",
      "\n",
      "Column 'project_prf_application_group':\n",
      "  Before: 5 categories\n",
      "  After:  7 categories\n",
      "  ✅ Enhanced with 2 new categories: ['missing', 'business application']\n",
      "\n",
      "Column 'project_prf_application_type':\n",
      "  Before: 51 categories\n",
      "  After:  51 categories\n",
      "  ✅ EXCLUDED column preserved (no new categories)\n",
      "\n",
      "Column 'project_prf_development_type':\n",
      "  Before: 5 categories\n",
      "  After:  5 categories\n",
      "  ➖ No enhancement needed\n",
      "\n",
      "Column 'tech_tf_development_platform':\n",
      "  Before: 6 categories\n",
      "  After:  6 categories\n",
      "  ➖ No enhancement needed\n",
      "\n",
      "Column 'tech_tf_language_type':\n",
      "  Before: 6 categories\n",
      "  After:  6 categories\n",
      "  ➖ No enhancement needed\n",
      "\n",
      "Column 'tech_tf_primary_programming_language':\n",
      "  Before: 49 categories\n",
      "  After:  49 categories\n",
      "  ➖ No enhancement needed\n",
      "\n",
      "Column 'project_prf_relative_size':\n",
      "  Before: 9 categories\n",
      "  After:  9 categories\n",
      "  ➖ No enhancement needed\n",
      "\n",
      "Column 'project_prf_case_tool_used':\n",
      "  Before: 4 categories\n",
      "  After:  4 categories\n",
      "  ➖ No enhancement needed\n",
      "\n",
      "Column 'tech_tf_architecture':\n",
      "  Before: 6 categories\n",
      "  After:  6 categories\n",
      "  ➖ No enhancement needed\n",
      "\n",
      "Column 'tech_tf_client_server':\n",
      "  Before: 5 categories\n",
      "  After:  5 categories\n",
      "  ➖ No enhancement needed\n",
      "\n",
      "Column 'tech_tf_dbms_used':\n",
      "  Before: 3 categories\n",
      "  After:  3 categories\n",
      "  ➖ No enhancement needed\n",
      "\n",
      "8. Applying final preprocessing stages...\n",
      "[PreservingMultiValueEncoder] Preserving column 'external_eef_industry_sector'\n",
      "[PreservingMultiValueEncoder] Preserving column 'external_eef_organisation_type'\n",
      "[PreservingMultiValueEncoder] Preserving column 'project_prf_application_type'\n",
      "Encoding 0 multi-value columns: []\n",
      "[PreservingCategoricalEncoder] Preserving column 'external_eef_industry_sector'\n",
      "[PreservingCategoricalEncoder] Preserving column 'external_eef_organisation_type'\n",
      "[PreservingCategoricalEncoder] Preserving column 'project_prf_application_type'\n",
      "[PreservingCategoricalEncoder] Will encode 12 columns (excluding 3 preserved)\n",
      "No duplicate column names after fixing\n",
      "Fixed 12 column names for PyCaret compatibility\n",
      "\n",
      "=== Final Data Validation ===\n",
      "Final shape: (941, 62)\n",
      "Target column: project_prf_normalised_work_effort\n",
      "Total missing values: 939\n",
      "Total infinite values: 0\n",
      "\n",
      "Data types:\n",
      "  Numeric columns: 12\n",
      "  Categorical columns: 4\n",
      "\n",
      "Target variable 'project_prf_normalised_work_effort' statistics:\n",
      "  Mean: 4139.71\n",
      "  Std: 9330.86\n",
      "  Min: 6.00\n",
      "  Max: 134211.00\n",
      "  Missing: 0\n",
      "\n",
      "9. Final validation and duplicate check...\n",
      "Original sample shape: (939, 51)\n",
      "Final processed shape: (941, 62)\n",
      "Columns added: 11\n",
      "Rows added: 2\n",
      "\\n=== FINAL COLUMN CHECK ===\n",
      "Total columns in final CSV: 62\n",
      "All columns: ['isbsg_project_id', 'project_prf_year_of_project', 'external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_type', 'tech_tf_primary_programming_language', 'project_prf_functional_size', 'project_prf_normalised_work_effort_level_1', 'project_prf_normalised_work_effort', 'project_prf_normalised_level_1_pdr_ufp', 'project_prf_normalised_pdr_ufp', 'project_prf_speed_of_delivery', 'project_prf_project_elapsed_time', 'process_pmf_docs', 'tech_tf_tools_used', 'project_prf_max_team_size', 'external_eef_data_quality_rating_B', 'external_eef_data_quality_rating_C', 'external_eef_data_quality_rating_D', 'project_prf_application_group_Infrastructure_Software', 'project_prf_application_group_Mathematically_Intensive_Application', 'project_prf_application_group_Missing', 'project_prf_application_group_Real_Time_Application', 'project_prf_application_group_business_application', 'project_prf_application_group_missing', 'project_prf_development_type_New_Development', 'project_prf_development_type_Not_Defined', 'project_prf_development_type_POC', 'project_prf_development_type_Re_development', 'tech_tf_development_platform_MR', 'tech_tf_development_platform_Missing', 'tech_tf_development_platform_Multi', 'tech_tf_development_platform_PC', 'tech_tf_development_platform_Proprietary', 'tech_tf_language_type_3GL', 'tech_tf_language_type_4GL', 'tech_tf_language_type_5GL', 'tech_tf_language_type_ApG', 'tech_tf_language_type_Missing', 'project_prf_relative_size_M1', 'project_prf_relative_size_M2', 'project_prf_relative_size_Missing', 'project_prf_relative_size_S', 'project_prf_relative_size_XL', 'project_prf_relative_size_XS', 'project_prf_relative_size_XXL', 'project_prf_relative_size_XXS', 'project_prf_case_tool_used_Missing', 'project_prf_case_tool_used_No', 'project_prf_case_tool_used_Yes', 'tech_tf_architecture_Missing', 'tech_tf_architecture_Multi_tier', 'tech_tf_architecture_Multi_tier_Client_server', 'tech_tf_architecture_Multi_tier_with_web_public_interface', 'tech_tf_architecture_Stand_alone', 'tech_tf_client_server_Missing', 'tech_tf_client_server_No', 'tech_tf_client_server_Not_Applicable', 'tech_tf_client_server_Yes', 'tech_tf_dbms_used_No', 'tech_tf_dbms_used_Yes', 'project_prf_team_size_group_Missing']\n",
      "\n",
      "============================================================\n",
      "PIPELINE COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "Final dataset saved to: ../data\\enhanced_sample_final.csv\n",
      "Final shape: (941, 62)\n",
      "Ready for PyCaret setup!\n",
      "\n",
      "SUMMARY:\n",
      "- Original sample rows: 939\n",
      "- Rows added from full dataset: 2\n",
      "- Final rows: 941\n",
      "- Original columns: 51\n",
      "- Final columns: 62\n"
     ]
    }
   ],
   "source": [
    "# Run the main function when script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    final_df, metadata = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ac3a9-bc0e-41c9-9124-3dd2be13e40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c520c646-d7cf-483f-aa5c-659eb3e8fbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb563e26-7450-4f38-9221-912aecd07190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8b117-c932-418a-b982-44c989db2612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61685304-5a54-4daf-9872-1e33ba246218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0966f-dd74-4d16-9b63-f3af45b43ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad9699-f16e-489e-9725-d38f2f22417d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262c7f6-cc14-442e-9fe8-7079e36344f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71847fde-1fe0-4d2f-83b3-2d2ba389dea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f02c95-8a89-4f2c-adfd-21894c2692ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c18ad0-d0a0-41b1-89e7-f459e5cc956d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead2007-b5c9-4e98-8ebe-640698dc3f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd6291-dd67-4a80-8851-a751e261a45e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
