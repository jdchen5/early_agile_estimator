{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f30650-ed19-4abf-bb03-5ef4005a32cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e08f925-be6c-46dc-92b4-da99269eda09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Complete Scikit-Learn Preprocessing Pipeline for ISBSG Data\\n    ===========================================================\\n\\n    This module provides a comprehensive preprocessing pipeline that handles:\\n    1. Data loading and initial cleaning\\n    2. Column name standardization\\n    3. Missing value handling\\n    4. Semicolon-separated value processing\\n    5. One-hot encoding for categorical variables\\n    6. Multi-label binarization for multi-value columns\\n    7. Feature selection and filtering\\n    8. Data validation and export\\n\\n    Based on the preprocessing steps from the provided notebooks.\\n   '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Complete Scikit-Learn Preprocessing Pipeline for ISBSG Data\n",
    "    ===========================================================\n",
    "    \n",
    "    This module provides a comprehensive preprocessing pipeline that handles:\n",
    "    1. Data loading and initial cleaning\n",
    "    2. Column name standardization\n",
    "    3. Missing value handling\n",
    "    4. Semicolon-separated value processing\n",
    "    5. One-hot encoding for categorical variables\n",
    "    6. Multi-label binarization for multi-value columns\n",
    "    7. Feature selection and filtering\n",
    "    8. Data validation and export\n",
    "    \n",
    "    Based on the preprocessing steps from the provided notebooks.\n",
    "   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2848fe58-b966-47fe-9339-6eea3764c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Any, Tuple\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b649e731-d0f9-4bf8-95ed-428c9c818aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_FOLDER = \"../data\"\n",
    "CONFIG_FOLDER = \"../config\"\n",
    "SAMPLE_FILE = \"ISBSG2016R1_1_financial_industry_seed.xlsx\"\n",
    "FULL_FILE = \"ISBSG2016R1_1_full_dataset.xlsx\"\n",
    "TARGET_COL = \"project_prf_normalised_work_effort\"  # be careful about case sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9986b36b-4364-4284-9381-c6ae3d99abd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def analyze_high_cardinality_multivalue(df, column, separator=';'):\n",
    "    \"\"\"\n",
    "    Analyze high-cardinality multi-value columns to choose best strategy\n",
    "    \"\"\"\n",
    "    print(f\"=== ANALYSIS FOR HIGH-CARDINALITY COLUMN: '{column}' ===\\n\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    non_null_data = df[column].dropna().astype(str)\n",
    "    split_values = non_null_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Get all unique values\n",
    "    all_values = []\n",
    "    for values_list in split_values:\n",
    "        all_values.extend(values_list)\n",
    "    \n",
    "    value_counts = Counter(all_values)\n",
    "    unique_values = list(value_counts.keys())\n",
    "    \n",
    "    print(f\"Total unique values: {len(unique_values)}\")\n",
    "    print(f\"Total value occurrences: {len(all_values)}\")\n",
    "    print(f\"Average values per row: {len(all_values) / len(split_values):.2f}\")\n",
    "    \n",
    "    # Show most common values\n",
    "    print(f\"\\nTop 15 most common values:\")\n",
    "    for value, count in value_counts.most_common(15):\n",
    "        percentage = (count / len(non_null_data)) * 100\n",
    "        print(f\"  '{value}': {count} times ({percentage:.1f}% of rows)\")\n",
    "    \n",
    "    # Show distribution of value frequencies\n",
    "    frequency_dist = Counter(value_counts.values())\n",
    "    print(f\"\\nFrequency distribution:\")\n",
    "    for freq, count in sorted(frequency_dist.items(), reverse=True)[:10]:\n",
    "        print(f\"  {count} values appear {freq} time(s)\")\n",
    "    \n",
    "    # Values per row distribution\n",
    "    values_per_row = split_values.apply(len)\n",
    "    print(f\"\\nValues per row:\")\n",
    "    print(f\"  Min: {values_per_row.min()}\")\n",
    "    print(f\"  Max: {values_per_row.max()}\")\n",
    "    print(f\"  Mean: {values_per_row.mean():.2f}\")\n",
    "    print(f\"  Median: {values_per_row.median():.2f}\")\n",
    "    \n",
    "    return value_counts, unique_values\n",
    "\n",
    "\n",
    "def handle_high_cardinality_multivalue(df, multi_value_columns, separator=';', strategy='top_k', **kwargs):\n",
    "    \"\"\"\n",
    "    Handle high-cardinality multi-value columns with various strategies\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    strategy options:\n",
    "    - 'top_k': Keep only top K most frequent values (k=kwargs['k'])\n",
    "    - 'frequency_threshold': Keep values that appear in at least X% of rows (threshold=kwargs['threshold'])\n",
    "    - 'tfidf': Use TF-IDF vectorization with dimensionality reduction (n_components=kwargs['n_components'])\n",
    "    - 'count_features': Simple counting features (count, unique_count, most_common)\n",
    "    - 'embedding': Create category embeddings (requires pre-trained embeddings)\n",
    "    \"\"\"\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    new_columns_mapping = {}\n",
    "    \n",
    "    for col in multi_value_columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing high-cardinality column '{col}' with strategy '{strategy}'...\")\n",
    "        \n",
    "        # Clean and split values\n",
    "        split_values = df[col].fillna('').astype(str).apply(\n",
    "            lambda x: [val.strip() for val in x.split(separator) if val.strip()]\n",
    "        )\n",
    "        \n",
    "        # Get value counts\n",
    "        all_values = []\n",
    "        for values_list in split_values:\n",
    "            all_values.extend(values_list)\n",
    "        value_counts = Counter(all_values)\n",
    "        \n",
    "        if strategy == 'top_k':\n",
    "            k = kwargs.get('k', 20)  # Default to top 20\n",
    "            top_values = [val for val, count in value_counts.most_common(k)]\n",
    "            \n",
    "            new_col_names = []\n",
    "            for value in top_values:\n",
    "                new_col_name = f\"{col}_top_{value}\".replace(' ', '_').replace('-', '_')\n",
    "                df_processed[new_col_name] = split_values.apply(lambda x: 1 if value in x else 0)\n",
    "                new_col_names.append(new_col_name)\n",
    "            \n",
    "            # Add \"other\" category for remaining values\n",
    "            other_col_name = f\"{col}_other\"\n",
    "            df_processed[other_col_name] = split_values.apply(\n",
    "                lambda x: 1 if any(val not in top_values for val in x) else 0\n",
    "            )\n",
    "            new_col_names.append(other_col_name)\n",
    "            \n",
    "            new_columns_mapping[col] = new_col_names\n",
    "            print(f\"  Created {len(new_col_names)} columns (top {k} + other)\")\n",
    "            \n",
    "        elif strategy == 'frequency_threshold':\n",
    "            threshold = kwargs.get('threshold', 0.05)  # Default 5%\n",
    "            min_occurrences = int(len(df) * threshold)\n",
    "            \n",
    "            frequent_values = [val for val, count in value_counts.items() if count >= min_occurrences]\n",
    "            \n",
    "            new_col_names = []\n",
    "            for value in frequent_values:\n",
    "                new_col_name = f\"{col}_freq_{value}\".replace(' ', '_').replace('-', '_')\n",
    "                df_processed[new_col_name] = split_values.apply(lambda x: 1 if value in x else 0)\n",
    "                new_col_names.append(new_col_name)\n",
    "            \n",
    "            # Add rare category\n",
    "            rare_col_name = f\"{col}_rare\"\n",
    "            df_processed[rare_col_name] = split_values.apply(\n",
    "                lambda x: 1 if any(val not in frequent_values for val in x) else 0\n",
    "            )\n",
    "            new_col_names.append(rare_col_name)\n",
    "            \n",
    "            new_columns_mapping[col] = new_col_names\n",
    "            print(f\"  Created {len(new_col_names)} columns ({len(frequent_values)} frequent + rare)\")\n",
    "            \n",
    "        elif strategy == 'count_features':\n",
    "            # Create aggregate features instead of individual columns\n",
    "            new_col_names = []\n",
    "            \n",
    "            # Total count of values\n",
    "            count_col = f\"{col}_count\"\n",
    "            df_processed[count_col] = split_values.apply(len)\n",
    "            new_col_names.append(count_col)\n",
    "            \n",
    "            # Unique count (in case of duplicates)\n",
    "            unique_count_col = f\"{col}_unique_count\"\n",
    "            df_processed[unique_count_col] = split_values.apply(lambda x: len(set(x)))\n",
    "            new_col_names.append(unique_count_col)\n",
    "            \n",
    "            # Most common value in the dataset appears in this row\n",
    "            most_common_value = value_counts.most_common(1)[0][0] if value_counts else None\n",
    "            if most_common_value:\n",
    "                most_common_col = f\"{col}_has_most_common\"\n",
    "                df_processed[most_common_col] = split_values.apply(lambda x: 1 if most_common_value in x else 0)\n",
    "                new_col_names.append(most_common_col)\n",
    "            \n",
    "            # Average frequency of values in this row\n",
    "            avg_freq_col = f\"{col}_avg_frequency\"\n",
    "            df_processed[avg_freq_col] = split_values.apply(\n",
    "                lambda x: np.mean([value_counts[val] for val in x]) if x else 0\n",
    "            )\n",
    "            new_col_names.append(avg_freq_col)\n",
    "            \n",
    "            new_columns_mapping[col] = new_col_names\n",
    "            print(f\"  Created {len(new_col_names)} aggregate feature columns\")\n",
    "            \n",
    "        elif strategy == 'tfidf':\n",
    "            n_components = kwargs.get('n_components', 10)  # Default to 10 components\n",
    "            \n",
    "            # Convert to text format for TF-IDF\n",
    "            text_data = split_values.apply(lambda x: ' '.join(x))\n",
    "            \n",
    "            # Apply TF-IDF\n",
    "            tfidf = TfidfVectorizer(max_features=100, stop_words=None)\n",
    "            tfidf_matrix = tfidf.fit_transform(text_data)\n",
    "            \n",
    "            # Reduce dimensionality\n",
    "            pca = PCA(n_components=n_components)\n",
    "            tfidf_reduced = pca.fit_transform(tfidf_matrix.toarray())\n",
    "            \n",
    "            # Create new columns\n",
    "            new_col_names = []\n",
    "            for i in range(n_components):\n",
    "                new_col_name = f\"{col}_tfidf_comp_{i+1}\"\n",
    "                df_processed[new_col_name] = tfidf_reduced[:, i]\n",
    "                new_col_names.append(new_col_name)\n",
    "            \n",
    "            new_columns_mapping[col] = new_col_names\n",
    "            print(f\"  Created {len(new_col_names)} TF-IDF component columns\")\n",
    "            print(f\"  Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "            \n",
    "        elif strategy == 'hierarchical':\n",
    "            # Group similar values into higher-level categories\n",
    "            # This requires domain knowledge - example implementation\n",
    "            hierarchy = kwargs.get('hierarchy', {})  # Dictionary mapping values to categories\n",
    "            \n",
    "            if not hierarchy:\n",
    "                print(\"  Warning: No hierarchy provided for hierarchical strategy\")\n",
    "                continue\n",
    "            \n",
    "            # Create columns for each high-level category\n",
    "            categories = set(hierarchy.values())\n",
    "            new_col_names = []\n",
    "            \n",
    "            for category in categories:\n",
    "                category_values = [val for val, cat in hierarchy.items() if cat == category]\n",
    "                new_col_name = f\"{col}_category_{category}\".replace(' ', '_')\n",
    "                df_processed[new_col_name] = split_values.apply(\n",
    "                    lambda x: 1 if any(val in category_values for val in x) else 0\n",
    "                )\n",
    "                new_col_names.append(new_col_name)\n",
    "            \n",
    "            new_columns_mapping[col] = new_col_names\n",
    "            print(f\"  Created {len(new_col_names)} hierarchical category columns\")\n",
    "        \n",
    "        # Remove original column\n",
    "        df_processed = df_processed.drop(columns=[col])\n",
    "    \n",
    "    return df_processed, new_columns_mapping\n",
    "\n",
    "\n",
    "def recommend_strategy(df, column, separator=';'):\n",
    "    \"\"\"\n",
    "    Recommend the best strategy based on data characteristics\n",
    "    \"\"\"\n",
    "    value_counts, unique_values = analyze_high_cardinality_multivalue(df, column, separator)\n",
    "    \n",
    "    total_unique = len(unique_values)\n",
    "    total_rows = len(df[column].dropna())\n",
    "    \n",
    "    print(f\"\\n=== STRATEGY RECOMMENDATIONS FOR '{column}' ===\")\n",
    "    \n",
    "    if total_unique > 100:\n",
    "        print(\"ðŸ”´ VERY HIGH CARDINALITY (100+ unique values)\")\n",
    "        print(\"Recommended strategies:\")\n",
    "        print(\"1. 'count_features' - Create aggregate features (safest)\")\n",
    "        print(\"2. 'top_k' with k=15-25 - Keep only most important values\")\n",
    "        print(\"3. 'tfidf' with n_components=5-10 - If values have semantic meaning\")\n",
    "        \n",
    "    elif total_unique > 50:\n",
    "        print(\"ðŸŸ¡ HIGH CARDINALITY (50+ unique values)\")\n",
    "        print(\"Recommended strategies:\")\n",
    "        print(\"1. 'top_k' with k=20-30 - Keep most frequent values\")\n",
    "        print(\"2. 'frequency_threshold' with threshold=0.02-0.05\")\n",
    "        print(\"3. 'count_features' - If you want aggregate information\")\n",
    "        \n",
    "    else:\n",
    "        print(\"ðŸŸ¢ MODERATE CARDINALITY (<50 unique values)\")\n",
    "        print(\"Recommended strategies:\")\n",
    "        print(\"1. 'frequency_threshold' with threshold=0.01\")\n",
    "        print(\"2. 'top_k' with k=30-40\")\n",
    "        print(\"3. Binary encoding might be acceptable\")\n",
    "    \n",
    "    # Check frequency distribution\n",
    "    freq_values = list(value_counts.values())\n",
    "    if max(freq_values) / min(freq_values) > 100:\n",
    "        print(\"\\nâš ï¸  HIGHLY SKEWED DISTRIBUTION detected\")\n",
    "        print(\"   Consider 'frequency_threshold' or 'top_k' strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24140d94-4ab5-4228-aa1f-cc0fc2081b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_multivalue_processing(df_original, df_processed, original_column, new_columns, separator=';', strategy='top_k'):\n",
    "    \"\"\"\n",
    "    Comprehensive validation of multi-value categorical processing\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_original : pd.DataFrame\n",
    "        Original dataset before processing\n",
    "    df_processed : pd.DataFrame  \n",
    "        Processed dataset after handling multi-value columns\n",
    "    original_column : str\n",
    "        Name of original multi-value column\n",
    "    new_columns : list\n",
    "        List of new column names created from the original column\n",
    "    separator : str\n",
    "        Separator used in original data\n",
    "    strategy : str\n",
    "        Strategy used for processing\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"=== VALIDATION REPORT FOR COLUMN '{original_column}' ===\\n\")\n",
    "    \n",
    "    # 1. BASIC CHECKS\n",
    "    print(\"1. BASIC INTEGRITY CHECKS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check row count consistency\n",
    "    original_rows = len(df_original)\n",
    "    processed_rows = len(df_processed)\n",
    "    print(f\"âœ“ Row count: {original_rows} â†’ {processed_rows} {'âœ“ SAME' if original_rows == processed_rows else 'âš ï¸  DIFFERENT'}\")\n",
    "    \n",
    "    # Check if original column was removed\n",
    "    original_removed = original_column not in df_processed.columns\n",
    "    print(f\"âœ“ Original column removed: {'âœ“ YES' if original_removed else 'âš ï¸  NO'}\")\n",
    "    \n",
    "    # Check if new columns exist\n",
    "    new_cols_exist = all(col in df_processed.columns for col in new_columns)\n",
    "    print(f\"âœ“ New columns created: {'âœ“ YES' if new_cols_exist else 'âŒ NO'} ({len(new_columns)} columns)\")\n",
    "    \n",
    "    if not new_cols_exist:\n",
    "        missing_cols = [col for col in new_columns if col not in df_processed.columns]\n",
    "        print(f\"  Missing columns: {missing_cols}\")\n",
    "        return False\n",
    "    \n",
    "    # 2. DATA CONSISTENCY CHECKS\n",
    "    print(f\"\\n2. DATA CONSISTENCY CHECKS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Get all unique values from original\n",
    "    all_original_values = set()\n",
    "    for values_list in split_original:\n",
    "        all_original_values.update(values_list)\n",
    "    all_original_values = sorted([v for v in all_original_values if v and v != 'nan'])\n",
    "    \n",
    "    print(f\"Original unique values: {len(all_original_values)}\")\n",
    "    \n",
    "    if strategy == 'top_k':\n",
    "        # Validate top-k strategy\n",
    "        validate_top_k_strategy(df_original, df_processed, original_column, new_columns, separator)\n",
    "    elif strategy == 'count_features':\n",
    "        validate_count_features_strategy(df_original, df_processed, original_column, new_columns, separator)\n",
    "    elif strategy == 'frequency_threshold':\n",
    "        validate_frequency_threshold_strategy(df_original, df_processed, original_column, new_columns, separator)\n",
    "    \n",
    "    # 3. SAMPLE VALIDATION\n",
    "    print(f\"\\n3. SAMPLE-BY-SAMPLE VALIDATION\")\n",
    "    print(\"-\" * 40)\n",
    "    validate_sample_rows(df_original, df_processed, original_column, new_columns, separator, n_samples=5)\n",
    "    \n",
    "    # 4. STATISTICAL VALIDATION\n",
    "    print(f\"\\n4. STATISTICAL VALIDATION\")\n",
    "    print(\"-\" * 40)\n",
    "    validate_statistics(df_original, df_processed, original_column, new_columns, separator)\n",
    "    \n",
    "    # 5. INFORMATION LOSS ASSESSMENT\n",
    "    print(f\"\\n5. INFORMATION LOSS ASSESSMENT\")\n",
    "    print(\"-\" * 40)\n",
    "    assess_information_loss(df_original, df_processed, original_column, new_columns, separator)\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_top_k_strategy(df_original, df_processed, original_column, new_columns, separator, k=None):\n",
    "    \"\"\"Validate top-k strategy specifically\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Get value counts\n",
    "    all_values = []\n",
    "    for values_list in split_original:\n",
    "        all_values.extend(values_list)\n",
    "    value_counts = Counter(all_values)\n",
    "    \n",
    "    # Determine k if not provided\n",
    "    if k is None:\n",
    "        # Exclude \"other\" column to determine k\n",
    "        non_other_cols = [col for col in new_columns if not col.endswith('_other')]\n",
    "        k = len(non_other_cols)\n",
    "    \n",
    "    top_k_values = [val for val, count in value_counts.most_common(k)]\n",
    "    print(f\"Top {k} values: {top_k_values[:5]}{'...' if len(top_k_values) > 5 else ''}\")\n",
    "    \n",
    "    # Check each top-k column\n",
    "    for col in new_columns:\n",
    "        if col.endswith('_other'):\n",
    "            # Validate \"other\" column\n",
    "            validate_other_column(df_original, df_processed, original_column, col, top_k_values, separator)\n",
    "        else:\n",
    "            # Extract the value name from column name\n",
    "            value_name = col.replace(f\"{original_column}_top_\", \"\").replace(f\"{original_column}_\", \"\")\n",
    "            validate_binary_column(df_original, df_processed, original_column, col, value_name, separator)\n",
    "\n",
    "\n",
    "def validate_binary_column(df_original, df_processed, original_column, new_column, value_name, separator):\n",
    "    \"\"\"Validate a single binary column\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Expected values: 1 if value_name in the list, 0 otherwise\n",
    "    expected = split_original.apply(lambda x: 1 if value_name in x else 0)\n",
    "    actual = df_processed[new_column]\n",
    "    \n",
    "    # Compare\n",
    "    matches = (expected == actual).sum()\n",
    "    total = len(expected)\n",
    "    match_rate = matches / total * 100\n",
    "    \n",
    "    print(f\"  '{new_column}': {matches}/{total} matches ({match_rate:.1f}%)\")\n",
    "    \n",
    "    if match_rate < 100:\n",
    "        mismatches = df_original.loc[expected != actual, original_column].head(3)\n",
    "        print(f\"    Sample mismatches: {list(mismatches)}\")\n",
    "\n",
    "\n",
    "def validate_other_column(df_original, df_processed, original_column, other_column, top_values, separator):\n",
    "    \"\"\"Validate the 'other' category column\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Expected: 1 if any value is NOT in top_values, 0 if all values are in top_values\n",
    "    expected = split_original.apply(lambda x: 1 if any(val not in top_values for val in x) else 0)\n",
    "    actual = df_processed[other_column]\n",
    "    \n",
    "    matches = (expected == actual).sum()\n",
    "    total = len(expected)\n",
    "    match_rate = matches / total * 100\n",
    "    \n",
    "    print(f\"  '{other_column}': {matches}/{total} matches ({match_rate:.1f}%)\")\n",
    "\n",
    "\n",
    "def validate_count_features_strategy(df_original, df_processed, original_column, new_columns, separator):\n",
    "    \"\"\"Validate count features strategy\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    for col in new_columns:\n",
    "        if col.endswith('_count'):\n",
    "            # Validate total count\n",
    "            expected = split_original.apply(len)\n",
    "            actual = df_processed[col]\n",
    "            matches = (expected == actual).sum()\n",
    "            print(f\"  '{col}': {matches}/{len(expected)} matches ({matches/len(expected)*100:.1f}%)\")\n",
    "            \n",
    "        elif col.endswith('_unique_count'):\n",
    "            # Validate unique count\n",
    "            expected = split_original.apply(lambda x: len(set(x)))\n",
    "            actual = df_processed[col]\n",
    "            matches = (expected == actual).sum()\n",
    "            print(f\"  '{col}': {matches}/{len(expected)} matches ({matches/len(expected)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "def validate_frequency_threshold_strategy(df_original, df_processed, original_column, new_columns, separator):\n",
    "    \"\"\"Validate frequency threshold strategy\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Get value counts\n",
    "    all_values = []\n",
    "    for values_list in split_original:\n",
    "        all_values.extend(values_list)\n",
    "    value_counts = Counter(all_values)\n",
    "    \n",
    "    for col in new_columns:\n",
    "        if col.endswith('_rare'):\n",
    "            # Validate rare column - similar to other column validation\n",
    "            continue\n",
    "        else:\n",
    "            # Extract the value name from column name\n",
    "            value_name = col.replace(f\"{original_column}_freq_\", \"\").replace(f\"{original_column}_\", \"\")\n",
    "            validate_binary_column(df_original, df_processed, original_column, col, value_name, separator)\n",
    "\n",
    "\n",
    "def validate_sample_rows(df_original, df_processed, original_column, new_columns, separator, n_samples=5):\n",
    "    \"\"\"Manually validate a few sample rows\"\"\"\n",
    "    \n",
    "    print(f\"Validating {n_samples} random samples:\")\n",
    "    \n",
    "    # Get random sample indices\n",
    "    sample_indices = np.random.choice(len(df_original), min(n_samples, len(df_original)), replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices, 1):\n",
    "        original_value = df_original.iloc[idx][original_column]\n",
    "        if pd.isna(original_value):\n",
    "            original_values = []\n",
    "        else:\n",
    "            original_values = [v.strip() for v in str(original_value).split(separator) if v.strip()]\n",
    "        \n",
    "        print(f\"\\n  Sample {i} (row {idx}):\")\n",
    "        print(f\"    Original: '{original_value}'\")\n",
    "        print(f\"    Parsed: {original_values}\")\n",
    "        \n",
    "        # Check new columns for this row\n",
    "        for col in new_columns[:5]:  # Show first 5 columns only\n",
    "            processed_value = df_processed.iloc[idx][col]\n",
    "            print(f\"    {col}: {processed_value}\")\n",
    "\n",
    "\n",
    "def validate_statistics(df_original, df_processed, original_column, new_columns, separator):\n",
    "    \"\"\"Validate statistical properties\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Original statistics\n",
    "    values_per_row = split_original.apply(len)\n",
    "    print(f\"Original values per row - Mean: {values_per_row.mean():.2f}, Std: {values_per_row.std():.2f}\")\n",
    "    \n",
    "    # New data statistics\n",
    "    if any('_count' in col for col in new_columns):\n",
    "        count_col = [col for col in new_columns if col.endswith('_count')][0]\n",
    "        new_counts = df_processed[count_col]\n",
    "        print(f\"Processed counts - Mean: {new_counts.mean():.2f}, Std: {new_counts.std():.2f}\")\n",
    "        \n",
    "        # They should match!\n",
    "        correlation = np.corrcoef(values_per_row, new_counts)[0, 1]\n",
    "        print(f\"Correlation between original and processed counts: {correlation:.4f}\")\n",
    "    \n",
    "    # Check for any impossible values\n",
    "    binary_cols = [col for col in new_columns if not col.endswith(('_count', '_frequency', '_avg_frequency'))]\n",
    "    for col in binary_cols:\n",
    "        unique_vals = df_processed[col].unique()\n",
    "        if not set(unique_vals).issubset({0, 1, np.nan}):\n",
    "            print(f\"âš ï¸  Warning: Non-binary values in '{col}': {unique_vals}\")\n",
    "\n",
    "\n",
    "def assess_information_loss(df_original, df_processed, original_column, new_columns, separator):\n",
    "    \"\"\"Assess how much information was lost in the transformation\"\"\"\n",
    "    \n",
    "    # Parse original data\n",
    "    original_data = df_original[original_column].fillna('').astype(str)\n",
    "    split_original = original_data.apply(lambda x: [v.strip() for v in x.split(separator) if v.strip()])\n",
    "    \n",
    "    # Get all unique values\n",
    "    all_original_values = set()\n",
    "    for values_list in split_original:\n",
    "        all_original_values.update(values_list)\n",
    "    all_original_values = sorted([v for v in all_original_values if v and v != 'nan'])\n",
    "    \n",
    "    # Count how many unique values are captured by new columns\n",
    "    captured_values = set()\n",
    "    for col in new_columns:\n",
    "        if not col.endswith(('_other', '_count', '_unique_count', '_frequency', '_avg_frequency', '_rare')):\n",
    "            # Extract value name from column name\n",
    "            value_parts = col.replace(f\"{original_column}_\", \"\").replace(\"top_\", \"\").replace(\"freq_\", \"\")\n",
    "            captured_values.add(value_parts)\n",
    "    \n",
    "    capture_rate = len(captured_values) / len(all_original_values) * 100 if all_original_values else 0\n",
    "    print(f\"Value capture rate: {len(captured_values)}/{len(all_original_values)} ({capture_rate:.1f}%)\")\n",
    "    \n",
    "    if len(all_original_values) - len(captured_values) > 0:\n",
    "        lost_values = set(all_original_values) - captured_values\n",
    "        print(f\"Lost values (first 10): {list(lost_values)[:10]}\")\n",
    "    \n",
    "    # Estimate row-level information preservation\n",
    "    if any('_other' in col for col in new_columns):\n",
    "        other_col = [col for col in new_columns if col.endswith('_other')][0]\n",
    "        rows_with_other = df_processed[other_col].sum()\n",
    "        print(f\"Rows with 'other' values: {rows_with_other}/{len(df_processed)} ({rows_with_other/len(df_processed)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "def quick_validation_summary(df_original, df_processed, column_mapping):\n",
    "    \"\"\"Quick validation summary for all processed columns\"\"\"\n",
    "    \n",
    "    print(\"=== QUICK VALIDATION SUMMARY ===\\n\")\n",
    "    \n",
    "    for original_col, new_cols in column_mapping.items():\n",
    "        print(f\"âœ“ {original_col} â†’ {len(new_cols)} new columns\")\n",
    "        \n",
    "        # Check for obvious issues\n",
    "        issues = []\n",
    "        \n",
    "        for col in new_cols:\n",
    "            if col not in df_processed.columns:\n",
    "                issues.append(f\"Missing column: {col}\")\n",
    "            else:\n",
    "                # Check for unexpected values in binary columns\n",
    "                if not col.endswith(('_count', '_frequency', '_avg_frequency')):\n",
    "                    unique_vals = set(df_processed[col].dropna().unique())\n",
    "                    if not unique_vals.issubset({0, 1, 0.0, 1.0}):\n",
    "                        issues.append(f\"Non-binary values in {col}: {unique_vals}\")\n",
    "        \n",
    "        if issues:\n",
    "            print(f\"  âš ï¸  Issues: {issues}\")\n",
    "        else:\n",
    "            print(f\"  âœ“ Looks good\")\n",
    "    \n",
    "    print(f\"\\nDataset size: {len(df_original)} â†’ {len(df_processed)} rows\")\n",
    "    print(f\"Column count: {len(df_original.columns)} â†’ {len(df_processed.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c7dfe5f-5eb6-4191-bcac-6e5370f5cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_missing_categories_from_full_dataset(\n",
    "    sample_df, \n",
    "    full_df, \n",
    "    categorical_columns, \n",
    "    samples_per_category=2,\n",
    "    exclude_columns=None  # Alternative parameter at this level\n",
    "):\n",
    "    \"\"\"\n",
    "    Add missing categorical values to sample dataset by sampling from full dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample_df : pd.DataFrame\n",
    "        Your limited sample dataset\n",
    "    full_df : pd.DataFrame  \n",
    "        Your complete dataset\n",
    "    categorical_columns : list\n",
    "        List of categorical column names\n",
    "    samples_per_category : int\n",
    "        Number of examples to add for each missing category\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Enhanced dataset with missing categories included\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Analyzing missing categories...\")\n",
    "\n",
    "    # Apply exclusions if provided at this level\n",
    "    if exclude_columns:\n",
    "        categorical_columns = [col for col in categorical_columns \n",
    "                              if col not in exclude_columns]\n",
    "        print(f\"Excluded columns: {exclude_columns}\")\n",
    "    \n",
    "    \n",
    "    # Find missing categories in sample compared to full dataset\n",
    "    missing_categories = {}\n",
    "    category_stats = {}\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col not in sample_df.columns or col not in full_df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in one of the datasets\")\n",
    "            continue\n",
    "            \n",
    "        full_categories = set(full_df[col].dropna().unique())\n",
    "        sample_categories = set(sample_df[col].dropna().unique())\n",
    "        missing = full_categories - sample_categories\n",
    "        \n",
    "        if missing:\n",
    "            missing_categories[col] = missing\n",
    "            category_stats[col] = {\n",
    "                'total_in_full': len(full_categories),\n",
    "                'in_sample': len(sample_categories),\n",
    "                'missing_count': len(missing)\n",
    "            }\n",
    "            print(f\"Column '{col}': Missing {len(missing)} out of {len(full_categories)} categories\")\n",
    "            print(f\"  Missing categories: {list(missing)[:5]}{'...' if len(missing) > 5 else ''}\")\n",
    "        else:\n",
    "            print(f\"Column '{col}': All categories present in sample\")\n",
    "    \n",
    "    if not missing_categories:\n",
    "        print(\"No missing categories found! Your sample already contains all category values.\")\n",
    "        return sample_df.copy()\n",
    "    \n",
    "    # Collect additional rows for missing categories\n",
    "    additional_rows = []\n",
    "    rows_added_by_category = defaultdict(int)\n",
    "    \n",
    "    for col, missing_vals in missing_categories.items():\n",
    "        print(f\"\\nSampling for column '{col}'...\")\n",
    "        \n",
    "        for val in missing_vals:\n",
    "            # Find all rows in full dataset with this category value\n",
    "            matching_rows = full_df[full_df[col] == val]\n",
    "            \n",
    "            if len(matching_rows) == 0:\n",
    "                print(f\"  Warning: No rows found for {col}='{val}' in full dataset\")\n",
    "                continue\n",
    "            \n",
    "            # Sample requested number of rows (or all available if fewer)\n",
    "            n_samples = min(samples_per_category, len(matching_rows))\n",
    "            sampled_rows = matching_rows.sample(n=n_samples, random_state=42)\n",
    "            \n",
    "            additional_rows.append(sampled_rows)\n",
    "            rows_added_by_category[f\"{col}='{val}'\"] = n_samples\n",
    "            print(f\"  Added {n_samples} rows for '{val}' (out of {len(matching_rows)} available)\")\n",
    "    \n",
    "    # Combine all additional rows\n",
    "    if additional_rows:\n",
    "        df_additional = pd.concat(additional_rows, ignore_index=True)\n",
    "        \n",
    "        # Remove potential duplicates (in case same row satisfies multiple missing categories)\n",
    "        initial_additional_count = len(df_additional)\n",
    "        df_additional = df_additional.drop_duplicates()\n",
    "        final_additional_count = len(df_additional)\n",
    "        \n",
    "        if initial_additional_count != final_additional_count:\n",
    "            print(f\"\\nRemoved {initial_additional_count - final_additional_count} duplicate rows\")\n",
    "        \n",
    "        # Combine with original sample\n",
    "        df_enhanced = pd.concat([sample_df, df_additional], ignore_index=True)\n",
    "        \n",
    "        print(f\"\\n=== SUMMARY ===\")\n",
    "        print(f\"Original sample size: {len(sample_df)}\")\n",
    "        print(f\"Additional rows added: {len(df_additional)}\")\n",
    "        print(f\"Final dataset size: {len(df_enhanced)}\")\n",
    "        print(f\"Size increase: {len(df_additional)/len(sample_df)*100:.1f}%\")\n",
    "        \n",
    "        return df_enhanced\n",
    "    \n",
    "    else:\n",
    "        print(\"No additional rows could be sampled\")\n",
    "        return sample_df.copy()\n",
    "\n",
    "\n",
    "def verify_categories_coverage(df_before, df_after, categorical_columns):\n",
    "    \"\"\"\n",
    "    Verify that the enhanced dataset now covers all categories\n",
    "    \"\"\"\n",
    "    print(\"\\n=== CATEGORY COVERAGE VERIFICATION ===\")\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col not in df_before.columns:\n",
    "            continue\n",
    "            \n",
    "        before_cats = set(df_before[col].dropna().unique())\n",
    "        after_cats = set(df_after[col].dropna().unique())\n",
    "        new_cats = after_cats - before_cats\n",
    "        \n",
    "        print(f\"\\nColumn '{col}':\")\n",
    "        print(f\"  Before: {len(before_cats)} categories\")\n",
    "        print(f\"  After:  {len(after_cats)} categories\")\n",
    "        if new_cats:\n",
    "            print(f\"  New categories added: {list(new_cats)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "502733c3-ec2e-41a2-ab19-1f449f514473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# === 1. DataLoader: Load data and check target column ===\n",
    "\n",
    "class DataLoader(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Load and perform initial data validation whether the target col exists:\n",
    "        - Handles both .xlsx and .csv.\n",
    "        - Stores the original shape of the data.\n",
    "        - Raises an error if the target column is missing.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, target_col='project_prf_normalised_work_effort'):\n",
    "        self.file_path = file_path\n",
    "        self.target_col = target_col  # This should be the standardized form\n",
    "        self.original_shape = None\n",
    "        self.original_target_col = None  # Store what we actually found\n",
    "        \n",
    "    def fit(self, X=None, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _standardize_column_name(self, col_name):\n",
    "        \"\"\"Convert column name to standardized format\"\"\"\n",
    "        return col_name.strip().lower().replace(' ', '_')\n",
    "    \n",
    "    def _find_target_column(self, df_columns):\n",
    "        \"\"\"\n",
    "        Smart target column finder - handles various formats\n",
    "        Returns the actual column name from the dataframe\n",
    "        \"\"\"\n",
    "        target_standardized = self.target_col.lower().replace(' ', '_')\n",
    "        \n",
    "        # Try exact match first\n",
    "        if self.target_col in df_columns:\n",
    "            return self.target_col\n",
    "            \n",
    "        # Try standardized versions of all columns\n",
    "        for col in df_columns:\n",
    "            col_standardized = self._standardize_column_name(col)\n",
    "            if col_standardized == target_standardized:\n",
    "                return col\n",
    "                \n",
    "        # If still not found, look for partial matches (for debugging)\n",
    "        similar_cols = []\n",
    "        target_words = set(target_standardized.split('_'))\n",
    "        for col in df_columns:\n",
    "            col_words = set(self._standardize_column_name(col).split('_'))\n",
    "            if len(target_words.intersection(col_words)) >= 2:  # At least 2 words match\n",
    "                similar_cols.append(col)\n",
    "                \n",
    "        return None, similar_cols\n",
    "    \n",
    "    def transform(self, X=None):\n",
    "        \"\"\"Load data from file with smart column handling\"\"\"\n",
    "\n",
    "        print(f\"Loading data from: {self.file_path}\")\n",
    "        \n",
    "        # Determine file type and load accordingly; support for Excel or CSV\n",
    "        if self.file_path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(self.file_path)\n",
    "        elif self.file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(self.file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Use .xlsx or .csv\")\n",
    "        \n",
    "        self.original_shape = df.shape\n",
    "        print(f\"Loaded data with shape: {df.shape}\")\n",
    "        \n",
    "        # Smart target column finding\n",
    "        result = self._find_target_column(df.columns)\n",
    "        \n",
    "        if isinstance(result, tuple):  # Not found, got similar columns\n",
    "            actual_col, similar_cols = result\n",
    "            error_msg = f\"Target column '{self.target_col}' not found in data.\"\n",
    "            if similar_cols:\n",
    "                error_msg += f\" Similar columns found: {similar_cols}\"\n",
    "            else:\n",
    "                error_msg += f\" Available columns: {list(df.columns)}\"\n",
    "            raise ValueError(error_msg)\n",
    "        else:\n",
    "            actual_col = result\n",
    "            \n",
    "        # Store the original column name we found\n",
    "        self.original_target_col = actual_col\n",
    "        \n",
    "        if actual_col != self.target_col:\n",
    "            print(f\"Target column found: '{actual_col}' -> will be standardized to '{self.target_col}'\")\n",
    "            \n",
    "        return df\n",
    "\n",
    "# === 2. ColumnNameStandardizer: Clean and standardize column names ===\n",
    "class ColumnNameStandardizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Standardize column names for consistency (lowercase, underscores, removes odd chars):\n",
    "        - Strips spaces, lowercases, replaces & with _&_, removes special chars.\n",
    "        - Useful for later steps and compatibility with modeling libraries.)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_col=None, original_target_col=None):\n",
    "        self.column_mapping = {}\n",
    "        self.target_col = target_col\n",
    "        self.original_target_col = original_target_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _standardize_columns(self, columns):\n",
    "        \"\"\"Standardize column names\"\"\"\n",
    "        return [col.strip().lower().replace(' ', '_') for col in columns]\n",
    "    \n",
    "    def _clean_column_names(self, columns):\n",
    "        \"\"\"Clean column names for compatibility\"\"\"\n",
    "        cleaned_cols = []\n",
    "        for col in columns:\n",
    "            # Replace ampersands with _&_ to match expected transformations\n",
    "            col_clean = col.replace(' & ', '_&_')\n",
    "            # Remove special characters except underscores and ampersands\n",
    "            col_clean = re.sub(r'[^\\w\\s&]', '', col_clean)\n",
    "            # Replace spaces with underscores\n",
    "            col_clean = col_clean.replace(' ', '_')\n",
    "            cleaned_cols.append(col_clean)\n",
    "        return cleaned_cols\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply column name standardization\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Store original column names\n",
    "        original_columns = df.columns.tolist()\n",
    "        \n",
    "        # Apply standardization\n",
    "        standardized_cols = self._standardize_columns(original_columns)\n",
    "        cleaned_cols = self._clean_column_names(standardized_cols)\n",
    "\n",
    "        # Special handling for target column\n",
    "        if self.original_target_col and self.target_col:\n",
    "            target_index = None\n",
    "            try:\n",
    "                target_index = original_columns.index(self.original_target_col)\n",
    "                cleaned_cols[target_index] = self.target_col\n",
    "                print(f\"Target column '{self.original_target_col}' -> '{self.target_col}'\")\n",
    "            except ValueError:\n",
    "                pass  # Original target col not found, proceed normally\n",
    "        \n",
    "        \n",
    "        # Create mapping\n",
    "        self.column_mapping = dict(zip(original_columns, cleaned_cols))\n",
    "        \n",
    "        # Apply new column names\n",
    "        df.columns = cleaned_cols\n",
    "        \n",
    "        # Report changes\n",
    "        changed_cols = sum(1 for orig, new in self.column_mapping.items() if orig != new)\n",
    "        print(f\"Standardized {changed_cols} column names\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 3. MissingValueAnalyzer: Analyze and handle missing values ===\n",
    "class MissingValueAnalyzer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Analyze and handle missing values\n",
    "        - Reports number of columns with >50% and >70% missing.\n",
    "        - Drops columns with a high proportion of missing data, except those you want to keep.\n",
    "        - Fills remaining missing values:\n",
    "            - Categorical: Fills with \"Missing\".\n",
    "            - Numeric: Fills with column median.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, high_missing_threshold=0.7, cols_to_keep=None):\n",
    "        self.high_missing_threshold = high_missing_threshold\n",
    "        self.cols_to_keep = cols_to_keep or []\n",
    "        self.high_missing_cols = []\n",
    "        self.missing_stats = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Analyze and handle missing values\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Calculate missing percentages\n",
    "        missing_pct = df.isnull().mean()\n",
    "        self.missing_stats = missing_pct.sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"\\nMissing value analysis:\")\n",
    "        print(f\"Columns with >50% missing: {sum(missing_pct > 0.5)}\")\n",
    "        print(f\"Columns with >70% missing: {sum(missing_pct > self.high_missing_threshold)}\")\n",
    "        \n",
    "        # Identify high missing columns\n",
    "        self.high_missing_cols = missing_pct[missing_pct > self.high_missing_threshold].index.tolist()\n",
    "        \n",
    "        # Filter out columns we want to keep\n",
    "        final_high_missing_cols = [col for col in self.high_missing_cols if col not in self.cols_to_keep]\n",
    "        \n",
    "        print(f\"Dropping {len(final_high_missing_cols)} columns with >{self.high_missing_threshold*100}% missing values\")\n",
    "        \n",
    "        # Drop high missing columns\n",
    "        df_clean = df.drop(columns=final_high_missing_cols)\n",
    "        \n",
    "        # Fill remaining missing values in categorical columns\n",
    "        cat_cols = df_clean.select_dtypes(include=['object', 'category']).columns\n",
    "        for col in cat_cols:\n",
    "            df_clean[col] = df_clean[col].fillna('Missing')\n",
    "        \n",
    "        # Fill remaining missing values in numerical columns with median\n",
    "        num_cols = df_clean.select_dtypes(include=['number']).columns\n",
    "        for col in num_cols:\n",
    "            if df_clean[col].isnull().sum() > 0:\n",
    "                median_val = df_clean[col].median()\n",
    "                df_clean[col] = df_clean[col].fillna(median_val)\n",
    "                print(f\"Filled {col} missing values with median: {median_val}\")\n",
    "        \n",
    "        print(f\"Data shape after missing value handling: {df_clean.shape}\")\n",
    "        return df_clean\n",
    "\n",
    "# === 4. SemicolonProcessor: Process multi-value columns (semicolon-separated) ===\n",
    "class SemicolonProcessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Process semicolon-separated values in columns (e.g., \"Python; Java; SQL\")\n",
    "        - Identifies columns with semicolons.\n",
    "        - Cleans: lowercases, strips, deduplicates, sorts, optionally standardizes values (e.g., \"stand alone\" â†’ \"stand-alone\").\n",
    "        - Useful for multi-value categorical features.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, standardization_mapping=None):\n",
    "        self.semicolon_cols = []\n",
    "        self.standardization_mapping = standardization_mapping or {\n",
    "            \"scrum\": \"agile development\",\n",
    "            \"file &/or print server\": \"file/print server\",\n",
    "        }\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _clean_and_sort_semicolon(self, val, apply_standardization=False, mapping=None):\n",
    "        \"\"\"Clean, deduplicate, sort, and standardize semicolon-separated values\"\"\"\n",
    "        if pd.isnull(val) or val == '':\n",
    "            return val\n",
    "        \n",
    "        parts = [x.strip().lower() for x in str(val).split(';') if x.strip()]\n",
    "        \n",
    "        if apply_standardization and mapping is not None:\n",
    "            parts = [mapping.get(part, part) for part in parts]\n",
    "        \n",
    "        unique_cleaned = sorted(set(parts))\n",
    "        return '; '.join(unique_cleaned)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Process semicolon-separated columns\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Identify columns with semicolons\n",
    "        self.semicolon_cols = [\n",
    "            col for col in df.columns\n",
    "            if df[col].dropna().astype(str).str.contains(';').any()\n",
    "        ]\n",
    "        \n",
    "        print(f\"Found {len(self.semicolon_cols)} columns with semicolons: {self.semicolon_cols}\")\n",
    "        \n",
    "        # Process each semicolon column\n",
    "        for col in self.semicolon_cols:\n",
    "            # Apply mapping for specific columns\n",
    "            apply_mapping = col in ['process_pmf_development_methodologies', 'tech_tf_server_roles']\n",
    "            mapping = self.standardization_mapping if apply_mapping else None\n",
    "            \n",
    "            # Clean the column\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: self._clean_and_sort_semicolon(x, apply_standardization=apply_mapping, mapping=mapping)\n",
    "            )\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 5. MultiValueEncoder: Encode semicolon columns using MultiLabelBinarizer ===\n",
    "class MultiValueEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Handle multi-value columns using MultiLabelBinarizer\n",
    "        - Only processes columns with a manageable number of unique values (max_cardinality).\n",
    "        - Each semicolon column becomes several binary columns (e.g., \"lang__python\", \"lang__java\", ...).     \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_cardinality=10):\n",
    "        # Ensure max_cardinality is always an integer\n",
    "        self.max_cardinality = int(max_cardinality) if max_cardinality is not None else 10\n",
    "        self.multi_value_cols = []\n",
    "        self.mlb_transformers = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Encode multi-value columns\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Identify semicolon columns (multi-value)\n",
    "        semicolon_cols = [\n",
    "            col for col in df.columns\n",
    "            if df[col].dropna().astype(str).str.contains(';').any()\n",
    "        ]\n",
    "        \n",
    "        # Filter for low cardinality multi-value columns\n",
    "        self.multi_value_cols = []\n",
    "        for col in semicolon_cols:\n",
    "            # Get unique values across all entries\n",
    "            all_values = set()\n",
    "            for val in df[col].dropna().astype(str):\n",
    "                values = [v.strip() for v in val.split(';') if v.strip()]\n",
    "                all_values.update(values)\n",
    "            \n",
    "            # Check cardinality (max_cardinality is already an integer from __init__)\n",
    "            if len(all_values) <= self.max_cardinality:\n",
    "                self.multi_value_cols.append(col)\n",
    "        \n",
    "        print(f\"Encoding {len(self.multi_value_cols)} multi-value columns: {self.multi_value_cols}\")\n",
    "        \n",
    "        # Process each multi-value column\n",
    "        for col in self.multi_value_cols:\n",
    "            # Prepare data for MultiLabelBinarizer\n",
    "            values = df[col].dropna().astype(str).apply(\n",
    "                lambda x: [item.strip() for item in x.split(';') if item.strip()]\n",
    "            )\n",
    "            \n",
    "            # Handle empty values - fill with empty list for MultiLabelBinarizer\n",
    "            if len(values) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Fit and transform\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            \n",
    "            # Convert to list of lists, handling NaN/empty cases\n",
    "            values_list = []\n",
    "            for idx in df.index:\n",
    "                if idx in values.index and values[idx]:\n",
    "                    values_list.append(values[idx])\n",
    "                else:\n",
    "                    values_list.append([])  # Empty list for missing values\n",
    "            \n",
    "            onehot = pd.DataFrame(\n",
    "                mlb.fit_transform(values_list),\n",
    "                columns=[f\"{col}__{cat}\" for cat in mlb.classes_],\n",
    "                index=df.index\n",
    "            )\n",
    "            \n",
    "            # Store transformer for later use\n",
    "            self.mlb_transformers[col] = mlb\n",
    "            \n",
    "            # Join with main dataframe\n",
    "            df = df.join(onehot, how='left')\n",
    "            \n",
    "            print(f\"Encoded {col} into {len(mlb.classes_)} binary columns\")\n",
    "        \n",
    "        # Remove original multi-value columns\n",
    "        df = df.drop(columns=self.multi_value_cols)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 6. CategoricalEncoder: One-hot encode regular categorical columns ===\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Handle single-value categorical columns\n",
    "        - Ignores semicolon columns.\n",
    "        - Only encodes columns with a number of categories â‰¤ max_cardinality (to avoid high-dimensional explosion).\n",
    "        - Can drop the first category for each variable to avoid multicollinearity.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_cardinality=10, drop_first=True):\n",
    "        self.max_cardinality = max_cardinality\n",
    "        self.drop_first = drop_first\n",
    "        self.categorical_cols = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Encode categorical columns\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Identify categorical columns\n",
    "        cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Identify semicolon columns to exclude\n",
    "        semicolon_cols = [\n",
    "            col for col in df.columns\n",
    "            if df[col].dropna().astype(str).str.contains(';').any()\n",
    "        ]\n",
    "        \n",
    "        # Filter for low cardinality single-value categorical columns\n",
    "        self.categorical_cols = [\n",
    "            col for col in cat_cols \n",
    "            if col not in semicolon_cols and df[col].nunique() <= self.max_cardinality\n",
    "        ]\n",
    "        \n",
    "        print(f\"One-hot encoding {len(self.categorical_cols)} categorical columns: {self.categorical_cols}\")\n",
    "        \n",
    "        # Apply one-hot encoding\n",
    "        if self.categorical_cols:\n",
    "            df = pd.get_dummies(df, columns=self.categorical_cols, drop_first=self.drop_first)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 7. ColumnNameFixer: Final column name cleanup for PyCaret etc ===\n",
    "class ColumnNameFixer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Fix column names for PyCaret compatibility (removes illegal characters, replaces spaces/ampersands, handles duplicates):\n",
    "        - No duplicate column names after encoding.\n",
    "        - Only alphanumeric and underscores. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.column_transformations = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Fix problematic column names\"\"\"\n",
    "        df = X.copy()\n",
    "        original_cols = df.columns.tolist()\n",
    "        fixed_columns = []\n",
    "        seen_columns = set()\n",
    "        \n",
    "        for col in original_cols:\n",
    "            # Replace spaces with underscores\n",
    "            fixed_col = col.replace(' ', '_')\n",
    "            # Replace ampersands\n",
    "            fixed_col = fixed_col.replace('&', 'and')\n",
    "            # Remove other problematic characters\n",
    "            fixed_col = ''.join(c if c.isalnum() or c == '_' else '_' for c in fixed_col)\n",
    "            # Remove multiple consecutive underscores\n",
    "            fixed_col = re.sub('_+', '_', fixed_col)\n",
    "            # Remove leading/trailing underscores\n",
    "            fixed_col = fixed_col.strip('_')\n",
    "            \n",
    "            # Handle duplicates\n",
    "            base_col = fixed_col\n",
    "            suffix = 1\n",
    "            while fixed_col in seen_columns:\n",
    "                fixed_col = f\"{base_col}_{suffix}\"\n",
    "                suffix += 1\n",
    "            \n",
    "            seen_columns.add(fixed_col)\n",
    "            fixed_columns.append(fixed_col)\n",
    "        \n",
    "        # Store transformations\n",
    "        self.column_transformations = dict(zip(original_cols, fixed_columns))\n",
    "        \n",
    "        # Apply new column names\n",
    "        df.columns = fixed_columns\n",
    "        \n",
    "        # Check for duplicates\n",
    "        dup_check = [item for item, count in pd.Series(fixed_columns).value_counts().items() if count > 1]\n",
    "        if dup_check:\n",
    "            print(f\"WARNING: Found {len(dup_check)} duplicate column names: {dup_check}\")\n",
    "        else:\n",
    "            print(\"No duplicate column names after fixing\")\n",
    "        \n",
    "        n_changed = sum(1 for old, new in self.column_transformations.items() if old != new)\n",
    "        print(f\"Fixed {n_changed} column names for PyCaret compatibility\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 8. DataValidator: Final summary and checks ===\n",
    "class DataValidator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        Validate final dataset\n",
    "        - Shape, missing values, infinities.\n",
    "        - Data types (numeric, categorical).\n",
    "        - Stats on the target column (mean, std, min, max, missing).\n",
    "        - Report issues if any.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_col):\n",
    "        self.target_col = target_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Validate the processed dataset\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        print(f\"\\n=== Final Data Validation ===\")\n",
    "        print(f\"Final shape: {df.shape}\")\n",
    "        print(f\"Target column: {self.target_col}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_count = df.isnull().sum().sum()\n",
    "        print(f\"Total missing values: {missing_count}\")\n",
    "        \n",
    "        # Check for infinite values\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        inf_count = np.isinf(df[numeric_cols].values).sum()\n",
    "        print(f\"Total infinite values: {inf_count}\")\n",
    "        \n",
    "        # Data types summary\n",
    "        print(f\"\\nData types:\")\n",
    "        print(f\"  Numeric columns: {len(df.select_dtypes(include=[np.number]).columns)}\")\n",
    "        print(f\"  Categorical columns: {len(df.select_dtypes(include=['object', 'category']).columns)}\")\n",
    "        \n",
    "        # Target variable summary\n",
    "        if self.target_col in df.columns:\n",
    "            target_stats = df[self.target_col].describe()\n",
    "            print(f\"\\nTarget variable '{self.target_col}' statistics:\")\n",
    "            print(f\"  Mean: {target_stats['mean']:.2f}\")\n",
    "            print(f\"  Std: {target_stats['std']:.2f}\")\n",
    "            print(f\"  Min: {target_stats['min']:.2f}\")\n",
    "            print(f\"  Max: {target_stats['max']:.2f}\")\n",
    "            print(f\"  Missing: {df[self.target_col].isnull().sum()}\")\n",
    "        else:\n",
    "            print(f\"WARNING: Target column '{self.target_col}' not found!\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === Pipeline creation function: returns the Scikit-learn pipeline ===\n",
    "def create_isbsg_preprocessing_pipeline(\n",
    "    target_col='project_prf_normalised_work_effort',\n",
    "    original_target_col=None,\n",
    "    high_missing_threshold=0.7,\n",
    "    cols_to_keep=None,\n",
    "    max_categorical_cardinality=10,\n",
    "    standardization_mapping=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create complete preprocessing pipeline with smart target column handling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    target_col : str\n",
    "        Name of target column\n",
    "    original_target_col : str\n",
    "        Original target column name found in data\n",
    "    high_missing_threshold : float\n",
    "        Threshold for dropping columns with high missing values\n",
    "    cols_to_keep : list\n",
    "        Columns to keep even if they have high missing values\n",
    "    max_categorical_cardinality : int\n",
    "        Maximum number of unique values for categorical encoding\n",
    "    standardization_mapping : dict\n",
    "        Custom mapping for standardizing semicolon-separated values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    sklearn.pipeline.Pipeline\n",
    "        Complete preprocessing pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    if cols_to_keep is None:\n",
    "        cols_to_keep = [\n",
    "            'project_prf_case_tool_used', \n",
    "            'process_pmf_prototyping_used',\n",
    "            'tech_tf_client_roles', \n",
    "            'tech_tf_type_of_server', \n",
    "            'tech_tf_clientserver_description'\n",
    "        ]\n",
    "    \n",
    "    # Ensure max_categorical_cardinality is an integer\n",
    "    if not isinstance(max_categorical_cardinality, int):\n",
    "        max_categorical_cardinality = 10\n",
    "        print(f\"Warning: max_categorical_cardinality was not an integer, defaulting to {max_categorical_cardinality}\")\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('column_standardizer', ColumnNameStandardizer(target_col, original_target_col)),\n",
    "        ('missing_handler', MissingValueAnalyzer(\n",
    "            high_missing_threshold=high_missing_threshold,\n",
    "            cols_to_keep=cols_to_keep\n",
    "        )),\n",
    "        ('semicolon_processor', SemicolonProcessor(standardization_mapping=standardization_mapping)),\n",
    "        ('multi_value_encoder', MultiValueEncoder(max_cardinality=max_categorical_cardinality)),\n",
    "        ('categorical_encoder', CategoricalEncoder(max_cardinality=max_categorical_cardinality)),\n",
    "        ('column_fixer', ColumnNameFixer()),\n",
    "        ('validator', DataValidator(target_col))\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# === Full workflow function: orchestrates loading, pipeline, and saving ===\n",
    "def preprocess_isbsg_data(\n",
    "    file_path,\n",
    "    target_col='project_prf_normalised_work_effort',  # Always use standardized form\n",
    "    output_dir='../data',\n",
    "    save_intermediate=True,\n",
    "    **pipeline_kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete preprocessing workflow for ISBSG data: loads the data, runs \n",
    "      the full preprocessing pipeline, saves processed data, pipeline \n",
    "      object, and a metadata report to disk, and returns the processed \n",
    "      DataFrame and metadata\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to input data file\n",
    "    target_col : str\n",
    "        Name of target column\n",
    "    output_dir : str\n",
    "        Directory to save processed data\n",
    "    save_intermediate : bool\n",
    "        Whether to save intermediate processing steps\n",
    "    **pipeline_kwargs : dict\n",
    "        Additional arguments for pipeline creation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Processed dataframe ready for modeling\n",
    "    dict\n",
    "        Processing metadata and statistics\n",
    "    \"\"\"\n",
    "\n",
    "    # print pipeline header\n",
    "    print(\"=\"*60)\n",
    "    print(\"ISBSG Data Preprocessing Pipeline\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    print(f\"Target column (standardized): {target_col}\")\n",
    "    print(f\"Timestamp: {datetime.now()}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load data with smart column detection\n",
    "    loader = DataLoader(file_path, target_col)\n",
    "    df_raw = loader.transform(X = None)\n",
    "    \n",
    "    # Create and fit preprocessing pipeline\n",
    "    pipeline = create_isbsg_preprocessing_pipeline(\n",
    "        target_col=target_col,\n",
    "        original_target_col=loader.original_target_col,  # Pass the found column name\n",
    "        **pipeline_kwargs\n",
    "    )\n",
    "    \n",
    "    # Apply preprocessing in order of ColumnNameStandardizer=> MissingValueAnalyzer =>\n",
    "    # SemicolonProcessor=> MultiValueEncoder=> CategoricalEncoder => ColumnNameFixer\n",
    "\n",
    "    # Apply preprocessing\n",
    "    df_processed = pipeline.fit_transform(df_raw)\n",
    "    \n",
    "    # Prepare metadata\n",
    "    metadata = {\n",
    "        'original_shape': loader.original_shape,\n",
    "        'processed_shape': df_processed.shape,\n",
    "        'processing_timestamp': datetime.now().isoformat(),\n",
    "        'target_column_standardized': target_col,\n",
    "        'target_column_original': loader.original_target_col,\n",
    "        'pipeline_steps': [step[0] for step in pipeline.steps]\n",
    "    }\n",
    "    \n",
    "    # Save processed data\n",
    "    file_stem = Path(file_path).stem\n",
    "    output_path = os.path.join(output_dir, f\"{file_stem}_preprocessed.csv\")\n",
    "    df_processed.to_csv(output_path, index=False)\n",
    "    print(f\"\\nProcessed data saved to: {output_path}\")\n",
    "    \n",
    "    # Save pipeline\n",
    "    pipeline_path = os.path.join(output_dir, f\"{file_stem}_preprocessing_pipeline.pkl\")\n",
    "    joblib.dump(pipeline, pipeline_path)\n",
    "    print(f\"Pipeline saved to: {pipeline_path}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = os.path.join(output_dir, f\"{file_stem}_preprocessing_metadata.txt\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        f.write(\"ISBSG Data Preprocessing Metadata\\n\")\n",
    "        f.write(\"=\"*40 + \"\\n\")\n",
    "        for key, value in metadata.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    print(f\"Metadata saved to: {metadata_path}\")\n",
    "\n",
    "    # Print completion & return results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Preprocessing completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return df_processed, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a67d417-a269-414e-9944-fc954d659ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def integrated_categorical_preprocessing(\n",
    "    sample_file_path: str,\n",
    "    full_file_path: str,\n",
    "    target_col: str,\n",
    "    output_dir: str,\n",
    "    cols_to_keep: List[str] = None,\n",
    "    high_card_columns: List[str] = None,\n",
    "    max_categorical_cardinality: int = 10,\n",
    "    samples_per_category: int = 3,\n",
    "    standardization_mapping: Dict[str, str] = None,\n",
    "    high_missing_threshold: float = 0.7,\n",
    "    separator: str = ';',\n",
    "    strategy: str = 'top_k',\n",
    "    k: int = 20,\n",
    "    exclude_from_enhancement: List[str] = None \n",
    ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Integrated pipeline to:\n",
    "    1. Load sample and full datasets\n",
    "    2. Apply consistent preprocessing to both datasets before comparison\n",
    "    2. Auto-detect categorical columns\n",
    "    3. Handle high-cardinality multi-value columns\n",
    "    4. Enhance sample with missing categories from full dataset\n",
    "    5. Apply standardization and final preprocessing\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    exclude_from_enhancement : List[str]\n",
    "        List of column names to exclude from getting additional categories from full dataset\n",
    "    \n",
    "    Returns:\n",
    "        - Enhanced and processed DataFrame\n",
    "        - Metadata about the processing steps\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"INTEGRATED CATEGORICAL PREPROCESSING PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Initialize exclude list if not provided\n",
    "    if exclude_from_enhancement is None:\n",
    "        exclude_from_enhancement = []\n",
    "    \n",
    "    # Step 1: Load datasets\n",
    "    print(\"\\n1. Loading datasets...\")\n",
    "    sample_df = pd.read_excel(sample_file_path)\n",
    "    full_df = pd.read_excel(full_file_path)\n",
    "\n",
    "    # Lowercase all column names in both DataFrames independently\n",
    "    sample_df.columns = [col1.lower() for col1 in sample_df.columns]\n",
    "    full_df.columns   = [col2.lower() for col2 in full_df.columns]\n",
    "\n",
    "    \n",
    "    print(f\"Sample dataset shape: {sample_df.shape}\")\n",
    "    print(f\"Full dataset shape: {full_df.shape}\")\n",
    "    \n",
    "    # Step 2: Create preprocessing pipeline (WITHOUT final validation)\n",
    "    print(\"\\n2. Creating preprocessing pipeline...\")\n",
    "    \n",
    "    # Create a pipeline that stops before final validation\n",
    "    initial_pipeline = Pipeline([\n",
    "        ('column_standardizer', ColumnNameStandardizer(target_col, target_col.lower())),\n",
    "        ('missing_handler', MissingValueAnalyzer(\n",
    "            high_missing_threshold=high_missing_threshold,\n",
    "            cols_to_keep=cols_to_keep or []\n",
    "        )),\n",
    "        ('semicolon_processor', SemicolonProcessor(standardization_mapping=standardization_mapping)),\n",
    "    ])\n",
    "    \n",
    "    # Step 3: Apply initial preprocessing to BOTH datasets\n",
    "    print(\"\\n3. Applying initial preprocessing to both datasets...\")\n",
    "    \n",
    "    # Process sample dataset\n",
    "    sample_df_preprocessed = initial_pipeline.fit_transform(sample_df)\n",
    "    print(f\"Sample after initial preprocessing: {sample_df_preprocessed.shape}\")\n",
    "    \n",
    "    # Process full dataset with same pipeline\n",
    "    full_df_preprocessed = initial_pipeline.transform(full_df)  # Use transform, not fit_transform\n",
    "    print(f\"Full dataset after initial preprocessing: {full_df_preprocessed.shape}\")\n",
    "    \n",
    "    # Step 4: Handle high-cardinality columns on PREPROCESSED datasets\n",
    "    print(\"\\n4. Processing high-cardinality multi-value columns...\")\n",
    "    if high_card_columns is None:\n",
    "        high_card_columns = ['external_eef_organisation_type', 'project_prf_application_type']\n",
    "    \n",
    "    col_mapping = {}\n",
    "    \n",
    "    # Process high-cardinality columns in both datasets\n",
    "    for col in high_card_columns:\n",
    "        if col in full_df_preprocessed.columns:\n",
    "            print(f\"\\nProcessing high-cardinality column: {col}\")\n",
    "            \n",
    "            # Process full dataset\n",
    "            full_df_preprocessed, temp_mapping = handle_high_cardinality_multivalue(\n",
    "                full_df_preprocessed,\n",
    "                multi_value_columns=[col],\n",
    "                separator=separator,\n",
    "                strategy=strategy,\n",
    "                k=k\n",
    "            )\n",
    "            \n",
    "            # Process sample dataset with same strategy\n",
    "            sample_df_preprocessed, _ = handle_high_cardinality_multivalue(\n",
    "                sample_df_preprocessed,\n",
    "                multi_value_columns=[col],\n",
    "                separator=separator,\n",
    "                strategy=strategy,\n",
    "                k=k\n",
    "            )\n",
    "            \n",
    "            col_mapping.update(temp_mapping)\n",
    "    \n",
    "    # Step 5: NOW identify categorical columns from preprocessed datasets\n",
    "    print(\"\\n5. Identifying categorical columns from preprocessed datasets...\")\n",
    "    categorical_columns = []\n",
    "    for col in sample_df_preprocessed.columns:\n",
    "        if (sample_df_preprocessed[col].dtype == 'object' or \n",
    "            sample_df_preprocessed[col].nunique() < max_categorical_cardinality):\n",
    "            categorical_columns.append(col)\n",
    "\n",
    "    categorical_columns = [col.lower() for col in categorical_columns]\n",
    "    print(f\"Detected categorical columns: {len(categorical_columns)} columns\")\n",
    "    \n",
    "    # Step 6: Enhanced category sampling with exclusions\n",
    "    print(\"\\n6. Enhancing sample with missing categories from preprocessed full dataset...\")\n",
    "    print(f\"Excluding columns from enhancement: {exclude_from_enhancement}\")\n",
    "    \n",
    "    # Filter out excluded columns before enhancement\n",
    "    columns_to_enhance = [col for col in categorical_columns \n",
    "                         if col not in exclude_from_enhancement]\n",
    "    \n",
    "    print(f\"Columns that will be enhanced: {len(columns_to_enhance)} out of {len(categorical_columns)}; Cols are : {columns_to_enhance}\")\n",
    "    \n",
    "    enhanced_df = add_missing_categories_from_full_dataset(\n",
    "        sample_df=sample_df_preprocessed,\n",
    "        full_df=full_df_preprocessed,  # Use preprocessed full dataset\n",
    "        categorical_columns=columns_to_enhance,\n",
    "        samples_per_category=samples_per_category\n",
    "    )\n",
    "    \n",
    "    print(f\"Enhanced dataset shape: {enhanced_df.shape}\")\n",
    "    \n",
    "    # Step 7: Verify categories coverage\n",
    "    print(\"\\n7. Verifying categories coverage...\")\n",
    "    verify_categories_coverage(sample_df_preprocessed, enhanced_df, categorical_columns)\n",
    "    \n",
    "    # Step 8: Apply final preprocessing stages\n",
    "    print(\"\\n8. Applying final preprocessing stages...\")\n",
    "    \n",
    "    # Create final pipeline for remaining steps\n",
    "    final_pipeline = Pipeline([\n",
    "        ('multi_value_encoder', MultiValueEncoder(max_cardinality=max_categorical_cardinality)),\n",
    "        ('categorical_encoder', CategoricalEncoder(max_cardinality=max_categorical_cardinality)),\n",
    "        ('column_fixer', ColumnNameFixer()),\n",
    "        ('validator', DataValidator(target_col))\n",
    "    ])\n",
    "    \n",
    "    final_df = final_pipeline.fit_transform(enhanced_df)\n",
    "    \n",
    "    # Step 9: Final validation and duplicate check\n",
    "    print(\"\\n9. Final validation and duplicate check...\")\n",
    "    \n",
    "    # Check for any remaining duplicates after all processing\n",
    "    final_duplicate_cols = final_df.columns[final_df.columns.duplicated()].tolist()\n",
    "    if final_duplicate_cols:\n",
    "        print(f\"Warning: Found duplicate columns in final dataset: {final_duplicate_cols}\")\n",
    "        final_df = final_df.loc[:, ~final_df.columns.duplicated()]\n",
    "        print(\"Removed final duplicate columns\")\n",
    "    \n",
    "    print(f\"Original sample shape: {sample_df.shape}\")\n",
    "    print(f\"Final processed shape: {final_df.shape}\")\n",
    "    print(f\"Columns added: {final_df.shape[1] - sample_df.shape[1]}\")\n",
    "    print(f\"Rows added: {final_df.shape[0] - sample_df.shape[0]}\")\n",
    "    \n",
    "    # Compile metadata\n",
    "    metadata = {\n",
    "        'original_sample_shape': sample_df.shape,\n",
    "        'original_full_shape': full_df.shape,\n",
    "        'final_shape': final_df.shape,\n",
    "        'categorical_columns_detected': categorical_columns,\n",
    "        'high_cardinality_columns_processed': high_card_columns,\n",
    "        'column_mapping': col_mapping,\n",
    "        'rows_added_from_full_dataset': final_df.shape[0] - sample_df.shape[0]\n",
    "    }\n",
    "    \n",
    "    return final_df, metadata\n",
    "\n",
    "def safe_preprocess_with_fallback(\n",
    "    enhanced_df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    output_dir: str,\n",
    "    cols_to_keep: List[str] = None,\n",
    "    max_categorical_cardinality: int = 10,\n",
    "    standardization_mapping: Dict[str, str] = None,\n",
    "    high_missing_threshold: float = 0.7\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Safe preprocessing function that handles the file_path requirement\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save enhanced dataset to temporary file\n",
    "    temp_enhanced_path = os.path.join(output_dir, 'temp_enhanced_sample.xlsx')\n",
    "    enhanced_df.to_excel(temp_enhanced_path, index=False)\n",
    "    \n",
    "    try:\n",
    "        # Apply preprocessing using existing function\n",
    "        final_df, preprocessing_metadata = preprocess_isbsg_data(\n",
    "            file_path=temp_enhanced_path,\n",
    "            target_col=target_col,\n",
    "            output_dir=output_dir,\n",
    "            cols_to_keep=cols_to_keep,\n",
    "            max_categorical_cardinality=max_categorical_cardinality,\n",
    "            standardization_mapping=standardization_mapping,\n",
    "            high_missing_threshold=high_missing_threshold\n",
    "        )\n",
    "        \n",
    "        return final_df, preprocessing_metadata\n",
    "        \n",
    "    finally:\n",
    "        # Clean up temporary file\n",
    "        try:\n",
    "            os.remove(temp_enhanced_path)\n",
    "        except:\n",
    "            print(f\"Warning: Could not remove temporary file {temp_enhanced_path}\")\n",
    "    \n",
    "    return enhanced_df, {'error': 'Preprocessing failed'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54df81cd-5503-4931-a2b8-2666049d5f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATA_FOLDER = ../data, SAMPLE_FILE = ISBSG2016R1_1_financial_industry_seed.xlsx, FULL_FILE = ISBSG2016R1_1_full_dataset.xlsx, TARGET_COL = project_prf_normalised_work_effort\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Configuration constants (define these at module level)\n",
    "#DATA_FOLDER = \"../data\"  # Update this path as needed\n",
    "#SAMPLE_FILE = \"sample_data.xlsx\"  # Update this filename as needed\n",
    "#FULL_FILE = \"full_data.xlsx\"  # Update this filename as needed\n",
    "#TARGET_COL = \"project_prf_normalised_work_effort\"\n",
    "\n",
    "print(f\"\\nDATA_FOLDER = {DATA_FOLDER}, SAMPLE_FILE = {SAMPLE_FILE}, FULL_FILE = {FULL_FILE}, TARGET_COL = {TARGET_COL}\")\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the integrated pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    sample_file_path = os.path.join(CONFIG_FOLDER, SAMPLE_FILE)\n",
    "    full_file_path = os.path.join(DATA_FOLDER, FULL_FILE)\n",
    "    FINANCE = \"finance\"\n",
    "    \n",
    "\n",
    "        # Columns to exclude (customize as needed)\n",
    "    cols_to_exclude_add_category = [\n",
    "        'external_eef_industry_sector', \n",
    "        'external_eef_organisation_type',\n",
    "        'project_prf_application_type', \n",
    "     ]\n",
    "    \n",
    "    # Columns to keep (customize as needed)\n",
    "    cols_to_keep = [\n",
    "        'Project_PRF_CASE_Tool_Used', \n",
    "        'Process_PMF_Prototyping_Used',\n",
    "        'Tech_TF_Client_Roles', \n",
    "        'Tech_TF_Type_of_Server', \n",
    "        'Tech_TF_ClientServer_Description'\n",
    "    ]\n",
    "    \n",
    "    # High-cardinality multi-value columns\n",
    "    high_card_columns = [\n",
    "        'external_eef_organisation_type', \n",
    "        'project_prf_application_type'\n",
    "    ]\n",
    "    \n",
    "    # Standardization rules\n",
    "    standardization_map = {\n",
    "        'stand alone': 'stand-alone',\n",
    "        'client server': 'client-server',\n",
    "        'mathematically intensive': 'mathematically-intensive',\n",
    "        #'mathematically intensive application': 'mathematically-intensive application',\n",
    "        \"file &/or print server\": \"file/print server\",\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Run integrated pipeline\n",
    "        final_df, metadata = integrated_categorical_preprocessing(\n",
    "            sample_file_path=sample_file_path,\n",
    "            full_file_path=full_file_path,\n",
    "            target_col=TARGET_COL,\n",
    "            output_dir=DATA_FOLDER,\n",
    "            cols_to_keep=cols_to_keep,\n",
    "            high_card_columns=high_card_columns,\n",
    "            max_categorical_cardinality=10,\n",
    "            samples_per_category=3,\n",
    "            standardization_mapping=standardization_map,\n",
    "            high_missing_threshold=0.7,\n",
    "            separator=';',\n",
    "            strategy='top_k',\n",
    "            k=20,\n",
    "            exclude_from_enhancement=cols_to_exclude_add_category\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        if FINANCE in sample_file_path:\n",
    "            output_path = os.path.join(DATA_FOLDER, f\"{FINANCE}_enhanced_sample_final.csv\")\n",
    "        else:\n",
    "            output_path = os.path.join(DATA_FOLDER, 'enhanced_sample_final.csv')\n",
    "            \n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Final dataset saved to: {output_path}\")\n",
    "        print(f\"Final shape: {final_df.shape}\")\n",
    "        print(f\"Ready for PyCaret setup!\")\n",
    "        \n",
    "        # Print summary of changes\n",
    "        print(f\"\\nSUMMARY:\")\n",
    "        print(f\"- Original sample rows: {metadata['original_sample_shape'][0]}\")\n",
    "        print(f\"- Rows added from full dataset: {metadata['rows_added_from_full_dataset']}\")\n",
    "        print(f\"- Final rows: {metadata['final_shape'][0]}\")\n",
    "        print(f\"- Original columns: {metadata['original_sample_shape'][1]}\")\n",
    "        print(f\"- Final columns: {metadata['final_shape'][1]}\")\n",
    "        \n",
    "        return final_df, metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in integrated pipeline: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bf03c5-31b5-4093-916e-9ca5efc4702b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fa1ff42-b05a-4697-b88a-390c4fac38f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INTEGRATED CATEGORICAL PREPROCESSING PIPELINE\n",
      "============================================================\n",
      "\n",
      "1. Loading datasets...\n",
      "Sample dataset shape: (939, 51)\n",
      "Full dataset shape: (7518, 52)\n",
      "\n",
      "2. Creating preprocessing pipeline...\n",
      "\n",
      "3. Applying initial preprocessing to both datasets...\n",
      "Standardized 26 column names\n",
      "\n",
      "Missing value analysis:\n",
      "Columns with >50% missing: 29\n",
      "Columns with >70% missing: 26\n",
      "Dropping 26 columns with >70.0% missing values\n",
      "Filled project_prf_functional_size missing values with median: 154.5\n",
      "Filled project_prf_normalised_work_effort_level_1 missing values with median: 1550.0\n",
      "Filled project_prf_normalised_work_effort missing values with median: 1652.0\n",
      "Filled project_prf_normalised_level_1_pdr_ufp missing values with median: 11.45\n",
      "Filled project_prf_normalised_pdr_ufp missing values with median: 11.9\n",
      "Filled project_prf_speed_of_delivery missing values with median: 27.05\n",
      "Filled project_prf_project_elapsed_time missing values with median: 6.0\n",
      "Data shape after missing value handling: (939, 25)\n",
      "Found 2 columns with semicolons: ['external_eef_organisation_type', 'project_prf_application_type']\n",
      "Sample after initial preprocessing: (939, 25)\n",
      "Standardized 27 column names\n",
      "\n",
      "Missing value analysis:\n",
      "Columns with >50% missing: 30\n",
      "Columns with >70% missing: 24\n",
      "Dropping 24 columns with >70.0% missing values\n",
      "Filled project_prf_functional_size missing values with median: 139.0\n",
      "Filled project_prf_normalised_work_effort_level_1 missing values with median: 1593.0\n",
      "Filled project_prf_normalised_work_effort missing values with median: 1699.0\n",
      "Filled project_prf_normalised_level_1_pdr_ufp missing values with median: 11.2\n",
      "Filled project_prf_normalised_pdr_ufp missing values with median: 11.6\n",
      "Filled project_prf_speed_of_delivery missing values with median: 26.8\n",
      "Filled project_prf_project_elapsed_time missing values with median: 6.0\n",
      "Filled project_prf_max_team_size missing values with median: 7.0\n",
      "Data shape after missing value handling: (7518, 28)\n",
      "Found 4 columns with semicolons: ['external_eef_organisation_type', 'project_prf_application_group', 'project_prf_application_type', 'process_pmf_development_methodologies']\n",
      "Full dataset after initial preprocessing: (7518, 28)\n",
      "\n",
      "4. Processing high-cardinality multi-value columns...\n",
      "\n",
      "Processing high-cardinality column: external_eef_organisation_type\n",
      "\n",
      "Processing high-cardinality column 'external_eef_organisation_type' with strategy 'top_k'...\n",
      "  Created 21 columns (top 20 + other)\n",
      "\n",
      "Processing high-cardinality column 'external_eef_organisation_type' with strategy 'top_k'...\n",
      "  Created 20 columns (top 20 + other)\n",
      "\n",
      "Processing high-cardinality column: project_prf_application_type\n",
      "\n",
      "Processing high-cardinality column 'project_prf_application_type' with strategy 'top_k'...\n",
      "  Created 21 columns (top 20 + other)\n",
      "\n",
      "Processing high-cardinality column 'project_prf_application_type' with strategy 'top_k'...\n",
      "  Created 21 columns (top 20 + other)\n",
      "\n",
      "5. Identifying categorical columns from preprocessed datasets...\n",
      "Detected categorical columns: 53 columns\n",
      "\n",
      "6. Enhancing sample with missing categories from preprocessed full dataset...\n",
      "Excluding columns from enhancement: ['external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_type']\n",
      "Columns that will be enhanced: 52 out of 53; Cols are : ['external_eef_data_quality_rating', 'project_prf_application_group', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'tech_tf_primary_programming_language', 'project_prf_relative_size', 'project_prf_case_tool_used', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_dbms_used', 'external_eef_organisation_type_top_banking', 'external_eef_organisation_type_top_financial,_property_&_business_services', 'external_eef_organisation_type_top_government', 'external_eef_organisation_type_top_communications', 'external_eef_organisation_type_top_education_institution', 'external_eef_organisation_type_top_medical_and_health_care', 'external_eef_organisation_type_top_transport_&_storage', 'external_eef_organisation_type_top_wholesale_&_retail_trade', 'external_eef_organisation_type_top_revenue', 'external_eef_organisation_type_top_financial_(banking,_insurance,_stock)', 'external_eef_organisation_type_top_missing', 'external_eef_organisation_type_top_credit_card_processor', 'external_eef_organisation_type_top_amusement/game_center', 'external_eef_organisation_type_top_insurance', 'external_eef_organisation_type_top_revenue_collection', 'external_eef_organisation_type_top_agriculture,_forestry,_fishing,_hunting', 'external_eef_organisation_type_top_(banking,_insurance,_stock)', 'external_eef_organisation_type_top_finance', 'external_eef_organisation_type_top_public_administration', 'external_eef_organisation_type_other', 'project_prf_application_type_top_financial_transaction_process/accounting', 'project_prf_application_type_top_client_server', 'project_prf_application_type_top_missing', 'project_prf_application_type_top_transaction/production_system', 'project_prf_application_type_top_management_information_system', 'project_prf_application_type_top_data_warehouse', 'project_prf_application_type_top_surveillance_and_security', 'project_prf_application_type_top_office_information_system', 'project_prf_application_type_top_business_application', 'project_prf_application_type_top_web_based_application', 'project_prf_application_type_top_sales_contact_management', 'project_prf_application_type_top_electronic_data_interchange', 'project_prf_application_type_top_workflow_support_&_management', 'project_prf_application_type_top_automated_customer_statements', 'project_prf_application_type_top_customer_billing/relationship_management', 'project_prf_application_type_top_mobile_application', 'project_prf_application_type_top_online_analysis_and_reporting', 'project_prf_application_type_top_office_automation', 'project_prf_application_type_top_executive_information_system', 'project_prf_application_type_top_personnel_system', 'project_prf_application_type_other']\n",
      "Analyzing missing categories...\n",
      "Column 'external_eef_data_quality_rating': All categories present in sample\n",
      "Column 'project_prf_application_group': Missing 7 out of 7 categories\n",
      "  Missing categories: ['mathematically intensive application', 'real-time application', 'infrastructure software', 'mathematically-intensive application', 'missing']...\n",
      "Column 'project_prf_development_type': Missing 2 out of 7 categories\n",
      "  Missing categories: ['Porting', 'Other']\n",
      "Column 'tech_tf_development_platform': Missing 1 out of 7 categories\n",
      "  Missing categories: ['Hand Held']\n",
      "Column 'tech_tf_language_type': Missing 1 out of 7 categories\n",
      "  Missing categories: ['APG']\n",
      "Column 'tech_tf_primary_programming_language': Missing 80 out of 129 categories\n",
      "  Missing categories: ['Object oriented language', 'AB INITIO', 'FORTRAN', 'BRE', 'Doc1 Designer (Entorno visual)']...\n",
      "Column 'project_prf_relative_size': Missing 1 out of 10 categories\n",
      "  Missing categories: ['XXXL']\n",
      "Column 'project_prf_case_tool_used': All categories present in sample\n",
      "Column 'tech_tf_architecture': Missing 2 out of 8 categories\n",
      "  Missing categories: ['Stand-alone', 'Multi-tier with web interface']\n",
      "Column 'tech_tf_client_server': All categories present in sample\n",
      "Column 'tech_tf_dbms_used': All categories present in sample\n",
      "Column 'external_eef_organisation_type_top_banking': All categories present in sample\n",
      "Column 'external_eef_organisation_type_top_financial,_property_&_business_services': All categories present in sample\n",
      "Column 'external_eef_organisation_type_top_government': All categories present in sample\n",
      "Column 'external_eef_organisation_type_top_communications': All categories present in sample\n",
      "Column 'external_eef_organisation_type_top_education_institution': All categories present in sample\n",
      "Column 'external_eef_organisation_type_top_medical_and_health_care': All categories present in sample\n",
      "Column 'external_eef_organisation_type_top_transport_&_storage': All categories present in sample\n",
      "Column 'external_eef_organisation_type_top_wholesale_&_retail_trade': All categories present in sample\n",
      "Warning: Column 'external_eef_organisation_type_top_revenue' not found in one of the datasets\n",
      "Warning: Column 'external_eef_organisation_type_top_financial_(banking,_insurance,_stock)' not found in one of the datasets\n",
      "Column 'external_eef_organisation_type_top_missing': All categories present in sample\n",
      "Warning: Column 'external_eef_organisation_type_top_credit_card_processor' not found in one of the datasets\n",
      "Warning: Column 'external_eef_organisation_type_top_amusement/game_center' not found in one of the datasets\n",
      "Column 'external_eef_organisation_type_top_insurance': All categories present in sample\n",
      "Warning: Column 'external_eef_organisation_type_top_revenue_collection' not found in one of the datasets\n",
      "Warning: Column 'external_eef_organisation_type_top_agriculture,_forestry,_fishing,_hunting' not found in one of the datasets\n",
      "Warning: Column 'external_eef_organisation_type_top_(banking,_insurance,_stock)' not found in one of the datasets\n",
      "Warning: Column 'external_eef_organisation_type_top_finance' not found in one of the datasets\n",
      "Column 'external_eef_organisation_type_top_public_administration': All categories present in sample\n",
      "Column 'external_eef_organisation_type_other': Missing 1 out of 2 categories\n",
      "  Missing categories: [1]\n",
      "Column 'project_prf_application_type_top_financial_transaction_process/accounting': All categories present in sample\n",
      "Column 'project_prf_application_type_top_client_server': All categories present in sample\n",
      "Column 'project_prf_application_type_top_missing': All categories present in sample\n",
      "Column 'project_prf_application_type_top_transaction/production_system': All categories present in sample\n",
      "Column 'project_prf_application_type_top_management_information_system': All categories present in sample\n",
      "Warning: Column 'project_prf_application_type_top_data_warehouse' not found in one of the datasets\n",
      "Warning: Column 'project_prf_application_type_top_surveillance_and_security' not found in one of the datasets\n",
      "Warning: Column 'project_prf_application_type_top_office_information_system' not found in one of the datasets\n",
      "Column 'project_prf_application_type_top_business_application': All categories present in sample\n",
      "Warning: Column 'project_prf_application_type_top_web_based_application' not found in one of the datasets\n",
      "Warning: Column 'project_prf_application_type_top_sales_contact_management' not found in one of the datasets\n",
      "Column 'project_prf_application_type_top_electronic_data_interchange': All categories present in sample\n",
      "Column 'project_prf_application_type_top_workflow_support_&_management': All categories present in sample\n",
      "Warning: Column 'project_prf_application_type_top_automated_customer_statements' not found in one of the datasets\n",
      "Column 'project_prf_application_type_top_customer_billing/relationship_management': All categories present in sample\n",
      "Warning: Column 'project_prf_application_type_top_mobile_application' not found in one of the datasets\n",
      "Column 'project_prf_application_type_top_online_analysis_and_reporting': All categories present in sample\n",
      "Warning: Column 'project_prf_application_type_top_office_automation' not found in one of the datasets\n",
      "Warning: Column 'project_prf_application_type_top_executive_information_system' not found in one of the datasets\n",
      "Warning: Column 'project_prf_application_type_top_personnel_system' not found in one of the datasets\n",
      "Column 'project_prf_application_type_other': All categories present in sample\n",
      "\n",
      "Sampling for column 'project_prf_application_group'...\n",
      "  Added 3 rows for 'mathematically intensive application' (out of 6 available)\n",
      "  Added 3 rows for 'real-time application' (out of 274 available)\n",
      "  Added 3 rows for 'infrastructure software' (out of 51 available)\n",
      "  Added 3 rows for 'mathematically-intensive application' (out of 219 available)\n",
      "  Added 3 rows for 'missing' (out of 2311 available)\n",
      "  Added 3 rows for 'business application' (out of 4655 available)\n",
      "  Added 2 rows for 'business application; infrastructure software' (out of 2 available)\n",
      "\n",
      "Sampling for column 'project_prf_development_type'...\n",
      "  Added 1 rows for 'Porting' (out of 1 available)\n",
      "  Added 3 rows for 'Other' (out of 4 available)\n",
      "\n",
      "Sampling for column 'tech_tf_development_platform'...\n",
      "  Added 1 rows for 'Hand Held' (out of 1 available)\n",
      "\n",
      "Sampling for column 'tech_tf_language_type'...\n",
      "  Added 1 rows for 'APG' (out of 1 available)\n",
      "\n",
      "Sampling for column 'tech_tf_primary_programming_language'...\n",
      "  Added 3 rows for 'Object oriented language' (out of 3 available)\n",
      "  Added 1 rows for 'AB INITIO' (out of 1 available)\n",
      "  Added 3 rows for 'FORTRAN' (out of 6 available)\n",
      "  Added 1 rows for 'BRE' (out of 1 available)\n",
      "  Added 1 rows for 'Doc1 Designer (Entorno visual)' (out of 1 available)\n",
      "  Added 1 rows for 'Enablon' (out of 1 available)\n",
      "  Added 1 rows for 'COGNOS' (out of 1 available)\n",
      "  Added 3 rows for 'PASCAL' (out of 8 available)\n",
      "  Added 3 rows for 'Ada' (out of 3 available)\n",
      "  Added 2 rows for 'iPlanet Netscape Application Server' (out of 2 available)\n",
      "  Added 3 rows for 'Centura' (out of 3 available)\n",
      "  Added 2 rows for 'PYTHON' (out of 2 available)\n",
      "  Added 3 rows for 'RPL' (out of 5 available)\n",
      "  Added 3 rows for 'Adobe Flex' (out of 8 available)\n",
      "  Added 1 rows for 'J2EE' (out of 1 available)\n",
      "  Added 3 rows for 'INGRES' (out of 6 available)\n",
      "  Added 1 rows for 'Express' (out of 1 available)\n",
      "  Added 3 rows for 'IDEAL' (out of 6 available)\n",
      "  Added 3 rows for 'BPM' (out of 7 available)\n",
      "  Added 2 rows for 'ABF' (out of 2 available)\n",
      "  Added 2 rows for 'Data base language' (out of 2 available)\n",
      "  Added 1 rows for 'IEF' (out of 1 available)\n",
      "  Added 3 rows for 'PERIPHONICS' (out of 6 available)\n",
      "  Added 3 rows for 'SAS' (out of 6 available)\n",
      "  Added 3 rows for 'COOL:Gen' (out of 109 available)\n",
      "  Added 1 rows for 'MANTIS' (out of 1 available)\n",
      "  Added 1 rows for 'A:G' (out of 1 available)\n",
      "  Added 1 rows for 'Magic' (out of 1 available)\n",
      "  Added 1 rows for 'Jdeveloper' (out of 1 available)\n",
      "  Added 1 rows for 'ADO.Net' (out of 1 available)\n",
      "  Added 3 rows for 'Datastage' (out of 9 available)\n",
      "  Added 3 rows for 'APPS' (out of 4 available)\n",
      "  Added 1 rows for 'BO' (out of 1 available)\n",
      "  Added 1 rows for 'PowerPlay' (out of 1 available)\n",
      "  Added 1 rows for 'EJB' (out of 1 available)\n",
      "  Added 3 rows for 'VisualFoxPro' (out of 3 available)\n",
      "  Added 3 rows for 'ColdFusion' (out of 8 available)\n",
      "  Added 3 rows for 'OutlookVBA' (out of 4 available)\n",
      "  Added 3 rows for 'C/AL' (out of 3 available)\n",
      "  Added 1 rows for 'TNSDL' (out of 1 available)\n",
      "  Added 1 rows for 'Huron/Object Star' (out of 1 available)\n",
      "  Added 2 rows for 'DRIFT' (out of 2 available)\n",
      "  Added 1 rows for 'Informatica PowerCenter' (out of 1 available)\n",
      "  Added 3 rows for 'Siebel' (out of 37 available)\n",
      "  Added 1 rows for 'LEX' (out of 1 available)\n",
      "  Added 3 rows for 'MATLAB' (out of 8 available)\n",
      "  Added 1 rows for 'NCR teradata scripting' (out of 1 available)\n",
      "  Added 1 rows for 'Delphi' (out of 1 available)\n",
      "  Added 3 rows for 'Upfront' (out of 5 available)\n",
      "  Added 3 rows for 'Visual Studio .Net' (out of 3 available)\n",
      "  Added 1 rows for 'Azure' (out of 1 available)\n",
      "  Added 3 rows for 'CICS' (out of 4 available)\n",
      "  Added 1 rows for 'ACCEL' (out of 1 available)\n",
      "  Added 3 rows for 'IIS' (out of 6 available)\n",
      "  Added 1 rows for 'LISP' (out of 1 available)\n",
      "  Added 1 rows for 'SLEL' (out of 1 available)\n",
      "  Added 3 rows for 'HPS' (out of 14 available)\n",
      "  Added 1 rows for 'BEA Weblogic' (out of 1 available)\n",
      "  Added 3 rows for 'ASAP' (out of 4 available)\n",
      "  Added 3 rows for 'PHP' (out of 36 available)\n",
      "  Added 1 rows for 'Spreadsheet' (out of 1 available)\n",
      "  Added 3 rows for 'Perl' (out of 3 available)\n",
      "  Added 3 rows for 'UNIFACE' (out of 5 available)\n",
      "  Added 1 rows for 'ARBOR/BP' (out of 1 available)\n",
      "  Added 3 rows for 'SLOGAN' (out of 7 available)\n",
      "  Added 1 rows for 'MS-Navision Properitory Language' (out of 1 available)\n",
      "  Added 2 rows for 'Periproducer' (out of 2 available)\n",
      "  Added 1 rows for 'Caa' (out of 1 available)\n",
      "  Added 3 rows for 'BASIC' (out of 4 available)\n",
      "  Added 1 rows for 'XGML' (out of 1 available)\n",
      "  Added 1 rows for 'IBM WTX' (out of 1 available)\n",
      "  Added 1 rows for 'Must Modeller' (out of 1 available)\n",
      "  Added 1 rows for 'gcc' (out of 1 available)\n",
      "  Added 1 rows for 'Pega Workflows' (out of 1 available)\n",
      "  Added 1 rows for 'REXX' (out of 1 available)\n",
      "  Added 1 rows for 'Mendix' (out of 1 available)\n",
      "  Added 2 rows for 'STAFFWARE' (out of 2 available)\n",
      "  Added 3 rows for 'Formspath' (out of 3 available)\n",
      "  Added 3 rows for 'ADS/Online' (out of 7 available)\n",
      "  Added 1 rows for 'Brightware proprietary' (out of 1 available)\n",
      "\n",
      "Sampling for column 'project_prf_relative_size'...\n",
      "  Added 2 rows for 'XXXL' (out of 2 available)\n",
      "\n",
      "Sampling for column 'tech_tf_architecture'...\n",
      "  Added 3 rows for 'Stand-alone' (out of 14 available)\n",
      "  Added 3 rows for 'Multi-tier with web interface' (out of 25 available)\n",
      "\n",
      "Sampling for column 'external_eef_organisation_type_other'...\n",
      "  Added 3 rows for '1' (out of 517 available)\n",
      "\n",
      "Removed 1 duplicate rows\n",
      "\n",
      "=== SUMMARY ===\n",
      "Original sample size: 939\n",
      "Additional rows added: 191\n",
      "Final dataset size: 1130\n",
      "Size increase: 20.3%\n",
      "Enhanced dataset shape: (1130, 86)\n",
      "\n",
      "7. Verifying categories coverage...\n",
      "\n",
      "=== CATEGORY COVERAGE VERIFICATION ===\n",
      "\n",
      "Column 'external_eef_data_quality_rating':\n",
      "  Before: 4 categories\n",
      "  After:  4 categories\n",
      "\n",
      "Column 'external_eef_industry_sector':\n",
      "  Before: 2 categories\n",
      "  After:  15 categories\n",
      "  New categories added: ['Service Industry', 'Communication', 'Professional Services', 'Insurance', 'Electronics & Computers', 'Logistics', 'Construction', 'Medical & Health Care', 'Manufacturing', 'Defence', 'Government', 'Wholesale & Retail', 'Missing']\n",
      "\n",
      "Column 'project_prf_application_group':\n",
      "  Before: 5 categories\n",
      "  After:  12 categories\n",
      "  New categories added: ['mathematically intensive application', 'real-time application', 'infrastructure software', 'mathematically-intensive application', 'missing', 'business application', 'business application; infrastructure software']\n",
      "\n",
      "Column 'project_prf_development_type':\n",
      "  Before: 5 categories\n",
      "  After:  7 categories\n",
      "  New categories added: ['Porting', 'Other']\n",
      "\n",
      "Column 'tech_tf_development_platform':\n",
      "  Before: 6 categories\n",
      "  After:  7 categories\n",
      "  New categories added: ['Hand Held']\n",
      "\n",
      "Column 'tech_tf_language_type':\n",
      "  Before: 6 categories\n",
      "  After:  7 categories\n",
      "  New categories added: ['APG']\n",
      "\n",
      "Column 'tech_tf_primary_programming_language':\n",
      "  Before: 49 categories\n",
      "  After:  129 categories\n",
      "  New categories added: ['Object oriented language', 'AB INITIO', 'FORTRAN', 'BRE', 'Doc1 Designer (Entorno visual)', 'Enablon', 'COGNOS', 'PASCAL', 'Ada', 'iPlanet Netscape Application Server', 'Centura', 'PYTHON', 'RPL', 'Adobe Flex', 'J2EE', 'INGRES', 'Express', 'IDEAL', 'BPM', 'ABF', 'Data base language', 'IEF', 'PERIPHONICS', 'SAS', 'COOL:Gen', 'MANTIS', 'A:G', 'Magic', 'Jdeveloper', 'ADO.Net', 'Datastage', 'APPS', 'BO', 'PowerPlay', 'EJB', 'VisualFoxPro', 'ColdFusion', 'OutlookVBA', 'C/AL', 'TNSDL', 'Huron/Object Star', 'DRIFT', 'Informatica PowerCenter', 'Siebel', 'LEX', 'MATLAB', 'NCR teradata scripting', 'Delphi', 'Upfront', 'Visual Studio .Net', 'Azure', 'CICS', 'ACCEL', 'IIS', 'LISP', 'SLEL', 'HPS', 'BEA Weblogic', 'ASAP', 'PHP', 'Spreadsheet', 'Perl', 'UNIFACE', 'ARBOR/BP', 'SLOGAN', 'MS-Navision Properitory Language', 'Periproducer', 'Caa', 'BASIC', 'XGML', 'IBM WTX', 'Must Modeller', 'gcc', 'Pega Workflows', 'REXX', 'Mendix', 'STAFFWARE', 'Formspath', 'ADS/Online', 'Brightware proprietary']\n",
      "\n",
      "Column 'project_prf_relative_size':\n",
      "  Before: 9 categories\n",
      "  After:  10 categories\n",
      "  New categories added: ['XXXL']\n",
      "\n",
      "Column 'project_prf_case_tool_used':\n",
      "  Before: 4 categories\n",
      "  After:  4 categories\n",
      "\n",
      "Column 'tech_tf_architecture':\n",
      "  Before: 6 categories\n",
      "  After:  8 categories\n",
      "  New categories added: ['Stand-alone', 'Multi-tier with web interface']\n",
      "\n",
      "Column 'tech_tf_client_server':\n",
      "  Before: 5 categories\n",
      "  After:  5 categories\n",
      "\n",
      "Column 'tech_tf_dbms_used':\n",
      "  Before: 3 categories\n",
      "  After:  3 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_banking':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_financial,_property_&_business_services':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_government':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_communications':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_education_institution':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_medical_and_health_care':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_transport_&_storage':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_wholesale_&_retail_trade':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_revenue':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_financial_(banking,_insurance,_stock)':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_missing':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_credit_card_processor':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_amusement/game_center':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_insurance':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_revenue_collection':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_agriculture,_forestry,_fishing,_hunting':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_(banking,_insurance,_stock)':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_finance':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_top_public_administration':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'external_eef_organisation_type_other':\n",
      "  Before: 1 categories\n",
      "  After:  2 categories\n",
      "  New categories added: [1]\n",
      "\n",
      "Column 'project_prf_application_type_top_financial_transaction_process/accounting':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_client_server':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_missing':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_transaction/production_system':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_management_information_system':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_data_warehouse':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_surveillance_and_security':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_office_information_system':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_business_application':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_web_based_application':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_sales_contact_management':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_electronic_data_interchange':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_workflow_support_&_management':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_automated_customer_statements':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_customer_billing/relationship_management':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_mobile_application':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_online_analysis_and_reporting':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_office_automation':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_executive_information_system':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_top_personnel_system':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "Column 'project_prf_application_type_other':\n",
      "  Before: 2 categories\n",
      "  After:  2 categories\n",
      "\n",
      "8. Applying final preprocessing stages...\n",
      "Encoding 1 multi-value columns: ['process_pmf_development_methodologies']\n",
      "Encoded process_pmf_development_methodologies into 8 binary columns\n",
      "One-hot encoding 10 categorical columns: ['external_eef_data_quality_rating', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'project_prf_relative_size', 'project_prf_case_tool_used', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_dbms_used', 'project_prf_team_size_group']\n",
      "No duplicate column names after fixing\n",
      "Fixed 42 column names for PyCaret compatibility\n",
      "\n",
      "=== Final Data Validation ===\n",
      "Final shape: (1130, 137)\n",
      "Target column: project_prf_normalised_work_effort\n",
      "Total missing values: 22218\n",
      "Total infinite values: 0\n",
      "\n",
      "Data types:\n",
      "  Numeric columns: 80\n",
      "  Categorical columns: 3\n",
      "\n",
      "Target variable 'project_prf_normalised_work_effort' statistics:\n",
      "  Mean: 4788.58\n",
      "  Std: 13705.26\n",
      "  Min: 6.00\n",
      "  Max: 230514.00\n",
      "  Missing: 0\n",
      "\n",
      "9. Final validation and duplicate check...\n",
      "Original sample shape: (939, 51)\n",
      "Final processed shape: (1130, 137)\n",
      "Columns added: 86\n",
      "Rows added: 191\n",
      "Error in integrated pipeline: [Errno 13] Permission denied: '../data\\\\enhanced_sample_final.csv'\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '../data\\\\enhanced_sample_final.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Optional: Run the main function when script is executed directly\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     final_df, metadata = main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m     output_path = os.path.join(DATA_FOLDER, \u001b[33m'\u001b[39m\u001b[33menhanced_sample_final.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m final_df.to_csv(output_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     81\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPIPELINE COMPLETED SUCCESSFULLY!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\pycaret311\\Lib\\site-packages\\pandas\\core\\generic.py:3902\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3891\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3893\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3894\u001b[39m     frame=df,\n\u001b[32m   3895\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3899\u001b[39m     decimal=decimal,\n\u001b[32m   3900\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3902\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter).to_csv(\n\u001b[32m   3903\u001b[39m     path_or_buf,\n\u001b[32m   3904\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   3905\u001b[39m     sep=sep,\n\u001b[32m   3906\u001b[39m     encoding=encoding,\n\u001b[32m   3907\u001b[39m     errors=errors,\n\u001b[32m   3908\u001b[39m     compression=compression,\n\u001b[32m   3909\u001b[39m     quoting=quoting,\n\u001b[32m   3910\u001b[39m     columns=columns,\n\u001b[32m   3911\u001b[39m     index_label=index_label,\n\u001b[32m   3912\u001b[39m     mode=mode,\n\u001b[32m   3913\u001b[39m     chunksize=chunksize,\n\u001b[32m   3914\u001b[39m     quotechar=quotechar,\n\u001b[32m   3915\u001b[39m     date_format=date_format,\n\u001b[32m   3916\u001b[39m     doublequote=doublequote,\n\u001b[32m   3917\u001b[39m     escapechar=escapechar,\n\u001b[32m   3918\u001b[39m     storage_options=storage_options,\n\u001b[32m   3919\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\pycaret311\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1152\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m   1131\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1133\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m   1134\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m   1135\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1150\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1151\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1152\u001b[39m csv_formatter.save()\n\u001b[32m   1154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1155\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\pycaret311\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:247\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[32m    248\u001b[39m     \u001b[38;5;28mself\u001b[39m.filepath_or_buffer,\n\u001b[32m    249\u001b[39m     \u001b[38;5;28mself\u001b[39m.mode,\n\u001b[32m    250\u001b[39m     encoding=\u001b[38;5;28mself\u001b[39m.encoding,\n\u001b[32m    251\u001b[39m     errors=\u001b[38;5;28mself\u001b[39m.errors,\n\u001b[32m    252\u001b[39m     compression=\u001b[38;5;28mself\u001b[39m.compression,\n\u001b[32m    253\u001b[39m     storage_options=\u001b[38;5;28mself\u001b[39m.storage_options,\n\u001b[32m    254\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    255\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    257\u001b[39m         handles.handle,\n\u001b[32m    258\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    263\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    264\u001b[39m     )\n\u001b[32m    266\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\pycaret311\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    858\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    859\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    860\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    861\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    862\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    864\u001b[39m             handle,\n\u001b[32m    865\u001b[39m             ioargs.mode,\n\u001b[32m    866\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    867\u001b[39m             errors=errors,\n\u001b[32m    868\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    869\u001b[39m         )\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    871\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    872\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: '../data\\\\enhanced_sample_final.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optional: Run the main function when script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    final_df, metadata = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ac3a9-bc0e-41c9-9124-3dd2be13e40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c520c646-d7cf-483f-aa5c-659eb3e8fbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb563e26-7450-4f38-9221-912aecd07190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8b117-c932-418a-b982-44c989db2612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61685304-5a54-4daf-9872-1e33ba246218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0966f-dd74-4d16-9b63-f3af45b43ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad9699-f16e-489e-9725-d38f2f22417d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262c7f6-cc14-442e-9fe8-7079e36344f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71847fde-1fe0-4d2f-83b3-2d2ba389dea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f02c95-8a89-4f2c-adfd-21894c2692ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c18ad0-d0a0-41b1-89e7-f459e5cc956d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead2007-b5c9-4e98-8ebe-640698dc3f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd6291-dd67-4a80-8851-a751e261a45e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
