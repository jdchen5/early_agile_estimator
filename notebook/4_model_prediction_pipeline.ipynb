{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a76e7ba-b60f-4e20-8bad-96153c1e5860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Prediction Pipeline - Using Three Trained Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e224d572-ce5f-4b10-b268-56fb1f6174c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbde0bb4-adea-4572-8e69-077d242f899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Try to import PyCaret for model loading\n",
    "try:\n",
    "    from pycaret.regression import load_model as pycaret_load_model, predict_model\n",
    "    PYCARET_AVAILABLE = True\n",
    "    logging.info(\"PyCaret is available for model loading\")\n",
    "except ImportError:\n",
    "    PYCARET_AVAILABLE = False\n",
    "    logging.warning(\"PyCaret not available, falling back to pickle loading\")\n",
    "\n",
    "# Configure logging with timestamps\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dfee12a-ae8f-459e-a504-2bc390ee00b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Define paths (adjust these to match your folder structure)\n",
    "MODELS_DIR= \"../models\"\n",
    "DATA_FOLDER = \"../data\"\n",
    "LOGS_FOLDER = \"../logs\"\n",
    "SCALER_FILE = 'standard_scaler'\n",
    "FEATURE_COLS_FILE = 'pycaret_processed_features_before_model_training.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c11274f0-7e4b-4414-a2aa-c44a5de98994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp printing activated.\n",
      "Cell executed at: 2025-05-27 20:40:46.359207\n"
     ]
    }
   ],
   "source": [
    "# Sets up an automatic timestamp printout after each Jupyter cell execution \n",
    "# and configures the default visualization style.\n",
    "from IPython import get_ipython\n",
    "\n",
    "def setup_timestamp_callback():\n",
    "    \"\"\"Setup a timestamp callback for Jupyter cells without clearing existing callbacks.\"\"\"\n",
    "    ip = get_ipython()\n",
    "    if ip is not None:\n",
    "        # Define timestamp function\n",
    "        def print_timestamp(*args, **kwargs):\n",
    "            \"\"\"Print timestamp after cell execution.\"\"\"\n",
    "            print(f\"Cell executed at: {datetime.now()}\")\n",
    "        \n",
    "        # Check if our callback is already registered\n",
    "        callbacks = ip.events.callbacks.get('post_run_cell', [])\n",
    "        for cb in callbacks:\n",
    "            if hasattr(cb, '__name__') and cb.__name__ == 'print_timestamp':\n",
    "                # Already registered\n",
    "                return\n",
    "                \n",
    "        # Register new callback if not already present\n",
    "        ip.events.register('post_run_cell', print_timestamp)\n",
    "        print(\"Timestamp printing activated.\")\n",
    "    else:\n",
    "        print(\"Not running in IPython/Jupyter environment.\")\n",
    "\n",
    "# Setup timestamp callback\n",
    "setup_timestamp_callback()\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38602b0c-b97c-469b-aa87-b1fe6f6af49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-05-27 20:40:46.383839\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def ensure_directories():\n",
    "    \"\"\"Ensure all required directories exist.\"\"\"\n",
    "    for directory in [MODELS_DIR, DATA_FOLDER, LOGS_FOLDER]:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            logging.info(f\"Created directory: {directory}\")\n",
    "\n",
    "def list_available_models() -> List[str]:\n",
    "    \"\"\"\n",
    "    List all available models in the models directory.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: Names of available trained models (without extension)\n",
    "    \"\"\"\n",
    "    ensure_directories()\n",
    "    \n",
    "    # Get all .pkl files except scaler.pkl\n",
    "    model_files = []\n",
    "    for f in os.listdir(MODELS_DIR):\n",
    "        if f.endswith('.pkl') and not (SCALER_FILE in f.lower()):\n",
    "            model_name = os.path.splitext(f)[0]\n",
    "            model_files.append(model_name)\n",
    "    \n",
    "    logging.info(f\"Found {len(model_files)} available models: {model_files}\")\n",
    "    return model_files\n",
    "\n",
    "def check_required_models() -> Dict[str, bool]:\n",
    "    \"\"\"\n",
    "    Check for existing models in the models directory.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, bool]: Dictionary with model status (True if found)\n",
    "    \"\"\"\n",
    "    ensure_directories()\n",
    "    \n",
    "    # Get all model files\n",
    "    existing_files = os.listdir(MODELS_DIR)\n",
    "    existing_models = [f for f in existing_files if f.endswith('.pkl')]\n",
    "    \n",
    "    # Check for at least one model and optionally a scaler\n",
    "    has_models = any(f for f in existing_models if not f.startswith(SCALER_FILE))\n",
    "    has_scaler = any(f for f in existing_models if f.startswith(SCALER_FILE))\n",
    "    \n",
    "    model_status = {\n",
    "        \"models_available\": has_models,\n",
    "        \"scaler_available\": has_scaler,\n",
    "        \"found_models\": [os.path.splitext(f)[0] for f in existing_models if not (SCALER_FILE in f.lower())]\n",
    "    }\n",
    "    \n",
    "    if has_models:\n",
    "        logging.info(f\"Found models in '{MODELS_DIR}': {[f for f in existing_models if not (SCALER_FILE in f.lower())]}\")\n",
    "    else:\n",
    "        logging.warning(f\"No trained models found in '{MODELS_DIR}'\")\n",
    "        logging.info(\"Please train and save models via PyCaret in your Jupyter notebook before using this app.\")\n",
    "    \n",
    "    return model_status\n",
    "\n",
    "def load_single_model(model_name: str) -> Optional[Any]:\n",
    "    \"\"\"\n",
    "    Load a model from the models directory, supporting both PyCaret and pickle formats.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model to load (without extension)\n",
    "        \n",
    "    Returns:\n",
    "        Optional[Any]: Loaded model object or None if not found/error\n",
    "    \"\"\"\n",
    "    model_path = os.path.join(MODELS_DIR, model_name)\n",
    "    model_path_with_ext = os.path.join(MODELS_DIR, f'{model_name}.pkl')\n",
    "    \n",
    "    # First, try PyCaret's load_model (without extension)\n",
    "    if PYCARET_AVAILABLE:\n",
    "        try:\n",
    "            model = pycaret_load_model(model_path)\n",
    "            logging.info(f\"Successfully loaded PyCaret model: {model_name}\")\n",
    "            logging.info(f\"Model type: {type(model)}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"PyCaret load failed for '{model_name}': {str(e)}\")\n",
    "    \n",
    "    # Fall back to pickle loading (with extension)\n",
    "    if os.path.exists(model_path_with_ext):\n",
    "        try:\n",
    "            with open(model_path_with_ext, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            \n",
    "            logging.info(f\"Successfully loaded pickle model: {model_name}\")\n",
    "            logging.info(f\"Loaded object type: {type(model)}\")\n",
    "            \n",
    "            # For PyCaret models saved with pickle, they might be wrapped\n",
    "            if hasattr(model, 'predict') or hasattr(model, '_predict'):\n",
    "                return model\n",
    "            else:\n",
    "                logging.error(f\"Loaded object '{model_name}' does not have prediction capability\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading model '{model_name}' with pickle: {str(e)}\")\n",
    "    \n",
    "    logging.error(f\"Model file '{model_name}' not found in either format\")\n",
    "    return None\n",
    "\n",
    "def load_scaler() -> Optional[Any]:\n",
    "    \"\"\"\n",
    "    Load the scaler if available in the models directory.\n",
    "    \n",
    "    Returns:\n",
    "        Optional[Any]: Loaded scaler object or None if not found/error\n",
    "    \"\"\"\n",
    "    ensure_directories()\n",
    "    scaler_files = [f for f in os.listdir(MODELS_DIR) \n",
    "                   if SCALER_FILE in f.lower() and f.endswith('.pkl')]\n",
    "    \n",
    "    if not scaler_files:\n",
    "        logging.info(\"No scaler found. Proceeding without scaling.\")\n",
    "        return None\n",
    "    \n",
    "    # Use the first scaler found\n",
    "    scaler_path = os.path.join(MODELS_DIR, scaler_files[0])\n",
    "    \n",
    "    try:\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        # Check it's a scaler, not an ndarray:\n",
    "        if hasattr(scaler, 'transform'):\n",
    "            logging.info(f\"Loaded scaler: {type(scaler)}\")\n",
    "            return scaler\n",
    "        else:\n",
    "            logging.error(\"Scaler file is not a transformer object.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading scaler: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def align_features_to_model(features_df: pd.DataFrame, expected_columns: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensures the features_df has all columns (in correct order) that the model expects.\n",
    "    Missing columns are filled with 0.\n",
    "    \"\"\"\n",
    "    for col in expected_columns:\n",
    "        if col not in features_df.columns:\n",
    "            features_df[col] = 0\n",
    "    return features_df[expected_columns]\n",
    "\n",
    "\n",
    "def get_expected_feature_names_from_model(model=None) -> list:\n",
    "    \"\"\"\n",
    "    Gets the expected feature names from a loaded model or falls back to CSV or hardcoded defaults.\n",
    "    \"\"\"\n",
    "    # 1. Try to extract from model directly (best, ensures proper order)\n",
    "    if model is not None:\n",
    "        if hasattr(model, 'feature_names_in_'):\n",
    "            return list(model.feature_names_in_)\n",
    "        # Some scikit-learn wrappers/transformers or PyCaret models may expose columns differently\n",
    "        elif hasattr(model, 'X') and hasattr(model.X, 'columns'):\n",
    "            return list(model.X.columns)\n",
    "        # Sometimes PyCaret pipelines have steps that expose feature names\n",
    "        elif hasattr(model, 'named_steps'):\n",
    "            for step in model.named_steps.values():\n",
    "                if hasattr(step, 'feature_names_in_'):\n",
    "                    return list(step.feature_names_in_)\n",
    "\n",
    "    # 2. Fall back to CSV file (as in PyCaret export)\n",
    "    feature_cols_path = os.path.join(DATA_FOLDER, FEATURE_COLS_FILE)\n",
    "    if os.path.exists(feature_cols_path):\n",
    "        try:\n",
    "            # Try reading as a single-column CSV (common PyCaret export)\n",
    "            cols = pd.read_csv(feature_cols_path, header=None)[0].tolist()\n",
    "            if isinstance(cols[0], str):  # Confirm it's feature names, not numbers\n",
    "                return cols\n",
    "        except Exception:\n",
    "            try:\n",
    "                # Try reading CSV as DataFrame with headers\n",
    "                df = pd.read_csv(feature_cols_path)\n",
    "                return list(df.columns)\n",
    "            except Exception:\n",
    "                pass  # Both CSV reads failed\n",
    "\n",
    "    # 3. Fallback: Hardcoded minimal list (order should match your model training)\n",
    "    return [\n",
    "        'project_prf_year_of_project',\n",
    "        'project_prf_functional_size', \n",
    "        'project_prf_max_team_size',\n",
    "        'process_pmf_docs',\n",
    "        'tech_tf_tools_used',\n",
    "        'people_prf_personnel_changes'\n",
    "    ]\n",
    "\n",
    "def prepare_full_feature_dataframe(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure the DataFrame has all features (in correct order) used by the model.\n",
    "    Missing columns are added with 0. Extra columns are ignored.\n",
    "    \"\"\"\n",
    "    feature_names = get_expected_feature_names_from_csv()  # Should return full list of expected features\n",
    "    # Align columns: keep only needed, add missing with 0, in correct order\n",
    "    output_df = input_df.reindex(columns=feature_names, fill_value=0)\n",
    "    return output_df\n",
    "\n",
    "    \n",
    "def prepare_features_for_pycaret(features_dict: dict, model=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare a prediction DataFrame with the exact columns expected by the model pipeline.\n",
    "    Missing columns will be filled with 0.\n",
    "\n",
    "    Args:\n",
    "        features_dict (dict): Input features as {column_name: value}\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with all expected columns, ready for prediction.\n",
    "    \"\"\"\n",
    "    feature_names = get_expected_feature_names_from_model(model)\n",
    "    # Make sure the input dict has all required keys\n",
    "    row = {col: features_dict.get(col, 0) for col in feature_names}\n",
    "    features_df = pd.DataFrame([row])\n",
    "    # Ensure order and add any missing cols (paranoia)\n",
    "    features_df = align_features_to_model(features_df, feature_names)\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee257c3-7e45-4a40-b3e9-c6e27c20c0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-05-27 20:40:46.413824\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENHANCED MODEL LOADING AND MANAGEMENT\n",
    "# =============================================================================\n",
    "\n",
    "def load_all_models() -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Load all available models from the models directory.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Dict]: Dictionary with model information and loaded objects\n",
    "    \"\"\"\n",
    "    print(\"Loading all available models...\")\n",
    "    \n",
    "    available_models = list_available_models()\n",
    "    loaded_models = {}\n",
    "    \n",
    "    for model_name in available_models:\n",
    "        try:\n",
    "            model = load_single_model(model_name)\n",
    "            if model is not None:\n",
    "                loaded_models[model_name] = {\n",
    "                    'model': model,\n",
    "                    'name': model_name,\n",
    "                    'type': type(model).__name__\n",
    "                }\n",
    "                print(f\"✓ Loaded: {model_name}\")\n",
    "            else:\n",
    "                print(f\"✗ Failed to load: {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading {model_name}: {str(e)}\")\n",
    "    \n",
    "    # Load scaler if available\n",
    "    scaler = load_scaler()\n",
    "    if scaler is not None:\n",
    "        loaded_models[SCALER_FILE] = {\n",
    "            'model': scaler,\n",
    "            'name': SCALER_FILE,\n",
    "            'type': type(scaler).__name__\n",
    "        }\n",
    "\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded {len([k for k in loaded_models.keys() if k != SCALER_FILE])} models\")\n",
    "    return loaded_models\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED PREDICTION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def predict_with_single_model(\n",
    "    features_dict: dict, \n",
    "    model_name: str, \n",
    "    loaded_models: Dict,\n",
    "    use_scaler: bool = True\n",
    ") -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Make a prediction using a specific model with enhanced error handling.\n",
    "    \n",
    "    Args:\n",
    "        features_dict (dict): Input features as dictionary\n",
    "        model_name (str): Name of the model to use\n",
    "        loaded_models (Dict): Dictionary of loaded models\n",
    "        use_scaler (bool): Whether to apply scaling if available\n",
    "        \n",
    "    Returns:\n",
    "        Optional[float]: Predicted value or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if model_name not in loaded_models:\n",
    "            logging.error(f\"Model '{model_name}' not loaded\")\n",
    "            return None\n",
    "        \n",
    "        model = loaded_models[model_name]['model']\n",
    "        \n",
    "        # Prepare features with all expected columns\n",
    "        features_df = prepare_features_for_pycaret(features_dict, model=model)\n",
    "        \n",
    "        # Apply scaler if present and requested\n",
    "        if use_scaler and 'standard_scaler' in loaded_models:\n",
    "            scaler = loaded_models['standard_scaler']['model']\n",
    "            num_cols = features_df.select_dtypes(include=[np.number]).columns\n",
    "            features_df[num_cols] = scaler.transform(features_df[num_cols])\n",
    "        \n",
    "        # PyCaret prediction\n",
    "        if PYCARET_AVAILABLE:\n",
    "            try:\n",
    "                predictions = predict_model(model, data=features_df)\n",
    "                # Look for typical prediction column\n",
    "                for col in ['prediction_label', 'Label', 'pred', 'prediction', 'target']:\n",
    "                    if col in predictions.columns:\n",
    "                        val = float(predictions[col].iloc[0])\n",
    "                        return max(0.1, val)\n",
    "                # Fallback: last column\n",
    "                return max(0.1, float(predictions.iloc[0, -1]))\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"PyCaret prediction failed: {str(e)}\")\n",
    "        # Fallback: scikit-learn style\n",
    "        if hasattr(model, 'predict'):\n",
    "            try:\n",
    "                pred = model.predict(features_df)\n",
    "                val = float(pred.flat[0]) if isinstance(pred, np.ndarray) else float(pred[0])\n",
    "                return max(0.1, val)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Standard prediction failed: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Prediction error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def make_predictions_all_models(\n",
    "    features_dict: dict, \n",
    "    loaded_models: dict,\n",
    "    use_scaler: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Make predictions using all loaded models (excluding scaler).\n",
    "    \"\"\"\n",
    "    predictions = {}\n",
    "    model_names = [name for name in loaded_models.keys() if name != 'standard_scaler']\n",
    "    print(f\"\\nMaking predictions with {len(model_names)} models...\")\n",
    "    for model_name in model_names:\n",
    "        try:\n",
    "            prediction = predict_with_single_model(\n",
    "                features_dict, model_name, loaded_models, use_scaler\n",
    "            )\n",
    "            if prediction is not None:\n",
    "                predictions[model_name] = prediction\n",
    "                print(f\"✓ {model_name}: {prediction:.3f}\")\n",
    "            else:\n",
    "                print(f\"✗ {model_name}: Prediction failed\")\n",
    "                predictions[model_name] = None\n",
    "        except Exception as e:\n",
    "            print(f\"✗ {model_name}: Error - {str(e)}\")\n",
    "            predictions[model_name] = None\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ce7d3b2-1264-4883-b7a4-ede6aa7eb5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-05-27 20:40:46.429063\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE USAGE AND DEMONSTRATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def demo_single_prediction():\n",
    "    \"\"\"Demonstrate single prediction functionality.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DEMO: Single Prediction\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load models\n",
    "    loaded_models = load_all_models()\n",
    "    if not loaded_models:\n",
    "        print(\"❌ No models available for demo\")\n",
    "        return\n",
    "    \n",
    "    # Sample features\n",
    "    sample_features = {\n",
    "        'project_prf_year_of_project': 2023,\n",
    "        'project_prf_functional_size': 150.5,\n",
    "        'project_prf_max_team_size': 8,\n",
    "        'process_pmf_docs': 1,\n",
    "        'tech_tf_tools_used': 2,\n",
    "        'people_prf_personnel_changes': 0\n",
    "    }\n",
    "    \n",
    "    # Make predictions with all models\n",
    "    predictions = make_predictions_all_models(sample_features, loaded_models)\n",
    "    \n",
    "    print(f\"\\nSample Input: {sample_features}\")\n",
    "    print(f\"Predictions: {predictions}\")\n",
    "    \n",
    "    # Plot predictions comparison\n",
    "    plot_predictions_comparison(predictions)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def demo_what_if_analysis():\n",
    "    \"\"\"Demonstrate what-if analysis functionality.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DEMO: What-If Analysis\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load models\n",
    "    loaded_models = load_all_models()\n",
    "    if not loaded_models:\n",
    "        print(\"❌ No models available for demo\")\n",
    "        return\n",
    "    \n",
    "    # Base features\n",
    "    base_features = {\n",
    "        'project_prf_year_of_project': 2023,\n",
    "        'project_prf_functional_size': 150.5,\n",
    "        'project_prf_max_team_size': 8,\n",
    "        'process_pmf_docs': 1,\n",
    "        'tech_tf_tools_used': 2,\n",
    "        'people_prf_personnel_changes': 0\n",
    "    }\n",
    "    \n",
    "    # Vary functional size\n",
    "    model_name = list(loaded_models.keys())[0]  # Use first available model\n",
    "    if model_name == SCALER_FILE:\n",
    "        model_name = list(loaded_models.keys())[1] if len(loaded_models) > 1 else None\n",
    "    \n",
    "    if model_name:\n",
    "        size_values = [50, 100, 150, 200, 250, 300]\n",
    "        what_if_results = analyze_what_if(\n",
    "            base_features, model_name, loaded_models,\n",
    "            'project_prf_functional_size', size_values\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nWhat-If Analysis Results for 'project_prf_functional_size':\")\n",
    "        for size, pred in zip(what_if_results[\"param_values\"], what_if_results[\"predictions\"]):\n",
    "            print(f\"  Size {size}: Prediction {pred:.3f}\")\n",
    "        \n",
    "        # Plot what-if analysis\n",
    "        plot_what_if_analysis(what_if_results, 'project_prf_functional_size')\n",
    "        \n",
    "        return what_if_results\n",
    "    else:\n",
    "        print(\"❌ No suitable model found for what-if analysis\")\n",
    "        return None\n",
    "\n",
    "def demo_feature_importance():\n",
    "    \"\"\"Demonstrate feature importance analysis.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DEMO: Feature Importance Analysis\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load models\n",
    "    loaded_models = load_all_models()\n",
    "    if not loaded_models:\n",
    "        print(\"❌ No models available for demo\")\n",
    "        return\n",
    "    \n",
    "    # Analyze feature importance\n",
    "    importance_results = analyze_feature_importance_all_models(loaded_models)\n",
    "    \n",
    "    if importance_results:\n",
    "        feature_names = get_expected_feature_names_from_csv()\n",
    "        \n",
    "        print(\"\\nFeature Importance Summary:\")\n",
    "        for model_name, importance in importance_results.items():\n",
    "            if len(importance) == len(feature_names):\n",
    "                print(f\"\\n{model_name}:\")\n",
    "                for feature, imp in zip(feature_names, importance):\n",
    "                    print(f\"  {feature}: {imp:.4f}\")\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plot_feature_importance(importance_results)\n",
    "        \n",
    "        return importance_results\n",
    "    else:\n",
    "        print(\"❌ No feature importance available from loaded models\")\n",
    "        return None\n",
    "\n",
    "def create_interactive_prediction_interface():\n",
    "    \"\"\"Create an interactive interface for making predictions.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"INTERACTIVE PREDICTION INTERFACE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load models\n",
    "    loaded_models = load_all_models()\n",
    "    if not loaded_models:\n",
    "        print(\"❌ No models available\")\n",
    "        return\n",
    "    \n",
    "    feature_names = get_expected_feature_names_from_csv()\n",
    "    \n",
    "    print(\"\\nEnter values for the following features:\")\n",
    "    print(\"(Press Enter to use default values)\")\n",
    "    \n",
    "    # Default values\n",
    "    default_values = {\n",
    "        'project_prf_year_of_project': 2023,\n",
    "        'project_prf_functional_size': 150.0,\n",
    "        'project_prf_max_team_size': 8,\n",
    "        'process_pmf_docs': 1,\n",
    "        'tech_tf_tools_used': 2,\n",
    "        'people_prf_personnel_changes': 0\n",
    "    }\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    for feature in feature_names:\n",
    "        default_val = default_values.get(feature, 0)\n",
    "        try:\n",
    "            user_input = input(f\"{feature} (default: {default_val}): \").strip()\n",
    "            if user_input:\n",
    "                features[feature] = float(user_input)\n",
    "            else:\n",
    "                features[feature] = default_val\n",
    "        except ValueError:\n",
    "            print(f\"Invalid input, using default value: {default_val}\")\n",
    "            features[feature] = default_val\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nOperation cancelled by user\")\n",
    "            return\n",
    "    \n",
    "    print(f\"\\nInput Features: {features}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = make_predictions_all_models(features, loaded_models)\n",
    "    \n",
    "    print(\"\\nPrediction Results:\")\n",
    "    for model_name, prediction in predictions.items():\n",
    "        if prediction is not None:\n",
    "            print(f\"  {model_name}: {prediction:.3f}\")\n",
    "        else:\n",
    "            print(f\"  {model_name}: Failed\")\n",
    "    \n",
    "    # Calculate ensemble\n",
    "    valid_predictions = [v for v in predictions.values() if v is not None]\n",
    "    if valid_predictions:\n",
    "        ensemble_pred = np.mean(valid_predictions)\n",
    "        ensemble_std = np.std(valid_predictions) if len(valid_predictions) > 1 else 0\n",
    "        print(f\"\\nEnsemble Prediction: {ensemble_pred:.3f} ± {ensemble_std:.3f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plot_predictions_comparison(predictions)\n",
    "    \n",
    "    return features, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acdcdcdc-c7d7-48cd-aa20-8d78936a5706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-05-27 20:40:46.443524\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BATCH PROCESSING UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def process_csv_file(csv_path: str, output_path: str = None):\n",
    "    \"\"\"\n",
    "    Process a CSV file with batch predictions.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to input CSV file\n",
    "        output_path (str): Path to save results (optional)\n",
    "    \"\"\"\n",
    "    print(f\"\\n📁 Processing CSV file: {csv_path}\")\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"❌ File not found: {csv_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Load models\n",
    "    loaded_models = load_all_models()\n",
    "    if not loaded_models:\n",
    "        print(\"❌ No models available\")\n",
    "        return None\n",
    "    \n",
    "    # Load data\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        print(f\"✓ Loaded data: {data.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading CSV: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Process predictions\n",
    "    results_df = batch_predict_dataframe(data, loaded_models)\n",
    "    \n",
    "    # Save results\n",
    "    if output_path is None:\n",
    "        output_path = csv_path.replace('.csv', '_predictions.csv')\n",
    "    \n",
    "    try:\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        print(f\"✓ Results saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving results: {str(e)}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def compare_model_performance(results_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compare performance metrics across models.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pd.DataFrame): DataFrame with predictions from multiple models\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get model columns (exclude original features and ensemble)\n",
    "    feature_names = get_expected_feature_names_from_csv()\n",
    "    model_columns = [col for col in results_df.columns \n",
    "                    if col not in feature_names \n",
    "                    and col not in ['ensemble_prediction', 'prediction_std']]\n",
    "    \n",
    "    if not model_columns:\n",
    "        print(\"❌ No model predictions found in results\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nPrediction Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_col in model_columns:\n",
    "        if model_col in results_df.columns:\n",
    "            predictions = results_df[model_col].dropna()\n",
    "            if len(predictions) > 0:\n",
    "                print(f\"\\n{model_col}:\")\n",
    "                print(f\"  Count: {len(predictions)}\")\n",
    "                print(f\"  Mean:  {predictions.mean():.3f}\")\n",
    "                print(f\"  Std:   {predictions.std():.3f}\")\n",
    "                print(f\"  Min:   {predictions.min():.3f}\")\n",
    "                print(f\"  Max:   {predictions.max():.3f}\")\n",
    "                print(f\"  Median: {predictions.median():.3f}\")\n",
    "    \n",
    "    # Correlation analysis\n",
    "    try:\n",
    "        model_data = results_df[model_columns].dropna()\n",
    "        if len(model_data) > 1 and len(model_columns) > 1:\n",
    "            print(\"\\nModel Correlation Matrix:\")\n",
    "            print(\"-\" * 30)\n",
    "            correlation_matrix = model_data.corr()\n",
    "            print(correlation_matrix.round(3))\n",
    "            \n",
    "            # Plot correlation heatmap if matplotlib is available\n",
    "            try:\n",
    "                import matplotlib.pyplot as plt\n",
    "                import seaborn as sns\n",
    "                \n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "                plt.title('Model Prediction Correlations')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "            except ImportError:\n",
    "                pass\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in correlation analysis: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd969244-de9f-47f3-880b-7948fce241d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Enhanced Model Prediction Pipeline - Jupyter Notebook Ready!\n",
      "============================================================\n",
      "Setting up notebook environment...\n",
      "\n",
      "Checking required packages:\n",
      "✓ pandas\n",
      "✓ numpy\n",
      "❌ scikit-learn - REQUIRED\n",
      "\n",
      "Checking optional packages:\n",
      "✓ matplotlib\n",
      "✓ seaborn\n",
      "✓ plotly\n",
      "\n",
      "📁 Directory structure:\n",
      "✓ ../models/\n",
      "✓ ../data/\n",
      "✓ ../logs/\n",
      "\n",
      "🎯 Notebook environment setup complete!\n",
      "\n",
      "============================================================\n",
      "🚀 QUICK START GUIDE\n",
      "============================================================\n",
      "\n",
      "📋 USAGE INSTRUCTIONS:\n",
      "\n",
      "1. SETUP:\n",
      "   - Ensure your trained models are saved in the 'models/' directory\n",
      "   - Models should be saved as .pkl files (PyCaret or pickle format)\n",
      "   - Feature names should be saved in 'data/pycaret_processed_features_before_model_training.csv'\n",
      "\n",
      "2. BASIC USAGE:\n",
      "   # Run complete analysis\n",
      "   results = run_complete_analysis()\n",
      "\n",
      "   # Single prediction\n",
      "   predictions = demo_single_prediction()\n",
      "\n",
      "   # Interactive interface\n",
      "   create_interactive_prediction_interface()\n",
      "\n",
      "3. BATCH PROCESSING:\n",
      "   # Process CSV file\n",
      "   results_df = process_csv_file('your_data.csv')\n",
      "\n",
      "   # Custom data\n",
      "   results_df, models, importance = run_enhanced_prediction_pipeline(\n",
      "       data_path='your_data.csv'\n",
      "   )\n",
      "\n",
      "4. ANALYSIS:\n",
      "   # What-if analysis\n",
      "   what_if_results = demo_what_if_analysis()\n",
      "\n",
      "   # Feature importance\n",
      "   importance_results = demo_feature_importance()\n",
      "\n",
      "5. FILES GENERATED:\n",
      "   - enhanced_predictions.csv (prediction results)\n",
      "   - feature_importance.csv (feature importance data)\n",
      "   - Various plots and visualizations\n",
      "\n",
      "💡 TIP: Start with setup_notebook_environment() to check your setup!\n",
      "    \n",
      "\n",
      "✅ Found 3 trained models!\n",
      "You can now run the analysis pipeline.\n",
      "Cell executed at: 2025-05-27 20:40:46.475185\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION AND JUPYTER NOTEBOOK INTERFACE\n",
    "# =============================================================================\n",
    "\n",
    "def run_complete_analysis():\n",
    "    \"\"\"Run a complete analysis pipeline with all demonstrations.\"\"\"\n",
    "    print(\"🚀 STARTING COMPLETE PREDICTION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # 1. Run main pipeline\n",
    "        print(\"\\n1️⃣ Running Enhanced Prediction Pipeline...\")\n",
    "        results_df, loaded_models, importance_results = run_enhanced_prediction_pipeline()\n",
    "        \n",
    "        if results_df is None:\n",
    "            print(\"❌ Pipeline failed to complete\")\n",
    "            return\n",
    "        \n",
    "        # 2. Demo single prediction\n",
    "        print(\"\\n2️⃣ Single Prediction Demo...\")\n",
    "        demo_predictions = demo_single_prediction()\n",
    "        \n",
    "        # 3. Demo what-if analysis\n",
    "        print(\"\\n3️⃣ What-If Analysis Demo...\")\n",
    "        what_if_results = demo_what_if_analysis()\n",
    "        \n",
    "        # 4. Demo feature importance\n",
    "        print(\"\\n4️⃣ Feature Importance Demo...\")\n",
    "        feature_importance = demo_feature_importance()\n",
    "        \n",
    "        # 5. Model performance comparison\n",
    "        print(\"\\n5️⃣ Model Performance Comparison...\")\n",
    "        compare_model_performance(results_df)\n",
    "        \n",
    "        print(\"\\n✅ COMPLETE ANALYSIS FINISHED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return {\n",
    "            'results_df': results_df,\n",
    "            'loaded_models': loaded_models,\n",
    "            'importance_results': importance_results,\n",
    "            'demo_predictions': demo_predictions,\n",
    "            'what_if_results': what_if_results,\n",
    "            'feature_importance': feature_importance\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in complete analysis: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# JUPYTER NOTEBOOK HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def setup_notebook_environment():\n",
    "    \"\"\"Setup the notebook environment with necessary imports and configurations.\"\"\"\n",
    "    print(\"Setting up notebook environment...\")\n",
    "    \n",
    "    # Check for required packages\n",
    "    required_packages = ['pandas', 'numpy', 'scikit-learn']\n",
    "    optional_packages = ['matplotlib', 'seaborn', 'plotly']\n",
    "    \n",
    "    print(\"\\nChecking required packages:\")\n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"✓ {package}\")\n",
    "        except ImportError:\n",
    "            print(f\"❌ {package} - REQUIRED\")\n",
    "    \n",
    "    print(\"\\nChecking optional packages:\")\n",
    "    for package in optional_packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"✓ {package}\")\n",
    "        except ImportError:\n",
    "            print(f\"⚠️ {package} - Optional (for visualization)\")\n",
    "    \n",
    "    # Setup directories\n",
    "    ensure_directories()\n",
    "    \n",
    "    print(\"\\n📁 Directory structure:\")\n",
    "    for directory in [MODELS_DIR, DATA_FOLDER, LOGS_FOLDER]:\n",
    "        status = \"✓\" if os.path.exists(directory) else \"❌\"\n",
    "        print(f\"{status} {directory}/\")\n",
    "    \n",
    "    print(\"\\n🎯 Notebook environment setup complete!\")\n",
    "\n",
    "def quick_start_guide():\n",
    "    \"\"\"Display a quick start guide for users.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🚀 QUICK START GUIDE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\"\"\n",
    "📋 USAGE INSTRUCTIONS:\n",
    "\n",
    "1. SETUP:\n",
    "   - Ensure your trained models are saved in the 'models/' directory\n",
    "   - Models should be saved as .pkl files (PyCaret or pickle format)\n",
    "   - Feature names should be saved in 'data/pycaret_processed_features_before_model_training.csv'\n",
    "\n",
    "2. BASIC USAGE:\n",
    "   # Run complete analysis\n",
    "   results = run_complete_analysis()\n",
    "   \n",
    "   # Single prediction\n",
    "   predictions = demo_single_prediction()\n",
    "   \n",
    "   # Interactive interface\n",
    "   create_interactive_prediction_interface()\n",
    "\n",
    "3. BATCH PROCESSING:\n",
    "   # Process CSV file\n",
    "   results_df = process_csv_file('your_data.csv')\n",
    "   \n",
    "   # Custom data\n",
    "   results_df, models, importance = run_enhanced_prediction_pipeline(\n",
    "       data_path='your_data.csv'\n",
    "   )\n",
    "\n",
    "4. ANALYSIS:\n",
    "   # What-if analysis\n",
    "   what_if_results = demo_what_if_analysis()\n",
    "   \n",
    "   # Feature importance\n",
    "   importance_results = demo_feature_importance()\n",
    "\n",
    "5. FILES GENERATED:\n",
    "   - enhanced_predictions.csv (prediction results)\n",
    "   - feature_importance.csv (feature importance data)\n",
    "   - Various plots and visualizations\n",
    "\n",
    "💡 TIP: Start with setup_notebook_environment() to check your setup!\n",
    "    \"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION BLOCK FOR JUPYTER NOTEBOOKS\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This block runs when the script is executed directly\n",
    "    print(\"🎯 Enhanced Model Prediction Pipeline - Jupyter Notebook Ready!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Setup environment\n",
    "    setup_notebook_environment()\n",
    "    \n",
    "    # Display quick start guide\n",
    "    quick_start_guide()\n",
    "    \n",
    "    # Check for models\n",
    "    model_status = check_required_models()\n",
    "    \n",
    "    if model_status[\"models_available\"]:\n",
    "        print(f\"\\n✅ Found {len(model_status['found_models'])} trained models!\")\n",
    "        print(\"You can now run the analysis pipeline.\")\n",
    "        \n",
    "        # Uncomment the line below to run complete analysis automatically\n",
    "        # run_complete_analysis()\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n⚠️ No trained models found.\")\n",
    "        print(\"Please train and save your models first using PyCaret.\")\n",
    "        print(\"Then run: run_complete_analysis()\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e51df2c8-5b13-4f29-a5cc-206b51096c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-05-27 20:40:46.480492\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def run_enhanced_prediction_pipeline(\n",
    "    data_path=None, \n",
    "    sample_data=None, \n",
    "    use_scaler=True,\n",
    "    save_results=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete enhanced prediction pipeline with all features.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"STARTING ENHANCED PREDICTION PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Check and load models\n",
    "    model_status = check_required_models()\n",
    "    if not model_status[\"models_available\"]:\n",
    "        print(\"❌ No models found! Please train and save models first.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Step 2: Load all models\n",
    "    loaded_models = load_all_models()\n",
    "    if not loaded_models:\n",
    "        print(\"❌ Failed to load any models!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Step 3: Prepare data\n",
    "    new_data = prepare_new_data(data_path, sample_data)\n",
    "    \n",
    "    # Step 4: Batch predictions\n",
    "    results_df = batch_predict_dataframe(new_data, loaded_models)\n",
    "    \n",
    "    # Step 5: Feature importance analysis\n",
    "    importance_results = analyze_feature_importance_all_models(loaded_models)\n",
    "    \n",
    "    # Step 6: Save results if requested\n",
    "    if save_results:\n",
    "        ensure_directories()\n",
    "        \n",
    "        # Save predictions\n",
    "        predictions_path = os.path.join(DATA_FOLDER, 'enhanced_predictions.csv')\n",
    "        results_df.to_csv(predictions_path, index=False)\n",
    "        print(f\"\\n📁 Predictions saved to: {predictions_path}\")\n",
    "        \n",
    "        # Save feature importance if available\n",
    "        if importance_results:\n",
    "            importance_path = os.path.join(DATA_FOLDER, 'feature_importance.csv')\n",
    "            feature_names = get_expected_feature_names_from_csv()\n",
    "            \n",
    "            importance_df_data = {}\n",
    "            for model_name, importance in importance_results.items():\n",
    "                if len(importance) == len(feature_names):\n",
    "                    importance_df_data[model_name] = importance\n",
    "                else:\n",
    "                    print(f\"⚠️ Feature importance length mismatch for {model_name}\")\n",
    "            \n",
    "            if importance_df_data:\n",
    "                importance_df = pd.DataFrame(importance_df_data, index=feature_names)\n",
    "                importance_df.to_csv(importance_path)\n",
    "                print(f\"📁 Feature importance saved to: {importance_path}\")\n",
    "    \n",
    "    # Step 7: Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ENHANCED PREDICTION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model_names = [name for name in loaded_models.keys() if name != SCALER_FILE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65232a82-6316-4278-b391-158f84a11c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-05-27 20:40:46.507332\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENHANCED BATCH PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_new_data(new_data_path=None, sample_data=None):\n",
    "    \"\"\"\n",
    "    Prepare new data for prediction with enhanced flexibility.\n",
    "    \n",
    "    Parameters:\n",
    "    - new_data_path: path to CSV file with new data\n",
    "    - sample_data: pandas DataFrame with new data\n",
    "    \n",
    "    Returns:\n",
    "    - prepared_data: DataFrame ready for prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    if new_data_path:\n",
    "        # Load new data from CSV\n",
    "        new_data = pd.read_csv(new_data_path)\n",
    "        print(f\"Loaded new data from {new_data_path}: {new_data.shape}\")\n",
    "    elif sample_data is not None:\n",
    "        new_data = sample_data.copy()\n",
    "        print(f\"Using provided sample data: {new_data.shape}\")\n",
    "    else:\n",
    "        # Create sample data for demonstration\n",
    "        print(\"Creating sample data for demonstration...\")\n",
    "        new_data = pd.DataFrame({\n",
    "            'project_prf_year_of_project': [2023, 2024, 2022, 2023],\n",
    "            'project_prf_functional_size': [150.5, 200.0, 89.3, 175.2],\n",
    "            'project_prf_used_methodology': ['Agile', 'Waterfall', 'Agile', 'Hybrid'],\n",
    "            'technology_prf_development_platform': ['Web', 'Desktop', 'Mobile', 'Web'],\n",
    "            'technology_prf_language_type': ['4GL', '3GL', '4GL', '3GL'],\n",
    "            'project_prf_development_type': ['New', 'Enhancement', 'New', 'Enhancement'],\n",
    "            'technology_prf_database_used': ['Yes', 'Yes', 'No', 'Yes'],\n",
    "            'project_prf_max_team_size': [8, 12, 5, 10]\n",
    "        })\n",
    "        print(f\"Created sample data: {new_data.shape}\")\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "def batch_predict_dataframe(data_df: pd.DataFrame, loaded_models: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make predictions for a batch of samples in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data_df (pd.DataFrame): DataFrame with samples to predict\n",
    "        loaded_models (Dict): Dictionary of loaded models\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with original data and predictions\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing batch predictions for {len(data_df)} samples...\")\n",
    "\n",
    "    # Align columns before iterating\n",
    "    features_df = prepare_full_feature_dataframe(data_df)\n",
    "    \n",
    "    results = []\n",
    "    model_names = [name for name in loaded_models.keys() if name != SCALER_FILE]\n",
    "\n",
    "    # Ensure all columns exist and in the correct order for every sample\n",
    "    feature_names = get_expected_feature_names_from_csv()\n",
    "    \n",
    "    for idx, row in features_df.iterrows():\n",
    "        row_dict = row.to_dict()\n",
    "        predictions = make_predictions_all_models(row_dict, loaded_models, use_scaler=True)\n",
    "\n",
    "        # Combine original (unaligned) row + predictions\n",
    "        orig_row_dict = data_df.iloc[idx].to_dict()\n",
    "        result_row = orig_row_dict.copy()\n",
    "        result_row.update(predictions)\n",
    "\n",
    "        # Ensemble\n",
    "        valid_preds = [v for v in predictions.values() if v is not None]\n",
    "        result_row['ensemble_prediction'] = np.mean(valid_preds) if valid_preds else None\n",
    "        result_row['prediction_std'] = np.std(valid_preds) if len(valid_preds) > 1 else 0\n",
    "\n",
    "        results.append(result_row)\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"Processed {idx + 1}/{len(data_df)} samples...\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"Batch prediction completed for {len(results_df)} samples\")\n",
    "    \n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3438a7e1-a83d-40a6-9199-8a15c8451399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-05-27 20:40:46.526809\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def plot_predictions_comparison(predictions_dict: Dict[str, float]):\n",
    "    \"\"\"Plot comparison of predictions from different models.\"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # Filter out None values\n",
    "        valid_predictions = {k: v for k, v in predictions_dict.items() if v is not None}\n",
    "        \n",
    "        if not valid_predictions:\n",
    "            print(\"No valid predictions to plot\")\n",
    "            return\n",
    "        \n",
    "        models = list(valid_predictions.keys())\n",
    "        values = list(valid_predictions.values())\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(models, values, color='skyblue', alpha=0.7)\n",
    "        plt.title('Model Predictions Comparison')\n",
    "        plt.xlabel('Models')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{value:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Matplotlib not available for plotting\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting predictions: {str(e)}\")\n",
    "\n",
    "def plot_feature_importance(importance_results: Dict[str, np.ndarray]):\n",
    "    \"\"\"Plot feature importance for all models.\"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if not importance_results:\n",
    "            print(\"No feature importance data to plot\")\n",
    "            return\n",
    "        \n",
    "        feature_names = get_expected_feature_names_from_csv()\n",
    "        \n",
    "        fig, axes = plt.subplots(len(importance_results), 1, \n",
    "                               figsize=(12, 4 * len(importance_results)))\n",
    "        \n",
    "        if len(importance_results) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, (model_name, importance) in enumerate(importance_results.items()):\n",
    "            if len(importance) == len(feature_names):\n",
    "                axes[idx].bar(feature_names, importance, alpha=0.7)\n",
    "                axes[idx].set_title(f'Feature Importance - {model_name}')\n",
    "                axes[idx].set_xlabel('Features')\n",
    "                axes[idx].set_ylabel('Importance')\n",
    "                axes[idx].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Matplotlib not available for plotting\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting feature importance: {str(e)}\")\n",
    "\n",
    "def plot_what_if_analysis(what_if_results: Dict[str, List], param_name: str):\n",
    "    \"\"\"Plot what-if analysis results.\"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if not what_if_results[\"param_values\"]:\n",
    "            print(\"No what-if analysis data to plot\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(what_if_results[\"param_values\"], what_if_results[\"predictions\"], \n",
    "                'o-', linewidth=2, markersize=8, color='darkblue')\n",
    "        plt.title(f'What-If Analysis: Impact of {param_name}')\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel('Predicted Value')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Matplotlib not available for plotting\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting what-if analysis: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e3c18e1-78de-47d4-a5fc-87d1f91af09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-05-27 20:40:46.537345\n"
     ]
    }
   ],
   "source": [
    "def compare_model_and_csv_features(model, csv_path):\n",
    "    # Get model features\n",
    "    if hasattr(model, 'feature_names_in_'):\n",
    "        model_features = list(model.feature_names_in_)\n",
    "    else:\n",
    "        model_features = []\n",
    "    \n",
    "    # Get CSV features\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        csv_features = list(df.columns)\n",
    "    except:\n",
    "        csv_features = list(pd.read_csv(csv_path, header=None)[0])\n",
    "    \n",
    "    print(\"Features in CSV file:\")\n",
    "    print(csv_features)\n",
    "    print(f\"\\nDoes CSV contain target column? {'project_prf_normalised_work_effort' in csv_features}\")\n",
    "\n",
    "   \n",
    "    # For PyCaret models, try different ways to get feature names\n",
    "    if hasattr(model, 'feature_names_in_'):\n",
    "        features = model.feature_names_in_\n",
    "    elif hasattr(model, 'feature_names_'):\n",
    "        features = model.feature_names_\n",
    "    else:\n",
    "        # Try to get from the pipeline\n",
    "        try:\n",
    "            features = model[:-1].get_feature_names_out()\n",
    "        except:\n",
    "            features = \"Could not determine features\"\n",
    "    \n",
    "    print(f\"\\n{model_name} expected features:\")\n",
    "    print(features)\n",
    "    print(f\"\\nDoes MODEL contain target column? {'project_prf_normalised_work_effort' in features}\")\n",
    "    \n",
    "    # Compare\n",
    "    model_not_in_csv = [f for f in model_features if f not in csv_features]\n",
    "    csv_not_in_model = [f for f in csv_features if f not in model_features]\n",
    "    print(\"Features in model but NOT in CSV:\", model_not_in_csv)\n",
    "    print(\"Features in CSV but NOT in model:\", csv_not_in_model)\n",
    "    return model_not_in_csv, csv_not_in_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2428e45f-9868-4b21-80e4-bdd29bfbf981",
   "metadata": {},
   "source": [
    "<span style=\"color: red;font-weight: bold; font-size: 40px;\">Temporarily fix PyCaret's PyCaret's model serialization issues </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "155db17b-0779-4af3-857f-2e90f4afa062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up notebook environment...\n",
      "\n",
      "Checking required packages:\n",
      "✓ pandas\n",
      "✓ numpy\n",
      "❌ scikit-learn - REQUIRED\n",
      "\n",
      "Checking optional packages:\n",
      "✓ matplotlib\n",
      "✓ seaborn\n",
      "✓ plotly\n",
      "\n",
      "📁 Directory structure:\n",
      "✓ ../models/\n",
      "✓ ../data/\n",
      "✓ ../logs/\n",
      "\n",
      "🎯 Notebook environment setup complete!\n",
      "\n",
      "============================================================\n",
      "🚀 QUICK START GUIDE\n",
      "============================================================\n",
      "\n",
      "📋 USAGE INSTRUCTIONS:\n",
      "\n",
      "1. SETUP:\n",
      "   - Ensure your trained models are saved in the 'models/' directory\n",
      "   - Models should be saved as .pkl files (PyCaret or pickle format)\n",
      "   - Feature names should be saved in 'data/pycaret_processed_features_before_model_training.csv'\n",
      "\n",
      "2. BASIC USAGE:\n",
      "   # Run complete analysis\n",
      "   results = run_complete_analysis()\n",
      "\n",
      "   # Single prediction\n",
      "   predictions = demo_single_prediction()\n",
      "\n",
      "   # Interactive interface\n",
      "   create_interactive_prediction_interface()\n",
      "\n",
      "3. BATCH PROCESSING:\n",
      "   # Process CSV file\n",
      "   results_df = process_csv_file('your_data.csv')\n",
      "\n",
      "   # Custom data\n",
      "   results_df, models, importance = run_enhanced_prediction_pipeline(\n",
      "       data_path='your_data.csv'\n",
      "   )\n",
      "\n",
      "4. ANALYSIS:\n",
      "   # What-if analysis\n",
      "   what_if_results = demo_what_if_analysis()\n",
      "\n",
      "   # Feature importance\n",
      "   importance_results = demo_feature_importance()\n",
      "\n",
      "5. FILES GENERATED:\n",
      "   - enhanced_predictions.csv (prediction results)\n",
      "   - feature_importance.csv (feature importance data)\n",
      "   - Various plots and visualizations\n",
      "\n",
      "💡 TIP: Start with setup_notebook_environment() to check your setup!\n",
      "    \n",
      "Loading all available models...\n",
      "Transformation Pipeline and Model Successfully Loaded\n",
      "✓ Loaded: top_model_1_KNeighborsRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Scaler file is not a transformer object.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Loaded\n",
      "✓ Loaded: top_model_2_OrthogonalMatchingPursuit\n",
      "Transformation Pipeline and Model Successfully Loaded\n",
      "✓ Loaded: top_model_3_ExtraTreesRegressor\n",
      "\n",
      "Successfully loaded 3 models\n",
      "\n",
      "--- Checking features for model: top_model_1_KNeighborsRegressor ---\n",
      "Features in CSV file:\n",
      "['project_prf_year_of_project', 'external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_type', 'project_prf_functional_size', 'project_prf_max_team_size', 'process_pmf_docs', 'tech_tf_tools_used', 'people_prf_personnel_changes', 'project_prf_application_group_business_application', 'project_prf_application_group_infrastructure_software', 'project_prf_application_group_mathematically_intensive_application', 'project_prf_application_group_real_time_application', 'project_prf_development_type_new_development', 'project_prf_development_type_re_development', 'tech_tf_development_platform_mr', 'tech_tf_development_platform_multi', 'tech_tf_development_platform_pc', 'tech_tf_development_platform_proprietary', 'tech_tf_language_type_4GL', 'tech_tf_language_type_5GL', 'tech_tf_primary_programming_language_abap', 'tech_tf_primary_programming_language_c_', 'tech_tf_primary_programming_language_c__', 'tech_tf_primary_programming_language_java', 'tech_tf_primary_programming_language_javascript', 'tech_tf_primary_programming_language_oracle', 'tech_tf_primary_programming_language_pl_i', 'tech_tf_primary_programming_language_proprietary_agile_platform', 'project_prf_relative_size_l', 'project_prf_relative_size_m1', 'project_prf_relative_size_m2', 'project_prf_relative_size_s', 'project_prf_relative_size_xs', 'project_prf_relative_size_xxs', 'project_prf_team_size_group_2', 'project_prf_team_size_group_21_30', 'project_prf_team_size_group_3_4', 'project_prf_team_size_group_41_50', 'project_prf_team_size_group_5_8', 'project_prf_team_size_group_61_70', 'project_prf_team_size_group_9_14', 'project_prf_team_size_group_Missing', 'process_pmf_development_methodologies_agile_development__iterative', 'process_pmf_development_methodologies_agile_development__joint_application_development_jad__multifunctional_teams', 'process_pmf_development_methodologies_agile_development__personal_software_process_psp__unified_process', 'process_pmf_development_methodologies_agile_development__scrum', 'process_pmf_development_methodologies_agile_development__unified_process', 'tech_tf_architecture_client_server', 'tech_tf_architecture_multi_tier_with_web_public_interface', 'tech_tf_architecture_stand_alone', 'tech_tf_client_server_no', 'tech_tf_client_server_yes', 'tech_tf_web_development_tech_tf_web_development', 'tech_tf_web_development_web', 'tech_tf_dbms_used_tech_tf_dbms_used', 'tech_tf_dbms_used_yes', 'project_prf_cost_currency_canada__dollar', 'project_prf_cost_currency_european__euro']\n",
      "\n",
      "Does CSV contain target column? False\n",
      "\n",
      "top_model_1_KNeighborsRegressor expected features:\n",
      "['project_prf_year_of_project', 'external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_type', 'project_prf_functional_size', 'project_prf_max_team_size', 'process_pmf_docs', 'tech_tf_tools_used', 'people_prf_personnel_changes', 'project_prf_application_group_business_application', 'project_prf_application_group_infrastructure_software', 'project_prf_application_group_mathematically_intensive_application', 'project_prf_application_group_real_time_application', 'project_prf_development_type_new_development', 'project_prf_development_type_re_development', 'tech_tf_development_platform_mr', 'tech_tf_development_platform_multi', 'tech_tf_development_platform_pc', 'tech_tf_development_platform_proprietary', 'tech_tf_language_type_4GL', 'tech_tf_language_type_5GL', 'tech_tf_primary_programming_language_abap', 'tech_tf_primary_programming_language_c_', 'tech_tf_primary_programming_language_c__', 'tech_tf_primary_programming_language_java', 'tech_tf_primary_programming_language_javascript', 'tech_tf_primary_programming_language_oracle', 'tech_tf_primary_programming_language_pl_i', 'tech_tf_primary_programming_language_proprietary_agile_platform', 'project_prf_relative_size_l', 'project_prf_relative_size_m1', 'project_prf_relative_size_m2', 'project_prf_relative_size_s', 'project_prf_relative_size_xs', 'project_prf_relative_size_xxs', 'project_prf_team_size_group_2', 'project_prf_team_size_group_21_30', 'project_prf_team_size_group_3_4', 'project_prf_team_size_group_41_50', 'project_prf_team_size_group_5_8', 'project_prf_team_size_group_61_70', 'project_prf_team_size_group_9_14', 'project_prf_team_size_group_Missing', 'process_pmf_development_methodologies_agile_development__iterative', 'process_pmf_development_methodologies_agile_development__joint_application_development_jad__multifunctional_teams', 'process_pmf_development_methodologies_agile_development__personal_software_process_psp__unified_process', 'process_pmf_development_methodologies_agile_development__scrum', 'process_pmf_development_methodologies_agile_development__unified_process', 'tech_tf_architecture_client_server', 'tech_tf_architecture_multi_tier_with_web_public_interface', 'tech_tf_architecture_stand_alone', 'tech_tf_client_server_no', 'tech_tf_client_server_yes', 'tech_tf_web_development_tech_tf_web_development', 'tech_tf_web_development_web', 'tech_tf_dbms_used_tech_tf_dbms_used', 'tech_tf_dbms_used_yes', 'project_prf_cost_currency_canada__dollar', 'project_prf_cost_currency_european__euro', 'project_prf_normalised_work_effort']\n",
      "\n",
      "Does MODEL contain target column? True\n",
      "Features in model but NOT in CSV: ['project_prf_normalised_work_effort']\n",
      "Features in CSV but NOT in model: []\n",
      "\n",
      "--- Checking features for model: top_model_2_OrthogonalMatchingPursuit ---\n",
      "Features in CSV file:\n",
      "['project_prf_year_of_project', 'external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_type', 'project_prf_functional_size', 'project_prf_max_team_size', 'process_pmf_docs', 'tech_tf_tools_used', 'people_prf_personnel_changes', 'project_prf_application_group_business_application', 'project_prf_application_group_infrastructure_software', 'project_prf_application_group_mathematically_intensive_application', 'project_prf_application_group_real_time_application', 'project_prf_development_type_new_development', 'project_prf_development_type_re_development', 'tech_tf_development_platform_mr', 'tech_tf_development_platform_multi', 'tech_tf_development_platform_pc', 'tech_tf_development_platform_proprietary', 'tech_tf_language_type_4GL', 'tech_tf_language_type_5GL', 'tech_tf_primary_programming_language_abap', 'tech_tf_primary_programming_language_c_', 'tech_tf_primary_programming_language_c__', 'tech_tf_primary_programming_language_java', 'tech_tf_primary_programming_language_javascript', 'tech_tf_primary_programming_language_oracle', 'tech_tf_primary_programming_language_pl_i', 'tech_tf_primary_programming_language_proprietary_agile_platform', 'project_prf_relative_size_l', 'project_prf_relative_size_m1', 'project_prf_relative_size_m2', 'project_prf_relative_size_s', 'project_prf_relative_size_xs', 'project_prf_relative_size_xxs', 'project_prf_team_size_group_2', 'project_prf_team_size_group_21_30', 'project_prf_team_size_group_3_4', 'project_prf_team_size_group_41_50', 'project_prf_team_size_group_5_8', 'project_prf_team_size_group_61_70', 'project_prf_team_size_group_9_14', 'project_prf_team_size_group_Missing', 'process_pmf_development_methodologies_agile_development__iterative', 'process_pmf_development_methodologies_agile_development__joint_application_development_jad__multifunctional_teams', 'process_pmf_development_methodologies_agile_development__personal_software_process_psp__unified_process', 'process_pmf_development_methodologies_agile_development__scrum', 'process_pmf_development_methodologies_agile_development__unified_process', 'tech_tf_architecture_client_server', 'tech_tf_architecture_multi_tier_with_web_public_interface', 'tech_tf_architecture_stand_alone', 'tech_tf_client_server_no', 'tech_tf_client_server_yes', 'tech_tf_web_development_tech_tf_web_development', 'tech_tf_web_development_web', 'tech_tf_dbms_used_tech_tf_dbms_used', 'tech_tf_dbms_used_yes', 'project_prf_cost_currency_canada__dollar', 'project_prf_cost_currency_european__euro']\n",
      "\n",
      "Does CSV contain target column? False\n",
      "\n",
      "top_model_2_OrthogonalMatchingPursuit expected features:\n",
      "['project_prf_year_of_project', 'external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_type', 'project_prf_functional_size', 'project_prf_max_team_size', 'process_pmf_docs', 'tech_tf_tools_used', 'people_prf_personnel_changes', 'project_prf_application_group_business_application', 'project_prf_application_group_infrastructure_software', 'project_prf_application_group_mathematically_intensive_application', 'project_prf_application_group_real_time_application', 'project_prf_development_type_new_development', 'project_prf_development_type_re_development', 'tech_tf_development_platform_mr', 'tech_tf_development_platform_multi', 'tech_tf_development_platform_pc', 'tech_tf_development_platform_proprietary', 'tech_tf_language_type_4GL', 'tech_tf_language_type_5GL', 'tech_tf_primary_programming_language_abap', 'tech_tf_primary_programming_language_c_', 'tech_tf_primary_programming_language_c__', 'tech_tf_primary_programming_language_java', 'tech_tf_primary_programming_language_javascript', 'tech_tf_primary_programming_language_oracle', 'tech_tf_primary_programming_language_pl_i', 'tech_tf_primary_programming_language_proprietary_agile_platform', 'project_prf_relative_size_l', 'project_prf_relative_size_m1', 'project_prf_relative_size_m2', 'project_prf_relative_size_s', 'project_prf_relative_size_xs', 'project_prf_relative_size_xxs', 'project_prf_team_size_group_2', 'project_prf_team_size_group_21_30', 'project_prf_team_size_group_3_4', 'project_prf_team_size_group_41_50', 'project_prf_team_size_group_5_8', 'project_prf_team_size_group_61_70', 'project_prf_team_size_group_9_14', 'project_prf_team_size_group_Missing', 'process_pmf_development_methodologies_agile_development__iterative', 'process_pmf_development_methodologies_agile_development__joint_application_development_jad__multifunctional_teams', 'process_pmf_development_methodologies_agile_development__personal_software_process_psp__unified_process', 'process_pmf_development_methodologies_agile_development__scrum', 'process_pmf_development_methodologies_agile_development__unified_process', 'tech_tf_architecture_client_server', 'tech_tf_architecture_multi_tier_with_web_public_interface', 'tech_tf_architecture_stand_alone', 'tech_tf_client_server_no', 'tech_tf_client_server_yes', 'tech_tf_web_development_tech_tf_web_development', 'tech_tf_web_development_web', 'tech_tf_dbms_used_tech_tf_dbms_used', 'tech_tf_dbms_used_yes', 'project_prf_cost_currency_canada__dollar', 'project_prf_cost_currency_european__euro', 'project_prf_normalised_work_effort']\n",
      "\n",
      "Does MODEL contain target column? True\n",
      "Features in model but NOT in CSV: ['project_prf_normalised_work_effort']\n",
      "Features in CSV but NOT in model: []\n",
      "\n",
      "--- Checking features for model: top_model_3_ExtraTreesRegressor ---\n",
      "Features in CSV file:\n",
      "['project_prf_year_of_project', 'external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_type', 'project_prf_functional_size', 'project_prf_max_team_size', 'process_pmf_docs', 'tech_tf_tools_used', 'people_prf_personnel_changes', 'project_prf_application_group_business_application', 'project_prf_application_group_infrastructure_software', 'project_prf_application_group_mathematically_intensive_application', 'project_prf_application_group_real_time_application', 'project_prf_development_type_new_development', 'project_prf_development_type_re_development', 'tech_tf_development_platform_mr', 'tech_tf_development_platform_multi', 'tech_tf_development_platform_pc', 'tech_tf_development_platform_proprietary', 'tech_tf_language_type_4GL', 'tech_tf_language_type_5GL', 'tech_tf_primary_programming_language_abap', 'tech_tf_primary_programming_language_c_', 'tech_tf_primary_programming_language_c__', 'tech_tf_primary_programming_language_java', 'tech_tf_primary_programming_language_javascript', 'tech_tf_primary_programming_language_oracle', 'tech_tf_primary_programming_language_pl_i', 'tech_tf_primary_programming_language_proprietary_agile_platform', 'project_prf_relative_size_l', 'project_prf_relative_size_m1', 'project_prf_relative_size_m2', 'project_prf_relative_size_s', 'project_prf_relative_size_xs', 'project_prf_relative_size_xxs', 'project_prf_team_size_group_2', 'project_prf_team_size_group_21_30', 'project_prf_team_size_group_3_4', 'project_prf_team_size_group_41_50', 'project_prf_team_size_group_5_8', 'project_prf_team_size_group_61_70', 'project_prf_team_size_group_9_14', 'project_prf_team_size_group_Missing', 'process_pmf_development_methodologies_agile_development__iterative', 'process_pmf_development_methodologies_agile_development__joint_application_development_jad__multifunctional_teams', 'process_pmf_development_methodologies_agile_development__personal_software_process_psp__unified_process', 'process_pmf_development_methodologies_agile_development__scrum', 'process_pmf_development_methodologies_agile_development__unified_process', 'tech_tf_architecture_client_server', 'tech_tf_architecture_multi_tier_with_web_public_interface', 'tech_tf_architecture_stand_alone', 'tech_tf_client_server_no', 'tech_tf_client_server_yes', 'tech_tf_web_development_tech_tf_web_development', 'tech_tf_web_development_web', 'tech_tf_dbms_used_tech_tf_dbms_used', 'tech_tf_dbms_used_yes', 'project_prf_cost_currency_canada__dollar', 'project_prf_cost_currency_european__euro']\n",
      "\n",
      "Does CSV contain target column? False\n",
      "\n",
      "top_model_3_ExtraTreesRegressor expected features:\n",
      "['project_prf_year_of_project', 'external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_type', 'project_prf_functional_size', 'project_prf_max_team_size', 'process_pmf_docs', 'tech_tf_tools_used', 'people_prf_personnel_changes', 'project_prf_application_group_business_application', 'project_prf_application_group_infrastructure_software', 'project_prf_application_group_mathematically_intensive_application', 'project_prf_application_group_real_time_application', 'project_prf_development_type_new_development', 'project_prf_development_type_re_development', 'tech_tf_development_platform_mr', 'tech_tf_development_platform_multi', 'tech_tf_development_platform_pc', 'tech_tf_development_platform_proprietary', 'tech_tf_language_type_4GL', 'tech_tf_language_type_5GL', 'tech_tf_primary_programming_language_abap', 'tech_tf_primary_programming_language_c_', 'tech_tf_primary_programming_language_c__', 'tech_tf_primary_programming_language_java', 'tech_tf_primary_programming_language_javascript', 'tech_tf_primary_programming_language_oracle', 'tech_tf_primary_programming_language_pl_i', 'tech_tf_primary_programming_language_proprietary_agile_platform', 'project_prf_relative_size_l', 'project_prf_relative_size_m1', 'project_prf_relative_size_m2', 'project_prf_relative_size_s', 'project_prf_relative_size_xs', 'project_prf_relative_size_xxs', 'project_prf_team_size_group_2', 'project_prf_team_size_group_21_30', 'project_prf_team_size_group_3_4', 'project_prf_team_size_group_41_50', 'project_prf_team_size_group_5_8', 'project_prf_team_size_group_61_70', 'project_prf_team_size_group_9_14', 'project_prf_team_size_group_Missing', 'process_pmf_development_methodologies_agile_development__iterative', 'process_pmf_development_methodologies_agile_development__joint_application_development_jad__multifunctional_teams', 'process_pmf_development_methodologies_agile_development__personal_software_process_psp__unified_process', 'process_pmf_development_methodologies_agile_development__scrum', 'process_pmf_development_methodologies_agile_development__unified_process', 'tech_tf_architecture_client_server', 'tech_tf_architecture_multi_tier_with_web_public_interface', 'tech_tf_architecture_stand_alone', 'tech_tf_client_server_no', 'tech_tf_client_server_yes', 'tech_tf_web_development_tech_tf_web_development', 'tech_tf_web_development_web', 'tech_tf_dbms_used_tech_tf_dbms_used', 'tech_tf_dbms_used_yes', 'project_prf_cost_currency_canada__dollar', 'project_prf_cost_currency_european__euro', 'project_prf_normalised_work_effort']\n",
      "\n",
      "Does MODEL contain target column? True\n",
      "Features in model but NOT in CSV: ['project_prf_normalised_work_effort']\n",
      "Features in CSV but NOT in model: []\n",
      "\n",
      "Making predictions with 3 models...\n",
      "✓ top_model_1_KNeighborsRegressor: 365.753\n",
      "✓ top_model_2_OrthogonalMatchingPursuit: 4872.226\n",
      "✓ top_model_3_ExtraTreesRegressor: 2632.680\n",
      "\n",
      "Sample Input: {'project_prf_year_of_project': 2023, 'project_prf_functional_size': 150.5, 'project_prf_max_team_size': 8, 'process_pmf_docs': 1, 'tech_tf_tools_used': 2, 'people_prf_personnel_changes': 0}\n",
      "Predictions: {'top_model_1_KNeighborsRegressor': 365.7528137345273, 'top_model_2_OrthogonalMatchingPursuit': 4872.2260905298, 'top_model_3_ExtraTreesRegressor': 2632.68}\n",
      "Cell executed at: 2025-05-27 20:40:47.480407\n"
     ]
    }
   ],
   "source": [
    "# Testing functions first\n",
    "TARGET_COLUMN = 'project_prf_normalised_work_effort'\n",
    "\n",
    "# CELL 1: Setup\n",
    "setup_notebook_environment()\n",
    "quick_start_guide()\n",
    "\n",
    "# CELL 2: Load models\n",
    "loaded_models = load_all_models()\n",
    "model_names = [name for name in loaded_models.keys() if name != SCALER_FILE]\n",
    "\n",
    "# CELL 2.5: Compare feature names between models and CSV file; models were trained by PyCaret\n",
    "csv_feature_file = f\"{DATA_FOLDER}/{FEATURE_COLS_FILE}\"\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n--- Checking features for model: {model_name} ---\")\n",
    "    model = loaded_models[model_name]['model']\n",
    "    compare_model_and_csv_features(model, csv_feature_file)\n",
    "\n",
    "# CELL 3: Define input for prediction\n",
    "sample_features = {\n",
    "    'project_prf_year_of_project': 2023,\n",
    "    'project_prf_functional_size': 150.5,\n",
    "    'project_prf_max_team_size': 8,\n",
    "    'process_pmf_docs': 1,\n",
    "    'tech_tf_tools_used': 2,\n",
    "    'people_prf_personnel_changes': 0\n",
    "  }\n",
    "\n",
    "# CELL 4: Predict\n",
    "print(f\"\\nMaking predictions with {len(model_names)} models...\")\n",
    "predictions = {}\n",
    "for model_name in model_names:\n",
    "    try:\n",
    "        prediction = predict_with_single_model(\n",
    "            sample_features, model_name, loaded_models, use_scaler=True\n",
    "        )\n",
    "        if prediction is not None:\n",
    "            predictions[model_name] = prediction\n",
    "            print(f\"✓ {model_name}: {prediction:.3f}\")\n",
    "        else:\n",
    "            print(f\"✗ {model_name}: Prediction failed\")\n",
    "            predictions[model_name] = None\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {model_name}: Error - {str(e)}\")\n",
    "        predictions[model_name] = None\n",
    "print(f\"\\nSample Input: {sample_features}\")\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00806c5-b4df-4996-b622-8d78187061f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d26d5-0b0c-48e6-97a7-f922f48237bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc2d458-c7ee-4e6a-988e-6e78a1c4d512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb61b2e5-80a1-4f8b-b3fe-c28c79d11c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6e80a6-c91c-4e2d-9a7c-637cca0147ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e49c3d8f-85f1-4910-b463-1f1533745e0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up notebook environment...\n",
      "\n",
      "Checking required packages:\n",
      "✓ pandas\n",
      "✓ numpy\n",
      "❌ scikit-learn - REQUIRED\n",
      "\n",
      "Checking optional packages:\n",
      "✓ matplotlib\n",
      "✓ seaborn\n",
      "✓ plotly\n",
      "\n",
      "📁 Directory structure:\n",
      "✓ ../models/\n",
      "✓ ../data/\n",
      "✓ ../logs/\n",
      "\n",
      "🎯 Notebook environment setup complete!\n",
      "\n",
      "============================================================\n",
      "🚀 QUICK START GUIDE\n",
      "============================================================\n",
      "\n",
      "📋 USAGE INSTRUCTIONS:\n",
      "\n",
      "1. SETUP:\n",
      "   - Ensure your trained models are saved in the 'models/' directory\n",
      "   - Models should be saved as .pkl files (PyCaret or pickle format)\n",
      "   - Feature names should be saved in 'data/pycaret_processed_features_before_model_training.csv'\n",
      "\n",
      "2. BASIC USAGE:\n",
      "   # Run complete analysis\n",
      "   results = run_complete_analysis()\n",
      "\n",
      "   # Single prediction\n",
      "   predictions = demo_single_prediction()\n",
      "\n",
      "   # Interactive interface\n",
      "   create_interactive_prediction_interface()\n",
      "\n",
      "3. BATCH PROCESSING:\n",
      "   # Process CSV file\n",
      "   results_df = process_csv_file('your_data.csv')\n",
      "\n",
      "   # Custom data\n",
      "   results_df, models, importance = run_enhanced_prediction_pipeline(\n",
      "       data_path='your_data.csv'\n",
      "   )\n",
      "\n",
      "4. ANALYSIS:\n",
      "   # What-if analysis\n",
      "   what_if_results = demo_what_if_analysis()\n",
      "\n",
      "   # Feature importance\n",
      "   importance_results = demo_feature_importance()\n",
      "\n",
      "5. FILES GENERATED:\n",
      "   - enhanced_predictions.csv (prediction results)\n",
      "   - feature_importance.csv (feature importance data)\n",
      "   - Various plots and visualizations\n",
      "\n",
      "💡 TIP: Start with setup_notebook_environment() to check your setup!\n",
      "    \n",
      "Loading all available models...\n",
      "Transformation Pipeline and Model Successfully Loaded\n",
      "✓ Loaded: top_model_1_KNeighborsRegressor\n",
      "Transformation Pipeline and Model Successfully Loaded\n",
      "✓ Loaded: top_model_2_OrthogonalMatchingPursuit\n",
      "Transformation Pipeline and Model Successfully Loaded\n",
      "✓ Loaded: top_model_3_ExtraTreesRegressor\n",
      "\n",
      "Successfully loaded 3 models\n",
      "\n",
      "==================================================\n",
      "DEMO: Single Prediction\n",
      "==================================================\n",
      "Loading all available models...\n",
      "Transformation Pipeline and Model Successfully Loaded\n",
      "✓ Loaded: top_model_1_KNeighborsRegressor\n",
      "Transformation Pipeline and Model Successfully Loaded\n",
      "✓ Loaded: top_model_2_OrthogonalMatchingPursuit\n",
      "Transformation Pipeline and Model Successfully Loaded\n",
      "✓ Loaded: top_model_3_ExtraTreesRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Prediction error: 'numpy.ndarray' object has no attribute 'transform'\n",
      "ERROR:root:Prediction error: 'numpy.ndarray' object has no attribute 'transform'\n",
      "ERROR:root:Prediction error: 'numpy.ndarray' object has no attribute 'transform'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded 3 models\n",
      "\n",
      "Making predictions with 3 models...\n",
      "✗ top_model_1_KNeighborsRegressor: Prediction failed\n",
      "✗ top_model_2_OrthogonalMatchingPursuit: Prediction failed\n",
      "✗ top_model_3_ExtraTreesRegressor: Prediction failed\n",
      "\n",
      "Sample Input: {'project_prf_year_of_project': 2023, 'project_prf_functional_size': 150.5, 'project_prf_max_team_size': 8, 'process_pmf_docs': 1, 'tech_tf_tools_used': 2, 'people_prf_personnel_changes': 0}\n",
      "Predictions: {'top_model_1_KNeighborsRegressor': None, 'top_model_2_OrthogonalMatchingPursuit': None, 'top_model_3_ExtraTreesRegressor': None}\n",
      "No valid predictions to plot\n",
      "\n",
      "📝 Copy the cell examples above to get started quickly in your Jupyter notebook!\n",
      "🎉 Setup complete! Happy predicting!\n",
      "\n",
      "Making predictions with 3 models...\n",
      "✗ top_model_1_KNeighborsRegressor: Error - name 'features_dict' is not defined\n",
      "✗ top_model_2_OrthogonalMatchingPursuit: Error - name 'features_dict' is not defined\n",
      "✗ top_model_3_ExtraTreesRegressor: Error - name 'features_dict' is not defined\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (286055997.py, line 63)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn predictions\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m 'return' outside function\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-05-27 14:50:13.706051\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NOTEBOOK CELL EXECUTION HELPERS\n",
    "# =============================================================================\n",
    "\n",
    "# For easy copy-paste into notebook cells:\n",
    "\n",
    "\n",
    "# CELL 1: Setup and Check Environment\n",
    "setup_notebook_environment()\n",
    "\n",
    "# CELL 2: Quick Start Guide\n",
    "quick_start_guide()\n",
    "\n",
    "loaded_models = load_all_models()\n",
    "model_names = [name for name in loaded_models.keys() if name != 'standard_scaler']\n",
    "\n",
    "# CELL 3: Single Prediction Demo\n",
    "predictions = demo_single_prediction()\n",
    "\n",
    "# CELL 4: Run Complete Analysis (if models are available)\n",
    "#results = run_complete_analysis()\n",
    "\n",
    "\n",
    "# CELL 5: Interactive Prediction Interface\n",
    "#create_interactive_prediction_interface()\n",
    "\n",
    "# CELL 6: Process your own CSV file\n",
    "# results_df = process_csv_file('path/to/your/data.csv')\n",
    "\n",
    "# CELL 7: What-If Analysis\n",
    "#what_if_results = demo_what_if_analysis()\n",
    "\n",
    "# CELL 8: Feature Importance Analysis\n",
    "#importance_results = demo_feature_importance()\n",
    "\n",
    "\n",
    "print(\"\\n📝 Copy the cell examples above to get started quickly in your Jupyter notebook!\")\n",
    "print(\"🎉 Setup complete! Happy predicting!\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nMaking predictions with {len(model_names)} models...\")\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    try:\n",
    "        prediction = predict_with_single_model(\n",
    "            features_dict, model_name, loaded_models, use_scaler\n",
    "        )\n",
    "\n",
    "        if prediction is not None:\n",
    "            predictions[model_name] = prediction\n",
    "            print(f\"✓ {model_name}: {prediction:.3f}\")\n",
    "        else:\n",
    "            print(f\"✗ {model_name}: Prediction failed\")\n",
    "            predictions[model_name] = None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {model_name}: Error - {str(e)}\")\n",
    "        predictions[model_name] = None\n",
    "\n",
    "return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6bd8eb-7e50-40b8-95b0-93ba56b01908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce7fb87-4257-48a3-91fa-9811e3d23900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42aa444-b62b-4a61-b1eb-d5a5e747b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE IMPORTANCE AND ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def get_feature_importance(model_name: str, loaded_models: Dict) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Get feature importance for a given model if available.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model to analyze\n",
    "        loaded_models (Dict): Dictionary of loaded models\n",
    "        \n",
    "    Returns:\n",
    "        Optional[np.ndarray]: Feature importance values or None if not available\n",
    "    \"\"\"\n",
    "    if model_name not in loaded_models:\n",
    "        logging.error(f\"Model '{model_name}' not found in loaded models\")\n",
    "        return None\n",
    "    \n",
    "    model = loaded_models[model_name]['model']\n",
    "    \n",
    "    try:\n",
    "        # For PyCaret models, try to access the underlying estimator\n",
    "        if hasattr(model, 'named_steps'):\n",
    "            # Pipeline case\n",
    "            estimator = None\n",
    "            for step_name, step in model.named_steps.items():\n",
    "                if hasattr(step, 'feature_importances_') or hasattr(step, 'coef_'):\n",
    "                    estimator = step\n",
    "                    break\n",
    "        elif hasattr(model, '_final_estimator'):\n",
    "            estimator = model._final_estimator\n",
    "        else:\n",
    "            estimator = model\n",
    "        \n",
    "        # Check for feature importances\n",
    "        if hasattr(estimator, 'feature_importances_'):\n",
    "            importances = estimator.feature_importances_\n",
    "            logging.info(f\"Retrieved feature importance from '{model_name}' (feature_importances_)\")\n",
    "            return importances\n",
    "        \n",
    "        # For linear models, use coefficients\n",
    "        elif hasattr(estimator, 'coef_'):\n",
    "            importances = np.abs(estimator.coef_)\n",
    "            if importances.ndim > 1:\n",
    "                importances = importances.flatten()\n",
    "            logging.info(f\"Retrieved feature importance from '{model_name}' (coefficients)\")\n",
    "            return importances\n",
    "        \n",
    "        logging.warning(f\"Model '{model_name}' does not support feature importance\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error getting feature importance for '{model_name}': {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_feature_importance_all_models(loaded_models: Dict) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Get feature importance for all models that support it.\n",
    "    \n",
    "    Args:\n",
    "        loaded_models (Dict): Dictionary of loaded models\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, np.ndarray]: Dictionary with model names and feature importances\n",
    "    \"\"\"\n",
    "    importance_results = {}\n",
    "    model_names = [name for name in loaded_models.keys() if name != SCALER_FILE]\n",
    "    \n",
    "    print(\"\\nAnalyzing feature importance for all models...\")\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        importance = get_feature_importance(model_name, loaded_models)\n",
    "        if importance is not None:\n",
    "            importance_results[model_name] = importance\n",
    "            print(f\"✓ {model_name}: Feature importance extracted\")\n",
    "        else:\n",
    "            print(f\"✗ {model_name}: No feature importance available\")\n",
    "    \n",
    "    return importance_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fdff01-a18a-4afe-a689-3ef374fbd51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# WHAT-IF ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_what_if(\n",
    "    base_features_dict: dict,\n",
    "    model_name: str,\n",
    "    loaded_models: Dict,\n",
    "    param_name: str,\n",
    "    param_values: List[float],\n",
    "    use_scaler: bool = True\n",
    ") -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Perform what-if analysis by varying one parameter and observing predictions.\n",
    "    \n",
    "    Args:\n",
    "        base_features_dict (dict): Base feature values as dictionary\n",
    "        model_name (str): Name of the model to use\n",
    "        loaded_models (Dict): Dictionary of loaded models\n",
    "        param_name (str): Name of the parameter to vary\n",
    "        param_values (List[float]): Values to use for the parameter\n",
    "        use_scaler (bool): Whether to apply scaling if available\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, List]: Dictionary with parameter values and predictions\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"param_values\": [],\n",
    "        \"predictions\": []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nPerforming what-if analysis for '{param_name}' with model '{model_name}'...\")\n",
    "    \n",
    "    for value in param_values:\n",
    "        try:\n",
    "            # Create a copy of features and modify the specified parameter\n",
    "            modified_features = base_features_dict.copy()\n",
    "            modified_features[param_name] = value\n",
    "            \n",
    "            # Make prediction\n",
    "            prediction = predict_with_single_model(\n",
    "                modified_features, model_name, loaded_models, use_scaler\n",
    "            )\n",
    "            \n",
    "            if prediction is not None:\n",
    "                results[\"param_values\"].append(value)\n",
    "                results[\"predictions\"].append(prediction)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in what-if analysis for value {value}: {str(e)}\")\n",
    "    \n",
    "    print(f\"What-if analysis completed: {len(results['predictions'])} data points\")\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e33b97-6f61-424b-bba6-93d1fd547360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8bd828-3448-42e1-95a9-8394e8e07e05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
