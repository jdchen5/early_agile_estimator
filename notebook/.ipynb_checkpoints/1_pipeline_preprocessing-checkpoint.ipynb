{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f790fa4-3694-4fc7-b8d0-b3a480e05053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df43052d-4e2f-497a-9f7d-7884f05342dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComplete Scikit-Learn Preprocessing Pipeline for ISBSG Data\\n===========================================================\\n\\nThis module provides a comprehensive preprocessing pipeline that handles:\\n1. Data loading and initial cleaning\\n2. Column name standardization\\n3. Missing value handling\\n4. Semicolon-separated value processing\\n5. One-hot encoding for categorical variables\\n6. Multi-label binarization for multi-value columns\\n7. Feature selection and filtering\\n8. Data validation and export\\n\\nBased on the preprocessing steps from the provided notebooks.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Scikit-Learn Preprocessing Pipeline for ISBSG Data\n",
    "===========================================================\n",
    "\n",
    "This module provides a comprehensive preprocessing pipeline that handles:\n",
    "1. Data loading and initial cleaning\n",
    "2. Column name standardization\n",
    "3. Missing value handling\n",
    "4. Semicolon-separated value processing\n",
    "5. One-hot encoding for categorical variables\n",
    "6. Multi-label binarization for multi-value columns\n",
    "7. Feature selection and filtering\n",
    "8. Data validation and export\n",
    "\n",
    "Based on the preprocessing steps from the provided notebooks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29e26088-9829-45c3-ad0e-f34c146cf120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "import os\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb3dd6fc-4a21-4b17-b2cc-9e1d160bfab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp printing activated.\n",
      "Cell executed at: 2025-06-02 16:14:37.931984\n"
     ]
    }
   ],
   "source": [
    "# Sets up an automatic timestamp printout after each Jupyter cell execution \n",
    "# and configures the default visualization style.\n",
    "from IPython import get_ipython\n",
    "\n",
    "def setup_timestamp_callback():\n",
    "    \"\"\"Setup a timestamp callback for Jupyter cells without clearing existing callbacks.\"\"\"\n",
    "    ip = get_ipython()\n",
    "    if ip is not None:\n",
    "        # Define timestamp function\n",
    "        def print_timestamp(*args, **kwargs):\n",
    "            \"\"\"Print timestamp after cell execution.\"\"\"\n",
    "            print(f\"Cell executed at: {datetime.now()}\")\n",
    "        \n",
    "        # Check if our callback is already registered\n",
    "        callbacks = ip.events.callbacks.get('post_run_cell', [])\n",
    "        for cb in callbacks:\n",
    "            if hasattr(cb, '__name__') and cb.__name__ == 'print_timestamp':\n",
    "                # Already registered\n",
    "                return\n",
    "                \n",
    "        # Register new callback if not already present\n",
    "        ip.events.register('post_run_cell', print_timestamp)\n",
    "        print(\"Timestamp printing activated.\")\n",
    "    else:\n",
    "        print(\"Not running in IPython/Jupyter environment.\")\n",
    "\n",
    "# Setup timestamp callback\n",
    "setup_timestamp_callback()\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e705a6-19d4-488a-a6cc-751224ebcccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-06-02 16:14:37.947790\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_FOLDER = \"../data\"\n",
    "SAMPLE_FILE = \"ISBSG2016R1_1_agile_dataset_only.xlsx\"\n",
    "FULL_FILE = \"ISBSG2016R1_1_full_dataset.xlsx\"\n",
    "TARGET_COL = \"project_prf_normalised_work_effort\"  # be careful about case sensitive\n",
    "EXCLUDE_FROM_CATEGORY_AUGMENT = ['project_prf_year_of_project']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9a0ee6-306c-49db-9f05-41e37171f310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-06-02 16:14:38.032588\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    As of 20250602:\n",
    "    Data preprocessing pipeline for machine learning using scikit-learn transformers: designed specifically for processing project effort estimation data with a target column like 'project_prf_normalised_work_effort':\n",
    "        - DataFrame validation\n",
    "        - Column standardization, missing value imputation, categorical encoding (including multi-value semicolon-separated columns)\n",
    "        - Data cleaning in a systematic way\n",
    "        - The main entry point is preprocess_dataframe() which applies all transformations and returns a clean, ML-ready dataset\n",
    "        - The pipeline uses one-hot encoding for both regular categorical variables and multi-label columns (semicolon-separated values).\n",
    "\"\"\"\n",
    "\n",
    "# === 1. DataFrameValidator: Validate input DataFrame and check target column ===\n",
    "class DataFrameValidator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Validate input DataFrame and perform initial checks:\n",
    "    - Ensures input is a DataFrame\n",
    "    - Validates target column exists with smart matching\n",
    "    - Stores original shape and target column info\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_col: str = 'project_prf_normalised_work_effort'):\n",
    "        self.target_col = target_col\n",
    "        self.original_shape = None\n",
    "        self.original_target_col = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        print(f\"{self.__class__.__name__}.fit() CALLED\")\n",
    "        return self\n",
    "    \n",
    "    def _standardize_column_name(self, col_name: str) -> str:\n",
    "        \"\"\"Convert column name to standardized format\"\"\"\n",
    "        return str(col_name).strip().lower().replace(' ', '_')\n",
    "    \n",
    "    def _find_target_column(self, df_columns) -> Union[str, Tuple[None, List[str]]]:\n",
    "        \"\"\"Smart target column finder - handles various formats\"\"\"\n",
    "        target_standardized = self._standardize_column_name(self.target_col)\n",
    "        \n",
    "        # Try exact match first\n",
    "        if self.target_col in df_columns:\n",
    "            return self.target_col\n",
    "            \n",
    "        # Try standardized versions of all columns\n",
    "        for col in df_columns:\n",
    "            col_standardized = self._standardize_column_name(col)\n",
    "            if col_standardized == target_standardized:\n",
    "                return col\n",
    "                \n",
    "        # If not found, look for partial matches\n",
    "        similar_cols = []\n",
    "        target_words = set(target_standardized.split('_'))\n",
    "        for col in df_columns:\n",
    "            col_words = set(self._standardize_column_name(col).split('_'))\n",
    "            if len(target_words.intersection(col_words)) >= 2:\n",
    "                similar_cols.append(col)\n",
    "                \n",
    "        return None, similar_cols\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Validate DataFrame and find target column\"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a pandas DataFrame\")\n",
    "        \n",
    "        df = X.copy()\n",
    "        self.original_shape = df.shape\n",
    "        print(f\"Processing DataFrame with shape: {df.shape}\")\n",
    "\n",
    "        # Standardize ALL object/categorical columns: lowercase and strip\n",
    "        for col in df.select_dtypes(include='object').columns:\n",
    "            df[col] = df[col].astype(str).str.lower().str.strip()\n",
    "\n",
    "        # Smart target column finding\n",
    "        result = self._find_target_column(df.columns)\n",
    "        \n",
    "        if isinstance(result, tuple):  # Not found\n",
    "            actual_col, similar_cols = result\n",
    "            error_msg = f\"Target column '{self.target_col}' not found in DataFrame.\"\n",
    "            if similar_cols:\n",
    "                error_msg += f\" Similar columns found: {similar_cols}\"\n",
    "            else:\n",
    "                error_msg += f\" Available columns: {list(df.columns)}\"\n",
    "            raise ValueError(error_msg)\n",
    "        else:\n",
    "            actual_col = result\n",
    "            \n",
    "        # Store the original column name we found\n",
    "        self.original_target_col = actual_col\n",
    "        \n",
    "        if actual_col != self.target_col:\n",
    "            print(f\"Target column found: '{actual_col}' -> will be standardized to '{self.target_col}'\")\n",
    "            \n",
    "        return df\n",
    "\n",
    "# === 2. ColumnNameStandardizer: Clean and standardize column names ===\n",
    "class ColumnNameStandardizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Standardize column names for consistency:\n",
    "    - Strips spaces, lowercases, replaces & with _&_, removes special chars\n",
    "    - Maintains mapping for reference\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_col: Optional[str] = None, original_target_col: Optional[str] = None):\n",
    "        self.column_mapping = {}\n",
    "        self.target_col = target_col\n",
    "        self.original_target_col = original_target_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        print(f\"{self.__class__.__name__}.fit() CALLED\")\n",
    "        return self\n",
    "    \n",
    "    def _standardize_columns(self, columns) -> List[str]:\n",
    "        \"\"\"Standardize column names\"\"\"\n",
    "        return [str(col).strip().lower().replace(' ', '_') for col in columns]\n",
    "    \n",
    "    def _clean_column_names(self, columns) -> List[str]:\n",
    "        \"\"\"Clean column names for compatibility\"\"\"\n",
    "        cleaned_cols = []\n",
    "        for col in columns:\n",
    "            # Replace ampersands with _&_ to match expected transformations\n",
    "            col_clean = str(col).replace(' & ', '_&_')\n",
    "            # Remove special characters except underscores and ampersands\n",
    "            col_clean = re.sub(r'[^\\w\\s&]', '', col_clean)\n",
    "            # Replace spaces with underscores\n",
    "            col_clean = col_clean.replace(' ', '_')\n",
    "            cleaned_cols.append(col_clean)\n",
    "        return cleaned_cols\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply column name standardization\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Store original column names\n",
    "        original_columns = df.columns.tolist()\n",
    "        \n",
    "        # Apply standardization\n",
    "        standardized_cols = self._standardize_columns(original_columns)\n",
    "        cleaned_cols = self._clean_column_names(standardized_cols)\n",
    "\n",
    "        # Special handling for target column\n",
    "        if self.original_target_col and self.target_col:\n",
    "            try:\n",
    "                target_index = original_columns.index(self.original_target_col)\n",
    "                cleaned_cols[target_index] = self.target_col\n",
    "                print(f\"Target column '{self.original_target_col}' -> '{self.target_col}'\")\n",
    "            except ValueError:\n",
    "                pass  # Original target col not found, proceed normally\n",
    "        \n",
    "        # Create mapping\n",
    "        self.column_mapping = dict(zip(original_columns, cleaned_cols))\n",
    "        \n",
    "        # Apply new column names\n",
    "        df.columns = cleaned_cols\n",
    "        \n",
    "        # Report changes\n",
    "        changed_cols = sum(1 for orig, new in self.column_mapping.items() if orig != new)\n",
    "        print(f\"Standardized {changed_cols} column names\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 3. CategoricalValueStandardizer: Apply standardization mapping to categorical values ===\n",
    "class CategoricalValueStandardizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Apply standardization mapping to categorical column values\"\"\"\n",
    "    \n",
    "    def __init__(self, mapping: Optional[Dict[str, str]] = None, columns: Optional[List[str]] = None):\n",
    "        self.mapping = mapping or {}\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(f\"{self.__class__.__name__}.fit() CALLED\")\n",
    "        print(f\"single-value columns are: {self.columns}\")\n",
    "        if self.columns is None:\n",
    "            possible_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "            single_value_cols = []\n",
    "            for col in possible_cols:\n",
    "                if not X[col].dropna().astype(str).str.contains(';').any():\n",
    "                    single_value_cols.append(col)\n",
    "            self.columns = single_value_cols\n",
    "            print(f\"CategoricalValueStandardizer[fit] Single-value categorical columns selected for mapping: {self.columns}\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #print(f\"Single-value categorical columns mapping: {self.mapping}\")\n",
    "        print(f\"[transform] Single-value Columns to be mapped: {self.columns}\")\n",
    "        df = X.copy()\n",
    "        for col in self.columns:\n",
    "            if col in df.columns:\n",
    "                #print(f\"[transform] Processing column: '{col}'\")\n",
    "                original_values = df[col].copy()  # assign before overwrite\n",
    "                #print(f\"  Unique values before mapping: {original_values.unique()[:10]}\")\n",
    "                df[col] = (\n",
    "                    df[col].astype(str)\n",
    "                    .str.strip()\n",
    "                    .str.lower()\n",
    "                    .map(lambda x: self.mapping.get(x, x))\n",
    "                )\n",
    "            # Print changes only (where original != new)\n",
    "            #print(f\"  Unique values after mapping: {df[col].unique()[:10]}\")\n",
    "            changed = original_values[original_values != df[col]]\n",
    "            #if not changed.empty:\n",
    "            #    print(f\"\\nColumn '{col}':\")\n",
    "            #    for idx in changed.index:\n",
    "            #        print(f\"  {original_values[idx]!r} -> {df[col][idx]!r}\")\n",
    "        return df\n",
    "\n",
    "# === 4. CategoricalValueCleaner: Clean categorical values ===\n",
    "class CategoricalValueCleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Clean categorical column values:\n",
    "    - Replace '-' with '_'\n",
    "    - Lowercase and strip whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        print(f\"{self.__class__.__name__}.fit() CALLED\")\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "        for col in cat_cols:\n",
    "            df[col] = (\n",
    "                df[col].astype(str)\n",
    "                .str.replace('-', '_')\n",
    "                .str.lower()\n",
    "                .str.strip()\n",
    "            )\n",
    "        return df\n",
    "\n",
    "# === 5. MissingValueAnalyzer: Analyze and handle missing values ===\n",
    "class MissingValueAnalyzer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Analyze and handle missing values:\n",
    "    - Reports missing value statistics\n",
    "    - Drops high-missing columns (except protected ones)\n",
    "    - Fills remaining missing values appropriately\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, high_missing_threshold: float = 0.7, cols_to_keep: Optional[List[str]] = None):\n",
    "        self.high_missing_threshold = high_missing_threshold\n",
    "        self.cols_to_keep = cols_to_keep or []\n",
    "        self.high_missing_cols = []\n",
    "        self.missing_stats = {}\n",
    "        self.fill_values = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Pre-calculate fill values for numeric columns\n",
    "        num_cols = X.select_dtypes(include=['number']).columns\n",
    "        self.fill_values = {col: X[col].median() for col in num_cols}\n",
    "        print(f\"{self.__class__.__name__}.fit() CALLED\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Analyze and handle missing values\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Calculate missing percentages\n",
    "        missing_pct = df.isnull().mean()\n",
    "        self.missing_stats = missing_pct.sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"\\nMissing value analysis:\")\n",
    "        print(f\"Columns with >50% missing: {sum(missing_pct > 0.5)}\")\n",
    "        print(f\"Columns with >70% missing: {sum(missing_pct > self.high_missing_threshold)}\")\n",
    "        \n",
    "        # Identify high missing columns\n",
    "        self.high_missing_cols = missing_pct[missing_pct > self.high_missing_threshold].index.tolist()\n",
    "        \n",
    "        # Filter out protected columns\n",
    "        cols_to_drop = [col for col in self.high_missing_cols if col not in self.cols_to_keep]\n",
    "        \n",
    "        if cols_to_drop:\n",
    "            print(f\"Dropping {len(cols_to_drop)} columns with >{self.high_missing_threshold*100}% missing values\")\n",
    "            print(f\"Columns to be dropped due to high missing values: {cols_to_drop}\")\n",
    "            df = df.drop(columns=cols_to_drop)\n",
    "        \n",
    "        # Fill missing values efficiently\n",
    "        # Categorical columns\n",
    "        cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "        df[cat_cols] = df[cat_cols].fillna('missing')\n",
    "        \n",
    "        # Numeric columns\n",
    "        for col in df.select_dtypes(include=['number']).columns:\n",
    "            if col in self.fill_values and df[col].isnull().any():\n",
    "                df[col] = df[col].fillna(self.fill_values[col])\n",
    "        \n",
    "        print(f\"Data shape after missing value handling: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "# === 6. SemicolonProcessor: Process multi-value columns ===\n",
    "class SemicolonProcessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Process semicolon-separated values in columns:\n",
    "    - Identifies columns with semicolons\n",
    "    - Cleans, deduplicates, and sorts values\n",
    "    - Applies standardization mapping where specified\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, standardization_mapping: Optional[Dict[str, str]] = None):\n",
    "        self.semicolon_cols = []\n",
    "        self.standardization_mapping = standardization_mapping or {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        print(f\"{self.__class__.__name__}.fit() CALLED\")\n",
    "        return self\n",
    "    \n",
    "    def _clean_and_sort_semicolon(self, val, apply_standardization: bool = False, \n",
    "                                 mapping: Optional[Dict[str, str]] = None) -> str:\n",
    "        \"\"\"Clean, deduplicate, sort, and standardize semicolon-separated values\"\"\"\n",
    "        if pd.isnull(val) or val == '':\n",
    "            return val\n",
    "        \n",
    "        parts = [x.strip().lower() for x in str(val).split(';') if x.strip()]\n",
    "        \n",
    "        if apply_standardization and mapping is not None:\n",
    "            parts = [mapping.get(part, part) for part in parts]\n",
    "        \n",
    "        unique_cleaned = sorted(set(parts))\n",
    "        return '; '.join(unique_cleaned)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Process semicolon-separated columns\"\"\"\n",
    "        df = X.copy()\n",
    "        # Identify columns with semicolons\n",
    "        self.semicolon_cols = [\n",
    "            col for col in df.columns\n",
    "            if df[col].dropna().astype(str).str.contains(';').any()\n",
    "        ]\n",
    "        print(f\"Found {len(self.semicolon_cols)} columns with semicolons: {self.semicolon_cols}\")\n",
    "        #print(f\"Semicolon mapping: {self.standardization_mapping}\")\n",
    "        # Process each semicolon column\n",
    "        for col in self.semicolon_cols:\n",
    "            apply_mapping = col in self.semicolon_cols\n",
    "            mapping = self.standardization_mapping if apply_mapping else None\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: self._clean_and_sort_semicolon(\n",
    "                    x, apply_standardization=apply_mapping, mapping=mapping\n",
    "                )\n",
    "            )\n",
    "        return df\n",
    "\n",
    "# === 7. MultiValueEncoder: Encode semicolon columns using MultiLabelBinarizer ===\n",
    "class MultiValueEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Handle multi-value columns using MultiLabelBinarizer:\n",
    "    - Only processes columns with manageable cardinality\n",
    "    - Creates binary columns for each unique value\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_cardinality: int = 10):\n",
    "        self.max_cardinality = max_cardinality\n",
    "        self.multi_value_cols = []\n",
    "        self.mlb_transformers = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Encode multi-value columns\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Identify semicolon columns\n",
    "        semicolon_cols = [\n",
    "            col for col in df.columns\n",
    "            if df[col].dropna().astype(str).str.contains(';').any()\n",
    "        ]\n",
    "        \n",
    "        # Filter for low cardinality multi-value columns\n",
    "        self.multi_value_cols = []\n",
    "        for col in semicolon_cols:\n",
    "            # Get unique values across all entries\n",
    "            all_values = set()\n",
    "            for val in df[col].dropna().astype(str):\n",
    "                values = [v.strip() for v in val.split(';') if v.strip()]\n",
    "                all_values.update(values)\n",
    "            \n",
    "            if len(all_values) <= self.max_cardinality:\n",
    "                self.multi_value_cols.append(col)\n",
    "        \n",
    "        print(f\"Encoding {len(self.multi_value_cols)} multi-value columns: {self.multi_value_cols}\")\n",
    "        \n",
    "        # Process each multi-value column\n",
    "        for col in self.multi_value_cols:\n",
    "            # Prepare data for MultiLabelBinarizer\n",
    "            values_list = []\n",
    "            for idx in df.index:\n",
    "                val = df.loc[idx, col]\n",
    "                if pd.notna(val) and str(val).strip():\n",
    "                    values_list.append([item.strip() for item in str(val).split(';') if item.strip()])\n",
    "                else:\n",
    "                    values_list.append([])\n",
    "            \n",
    "            if not any(values_list):  # Skip if no valid values\n",
    "                continue\n",
    "                \n",
    "            # Fit and transform\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            onehot_array = mlb.fit_transform(values_list)\n",
    "            \n",
    "            # Create DataFrame with proper column names\n",
    "            onehot_df = pd.DataFrame(\n",
    "                onehot_array,\n",
    "                columns=[f\"{col}__{cat}\" for cat in mlb.classes_],\n",
    "                index=df.index\n",
    "            )\n",
    "            \n",
    "            # Store transformer\n",
    "            self.mlb_transformers[col] = mlb\n",
    "            \n",
    "            # Check for column conflicts and resolve\n",
    "            overlap = df.columns.intersection(onehot_df.columns)\n",
    "            if not overlap.empty:\n",
    "                print(f\"Resolving column conflicts for {col}: {list(overlap)}\")\n",
    "                onehot_df = onehot_df.drop(columns=overlap)\n",
    "            \n",
    "            # Join with main dataframe\n",
    "            df = pd.concat([df, onehot_df], axis=1)\n",
    "            \n",
    "            print(f\"Encoded {col} into {len(mlb.classes_)} binary columns\")\n",
    "        \n",
    "        # Remove original multi-value columns\n",
    "        df = df.drop(columns=self.multi_value_cols)\n",
    "\n",
    "        print(\"Columns after multi-value encoding:\")\n",
    "        print(df.columns.tolist())\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 8. CategoricalEncoder: One-hot encode regular categorical columns ===\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Handle single-value categorical columns:\n",
    "    - Excludes semicolon columns\n",
    "    - Only encodes low-cardinality columns\n",
    "    - Optionally drops first category to avoid multicollinearity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_cardinality: int = 10, drop_first: bool = False):\n",
    "        self.max_cardinality = max_cardinality\n",
    "        self.drop_first = drop_first\n",
    "        self.categorical_cols = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        print(f\"{self.__class__.__name__}.fit() CALLED\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Encode categorical columns\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Identify categorical columns\n",
    "        cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        print(f\"Found all categorical columns for single-valued encoding: {cat_cols}\")\n",
    "        \n",
    "        # Exclude semicolon(multi-value) columns \n",
    "        semicolon_cols = [\n",
    "            col for col in df.columns\n",
    "            if df[col].dropna().astype(str).str.contains(';').any()\n",
    "        ]\n",
    "        #print(f\"Found multi-value columns are: {semicolon_cols}\")\n",
    "        \n",
    "        # Filter for low cardinality single-value categorical columns\n",
    "        self.categorical_cols = [\n",
    "            col for col in cat_cols \n",
    "            if col not in semicolon_cols and df[col].nunique() <= self.max_cardinality\n",
    "        ]\n",
    "        \n",
    "        print(f\"One-hot encoding for single value categorical data{len(self.categorical_cols)} categorical columns: {self.categorical_cols}\")\n",
    "\n",
    "        # Apply one-hot encoding\n",
    "        if self.categorical_cols:\n",
    "            df = pd.get_dummies(df, columns=self.categorical_cols, drop_first=self.drop_first)\n",
    "\n",
    "        #print(\"****Columns after one-hot encoding:\")\n",
    "        #print(df.columns.tolist())\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 9. ColumnNameFixer: Final column name cleanup ===\n",
    "class ColumnNameFixer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Fix column names for compatibility:\n",
    "    - Removes illegal characters\n",
    "    - Handles duplicates\n",
    "    - Ensures clean, consistent naming\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.column_transformations = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        print(f\"{self.__class__.__name__}.fit() CALLED\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Fix problematic column names\"\"\"\n",
    "        df = X.copy()\n",
    "        original_cols = df.columns.tolist()\n",
    "        fixed_columns = []\n",
    "        seen_columns = set()\n",
    "        \n",
    "        for col in original_cols:\n",
    "            # Clean column name\n",
    "            fixed_col = str(col).replace(' ', '_').replace('&', 'and')\n",
    "            fixed_col = ''.join(c if c.isalnum() or c == '_' else '_' for c in fixed_col)\n",
    "            fixed_col = re.sub('_+', '_', fixed_col).strip('_')\n",
    "            \n",
    "            # Handle duplicates\n",
    "            base_col = fixed_col\n",
    "            suffix = 1\n",
    "            while fixed_col in seen_columns:\n",
    "                fixed_col = f\"{base_col}_{suffix}\"\n",
    "                suffix += 1\n",
    "            \n",
    "            seen_columns.add(fixed_col)\n",
    "            fixed_columns.append(fixed_col)\n",
    "        \n",
    "        # Store transformations\n",
    "        self.column_transformations = dict(zip(original_cols, fixed_columns))\n",
    "        df.columns = fixed_columns\n",
    "        \n",
    "        n_changed = sum(1 for old, new in self.column_transformations.items() if old != new)\n",
    "        print(f\"Fixed {n_changed} column names for compatibility\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === 10. DataValidator: Final validation and summary ===\n",
    "class DataValidator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Validate final dataset:\n",
    "    - Check shape, missing values, data types\n",
    "    - Provide target variable summary\n",
    "    - Report any issues\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_col: str):\n",
    "        self.target_col = target_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Validate the processed dataset\"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        print(f\"\\n=== Final Data Validation ===\")\n",
    "        print(f\"Final shape: {df.shape}\")\n",
    "        print(f\"Target column: {self.target_col}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_count = df.isnull().sum().sum()\n",
    "        print(f\"Total missing values: {missing_count}\")\n",
    "        \n",
    "        # Check for infinite values\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            inf_count = np.isinf(df[numeric_cols].values).sum()\n",
    "            print(f\"Total infinite values: {inf_count}\")\n",
    "        \n",
    "        # Data types summary\n",
    "        print(f\"\\nData types:\")\n",
    "        print(f\"  Numeric columns: {len(df.select_dtypes(include=[np.number]).columns)}\")\n",
    "        print(f\"  Categorical columns: {len(df.select_dtypes(include=['object', 'category']).columns)}\")\n",
    "        \n",
    "        # Target variable summary\n",
    "        if self.target_col in df.columns:\n",
    "            target_stats = df[self.target_col].describe()\n",
    "            print(f\"\\nTarget variable '{self.target_col}' statistics:\")\n",
    "            print(f\"  Count: {target_stats['count']}\")\n",
    "            print(f\"  Mean: {target_stats['mean']:.2f}\")\n",
    "            print(f\"  Std: {target_stats['std']:.2f}\")\n",
    "            print(f\"  Min: {target_stats['min']:.2f}\")\n",
    "            print(f\"  Max: {target_stats['max']:.2f}\")\n",
    "            print(f\"  Missing: {df[self.target_col].isnull().sum()}\")\n",
    "        else:\n",
    "            print(f\"WARNING: Target column '{self.target_col}' not found!\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# === Pipeline creation function ===\n",
    "def create_preprocessing_pipeline(\n",
    "    target_col: str = 'project_prf_normalised_work_effort',\n",
    "    high_missing_threshold: float = 0.7,\n",
    "    cols_to_keep: Optional[List[str]] = None,\n",
    "    max_categorical_cardinality: int = 10,\n",
    "    standardization_mapping: Optional[Dict[str, str]] = None\n",
    ") -> Pipeline:\n",
    "    \"\"\"\n",
    "    Create complete preprocessing pipeline for DataFrame input\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    target_col : str\n",
    "        Name of target column\n",
    "    high_missing_threshold : float\n",
    "        Threshold for dropping columns with high missing values\n",
    "    cols_to_keep : list\n",
    "        Columns to keep even if they have high missing values\n",
    "    max_categorical_cardinality : int\n",
    "        Maximum number of unique values for categorical encoding\n",
    "    standardization_mapping : dict\n",
    "        Custom mapping for standardizing values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Pipeline\n",
    "        Complete preprocessing pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    if cols_to_keep is None:\n",
    "        cols_to_keep = [\n",
    "            'project_prf_case_tool_used', \n",
    "            'process_pmf_prototyping_used',\n",
    "            'tech_tf_client_roles', \n",
    "            'tech_tf_type_of_server', \n",
    "            'tech_tf_clientserver_description'\n",
    "        ]\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('validator', DataFrameValidator(target_col)),\n",
    "        ('column_standardizer', ColumnNameStandardizer()),\n",
    "        ('missing_handler', MissingValueAnalyzer(\n",
    "            high_missing_threshold=high_missing_threshold,\n",
    "            cols_to_keep=cols_to_keep\n",
    "        )),\n",
    "        ('cat_value_cleaner', CategoricalValueCleaner()),\n",
    "        ('semicolon_processor', SemicolonProcessor(standardization_mapping=standardization_mapping)),\n",
    "        ('cat_value_standardizer', CategoricalValueStandardizer(\n",
    "            mapping=standardization_mapping,\n",
    "            columns=None\n",
    "        )),\n",
    "        ('multi_value_encoder', MultiValueEncoder(max_cardinality=max_categorical_cardinality)),\n",
    "        ('categorical_encoder', CategoricalEncoder(max_cardinality=max_categorical_cardinality)),\n",
    "        ('column_fixer', ColumnNameFixer()),\n",
    "        ('final_validator', DataValidator(target_col))\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# === Simplified preprocessing function ===\n",
    "def preprocess_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = 'project_prf_normalised_work_effort',\n",
    "    **pipeline_kwargs\n",
    ") -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Preprocess a DataFrame using the complete pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame to preprocess\n",
    "    target_col : str\n",
    "        Name of target column\n",
    "    **pipeline_kwargs : dict\n",
    "        Additional arguments for pipeline creation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Processed DataFrame ready for modeling\n",
    "    dict\n",
    "        Processing metadata and statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"DataFrame Preprocessing Pipeline\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Input shape: {df.shape}\")\n",
    "    print(f\"Target column: {target_col}\")\n",
    "    print(f\"Timestamp: {datetime.now()}\")\n",
    "    \n",
    "    # Create and apply pipeline\n",
    "    pipeline = create_preprocessing_pipeline(target_col=target_col, **pipeline_kwargs)\n",
    "    #print(f\"Pipeline is: {pipeline}\")\n",
    "    \n",
    "    # Get original target column info from validator\n",
    "    validator = pipeline.named_steps['validator']\n",
    "    df_processed = pipeline.fit_transform(df)\n",
    "    \n",
    "    # Prepare metadata\n",
    "    metadata = {\n",
    "        'original_shape': df.shape,\n",
    "        'processed_shape': df_processed.shape,\n",
    "        'processing_timestamp': datetime.now().isoformat(),\n",
    "        'target_column_standardized': target_col,\n",
    "        'target_column_original': validator.original_target_col,\n",
    "        'pipeline_steps': [step[0] for step in pipeline.steps]\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Preprocessing completed successfully!\")\n",
    "    print(f\"Shape: {df.shape} -> {df_processed.shape}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return df_processed, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca7a5996-1dc9-42cb-aca9-c52df15b3a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-06-02 16:14:38.052775\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "# As of 20250206\n",
    "\n",
    "def integrated_categorical_preprocessing(\n",
    "    sample_df: pd.DataFrame,\n",
    "    full_df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    cols_to_keep: List[str] = None,\n",
    "    high_card_columns: List[str] = None,\n",
    "    max_categorical_cardinality: int = 10,\n",
    "    samples_per_category: int = 3,\n",
    "    standardization_mapping: Dict[str, str] = None,\n",
    "    high_missing_threshold: float = 0.7,\n",
    "    separator: str = ';',\n",
    "    strategy: str = 'top_k',\n",
    "    k: int = 20,\n",
    "    exclude_from_augment: List[str] = None\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Efficient in-memory categorical preprocessing and augmentation pipeline.\n",
    "    Args:\n",
    "        sample_df, full_df: DataFrames\n",
    "        target_col: target column\n",
    "        cols_to_keep: columns to retain (optional)\n",
    "        high_card_columns: list of high-cardinality categorical columns\n",
    "        ... [other params as above]\n",
    "    Returns:\n",
    "        enhanced_sample: DataFrame\n",
    "        metadata: dict\n",
    "    \"\"\"\n",
    "    # Step 0: (Optional) Column Filtering\n",
    "    #if cols_to_keep:\n",
    "    #    sample_df = sample_df[cols_to_keep + [target_col]].copy()\n",
    "    #    full_df = full_df[cols_to_keep + [target_col]].copy()\n",
    "\n",
    "    # Step 1: Standardize/clean categorical columns (optional, minimal here)\n",
    "    if standardization_mapping:\n",
    "        for col, mapped_col in standardization_mapping.items():\n",
    "            if col in sample_df and mapped_col in sample_df:\n",
    "                sample_df[col] = sample_df[mapped_col]\n",
    "            if col in full_df and mapped_col in full_df:\n",
    "                full_df[col] = full_df[mapped_col]\n",
    "\n",
    "    # Step 2: Detect categorical columns (vectorized, robust)\n",
    "    # Only include columns with \"object\" or \"category\" dtype and limited cardinality\n",
    "    potential_cats = [\n",
    "        col for col in sample_df.columns\n",
    "        if col != target_col and (\n",
    "            sample_df[col].dtype == \"object\"\n",
    "            or pd.api.types.is_categorical_dtype(sample_df[col])\n",
    "        )\n",
    "    ]\n",
    "    categorical_columns = [\n",
    "        col for col in potential_cats\n",
    "        if sample_df[col].nunique(dropna=True) <= max_categorical_cardinality\n",
    "    ]\n",
    "\n",
    "    # Step 3: Handle high-cardinality multi-value columns, both in full/sample\n",
    "    if high_card_columns is None:\n",
    "        high_card_columns = []\n",
    "\n",
    "    col_mapping = {}\n",
    "    for df in [full_df, sample_df]:\n",
    "        for col in high_card_columns:\n",
    "            if col in df.columns:\n",
    "                # Fast top-k binarization of multi-value column\n",
    "                all_vals = df[col].dropna().astype(str).str.split(separator).explode().str.strip()\n",
    "                top_values = [v for v, c in Counter(all_vals).most_common(k)]\n",
    "                for v in top_values:\n",
    "                    df[f\"{col}_top_{v}\"] = df[col].fillna(\"\").astype(str).apply(lambda x: int(v in [e.strip() for e in x.split(separator)]))\n",
    "                # Add 'other' bin\n",
    "                df[f\"{col}_other\"] = df[col].fillna(\"\").astype(str).apply(\n",
    "                    lambda x: int(any(e.strip() not in top_values for e in x.split(separator) if e.strip()))\n",
    "                )\n",
    "                col_mapping[col] = [f\"{col}_top_{v}\" for v in top_values] + [f\"{col}_other\"]\n",
    "                # Remove original\n",
    "                df.drop(columns=[col], inplace=True)\n",
    "    # Update list of categorical columns after processing\n",
    "    updated_cats = [\n",
    "        col for col in sample_df.columns\n",
    "        if (sample_df[col].dtype == \"object\" or pd.api.types.is_categorical_dtype(sample_df[col]))\n",
    "        and sample_df[col].nunique(dropna=True) <= max_categorical_cardinality\n",
    "        and col != target_col\n",
    "    ]\n",
    "    # Remove excluded columns (if any)\n",
    "    final_cats = [col for col in updated_cats if not (exclude_from_augment and col in exclude_from_augment)]\n",
    "\n",
    "    # Step 4: Augment sample_df with missing categories found only in full_df\n",
    "    # For each categorical column, sample a few rows from full_df for each missing category\n",
    "    additional_rows = []\n",
    "    missing_report = {}\n",
    "    for col in final_cats:\n",
    "        full_cats = set(full_df[col].dropna().unique())\n",
    "        sample_cats = set(sample_df[col].dropna().unique())\n",
    "        missing = full_cats - sample_cats\n",
    "        missing_report[col] = list(missing)\n",
    "        for val in missing:\n",
    "            matches = full_df[full_df[col] == val]\n",
    "            if not matches.empty:\n",
    "                additional = matches.sample(\n",
    "                    n=min(samples_per_category, len(matches)),\n",
    "                    random_state=42\n",
    "                )\n",
    "                additional_rows.append(additional)\n",
    "    # Combine new rows with original sample\n",
    "    if additional_rows:\n",
    "        df_aug = pd.concat([sample_df] + additional_rows, ignore_index=True).drop_duplicates()\n",
    "    else:\n",
    "        df_aug = sample_df.copy()\n",
    "\n",
    "    # Step 5: Clean up duplicate columns\n",
    "    df_aug = df_aug.loc[:, ~df_aug.columns.duplicated()]\n",
    "\n",
    "    # Step 6: Compile metadata\n",
    "    metadata = {\n",
    "        \"original_sample_shape\": sample_df.shape,\n",
    "        \"original_full_shape\": full_df.shape,\n",
    "        \"final_shape\": df_aug.shape,\n",
    "        \"categorical_columns_detected\": categorical_columns,\n",
    "        \"updated_categorical_columns\": updated_cats,\n",
    "        \"final_augmented_categorical_columns\": final_cats,\n",
    "        \"high_cardinality_columns_processed\": high_card_columns,\n",
    "        \"column_mapping\": col_mapping,\n",
    "        \"missing_categories_report\": missing_report,\n",
    "        \"rows_added_from_full_dataset\": df_aug.shape[0] - sample_df.shape[0]\n",
    "    }\n",
    "\n",
    "    return df_aug, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfcb48d-030b-4b1b-bfaf-4e4c47fd53b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20766b9d-a7d7-45f7-868d-029600fe4a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-06-02 16:14:38.068806\n"
     ]
    }
   ],
   "source": [
    "# Main execution function\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the integrated pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    sample_file_path = os.path.join(DATA_FOLDER, SAMPLE_FILE)\n",
    "    full_file_path = os.path.join(DATA_FOLDER, FULL_FILE)\n",
    "\n",
    "    sample_df = pd.read_excel(sample_file_path)\n",
    "    full_df = pd.read_excel(full_file_path)\n",
    "    \n",
    "    # Columns to keep (customize as needed)\n",
    "    cols_to_keep = [\n",
    "        'project_prf_case_tool_used', \n",
    "        'process_pmf_prototyping_used',\n",
    "        'tech_tf_client_roles', \n",
    "        'tech_tf_type_of_server', \n",
    "        'tech_tf_clientserver_description',\n",
    "        'people_prf_project_user_involvement'\n",
    "    ]\n",
    "    \n",
    "    # High-cardinality multi-value columns\n",
    "    high_card_columns = [\n",
    "        'external_eef_organisation_type', \n",
    "        'project_prf_application_type'\n",
    "    ]\n",
    "\n",
    "   \n",
    "    # Standardization rules\n",
    "    standardization_map = {\n",
    "        # Programming languages\n",
    "        '.net': 'dotnet',\n",
    "        'c': 'c_lang',         # or simply 'c'\n",
    "        'c++': 'cpp',\n",
    "        'c#': 'csharp',\n",
    "        # Architecture\n",
    "        'stand alone': 'standalone',\n",
    "        'stand-alone': 'standalone',\n",
    "        'client server': 'client-server',\n",
    "        # Application group\n",
    "        'mathematically_intensive application': 'mathematically_intensive_application',\n",
    "        # Web development\n",
    "        \"Web?\": \"web\",\n",
    "        # Server roles\n",
    "        \"file &/or print server\": \"file/print server\",\n",
    "        # Add others as needed\n",
    "    }\n",
    "    \n",
    "\n",
    "    try:\n",
    "        # Create and apply pipeline for sample\n",
    "        processed_sample_df, sample_metadata = preprocess_dataframe(\n",
    "            sample_df, \n",
    "            target_col='project_prf_normalised_work_effort',\n",
    "            cols_to_keep=cols_to_keep,\n",
    "            max_categorical_cardinality=10,\n",
    "            standardization_mapping=standardization_map,\n",
    "            high_missing_threshold=0.7\n",
    "        )\n",
    "        \n",
    "        # Create and apply pipeline for full dataset\n",
    "        processed_full_df, full_metadata = preprocess_dataframe(\n",
    "            full_df, \n",
    "            target_col='project_prf_normalised_work_effort',\n",
    "            cols_to_keep=cols_to_keep,\n",
    "            max_categorical_cardinality=10,\n",
    "            standardization_mapping=standardization_map,\n",
    "            high_missing_threshold=0.7\n",
    "        )\n",
    "\n",
    "        # adding back missing categorical values to the sample dataset\n",
    "        final_df, meta = integrated_categorical_preprocessing(\n",
    "            sample_df=processed_sample_df,\n",
    "            full_df=processed_full_df,\n",
    "            target_col=TARGET_COL,\n",
    "            cols_to_keep=cols_to_keep,\n",
    "            high_card_columns=high_card_columns,\n",
    "            max_categorical_cardinality=10,\n",
    "            samples_per_category=3,\n",
    "            standardization_mapping=standardization_map,\n",
    "            high_missing_threshold=0.7,\n",
    "            separator=';',\n",
    "            strategy='top_k',\n",
    "            k=20\n",
    "        )\n",
    "\n",
    "       \n",
    "        # Save results\n",
    "        output_path = os.path.join(DATA_FOLDER, 'enhanced_sample_final.csv')\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Final dataset saved to: {output_path}\")\n",
    "        print(f\"Final shape: {final_df.shape}\")\n",
    "        print(f\"Final meta: {meta}\")\n",
    "        print(f\"Ready for PyCaret setup!\")\n",
    "        \n",
    "        # Print summary of changes\n",
    "        print(f\"\\nSUMMARY:\")\n",
    "        #print(f\"- Original sample rows: {metadata['original_sample_shape'][0]}\")\n",
    "        #print(f\"- Rows added from full dataset: {metadata['rows_added_from_full_dataset']}\")\n",
    "        #print(f\"- Final rows: {metadata['final_shape'][0]}\")\n",
    "        #print(f\"- Original columns: {metadata['original_sample_shape'][1]}\")\n",
    "        #print(f\"- Final columns: {metadata['final_shape'][1]}\")\n",
    "        \n",
    "        return final_df, meta\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in integrated pipeline: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d347595d-4629-4d6a-96ef-d39327fc4a12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DataFrame Preprocessing Pipeline\n",
      "============================================================\n",
      "Input shape: (3786, 52)\n",
      "Target column: project_prf_normalised_work_effort\n",
      "Timestamp: 2025-06-02 16:14:41.629858\n",
      "DataFrameValidator.fit() CALLED\n",
      "Processing DataFrame with shape: (3786, 52)\n",
      "Target column found: 'Project_PRF_Normalised Work Effort' -> will be standardized to 'project_prf_normalised_work_effort'\n",
      "ColumnNameStandardizer.fit() CALLED\n",
      "Standardized 52 column names\n",
      "MissingValueAnalyzer.fit() CALLED\n",
      "\n",
      "Missing value analysis:\n",
      "Columns with >50% missing: 16\n",
      "Columns with >70% missing: 15\n",
      "Dropping 15 columns with >70.0% missing values\n",
      "Columns to be dropped due to high missing values: ['project_prf_defect_density', 'project_prf_manpower_delivery_rate', 'people_prf_ba_team_experience_less_than_1_yr', 'people_prf_ba_team_experience_1_to_3_yr', 'people_prf_ba_team_experience_great_than_3_yr', 'people_prf_it_experience_less_than_1_yr', 'people_prf_it_experience_1_to_3_yr', 'people_prf_it_experience_great_than_3_yr', 'people_prf_it_experience_less_than_3_yr', 'people_prf_it_experience_3_to_9_yr', 'people_prf_it_experience_great_than_9_yr', 'people_prf_project_manage_experience', 'people_prf_project_manage_changes', 'people_prf_personnel_changes', 'project_prf_total_project_cost']\n",
      "Data shape after missing value handling: (3786, 37)\n",
      "CategoricalValueCleaner.fit() CALLED\n",
      "SemicolonProcessor.fit() CALLED\n",
      "Found 8 columns with semicolons: ['external_eef_organisation_type', 'project_prf_application_group', 'project_prf_application_type', 'process_pmf_development_methodologies', 'tech_tf_client_roles', 'tech_tf_server_roles', 'tech_tf_type_of_server', 'tech_tf_clientserver_description']\n",
      "CategoricalValueStandardizer.fit() CALLED\n",
      "single-value columns are: None\n",
      "CategoricalValueStandardizer[fit] Single-value categorical columns selected for mapping: ['external_eef_data_quality_rating', 'external_eef_industry_sector', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'tech_tf_primary_programming_language', 'project_prf_relative_size', 'project_prf_team_size_group', 'project_prf_case_tool_used', 'process_pmf_prototyping_used', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_type_of_server', 'tech_tf_web_development', 'tech_tf_dbms_used', 'people_prf_project_user_involvement', 'project_prf_cost_currency', 'project_prf_currency_multiple']\n",
      "[transform] Single-value Columns to be mapped: ['external_eef_data_quality_rating', 'external_eef_industry_sector', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'tech_tf_primary_programming_language', 'project_prf_relative_size', 'project_prf_team_size_group', 'project_prf_case_tool_used', 'process_pmf_prototyping_used', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_type_of_server', 'tech_tf_web_development', 'tech_tf_dbms_used', 'people_prf_project_user_involvement', 'project_prf_cost_currency', 'project_prf_currency_multiple']\n",
      "Encoding 2 multi-value columns: ['project_prf_application_group', 'tech_tf_clientserver_description']\n",
      "Encoded project_prf_application_group into 6 binary columns\n",
      "Encoded tech_tf_clientserver_description into 10 binary columns\n",
      "Columns after multi-value encoding:\n",
      "['isbsg_project_id', 'external_eef_data_quality_rating', 'project_prf_year_of_project', 'external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_type', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'tech_tf_primary_programming_language', 'project_prf_functional_size', 'project_prf_relative_size', 'project_prf_normalised_work_effort_level_1', 'project_prf_normalised_work_effort', 'project_prf_normalised_level_1_pdr_ufp', 'project_prf_normalised_pdr_ufp', 'project_prf_speed_of_delivery', 'project_prf_project_elapsed_time', 'project_prf_team_size_group', 'project_prf_max_team_size', 'project_prf_case_tool_used', 'process_pmf_development_methodologies', 'process_pmf_prototyping_used', 'process_pmf_docs', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_client_roles', 'tech_tf_server_roles', 'tech_tf_type_of_server', 'tech_tf_web_development', 'tech_tf_dbms_used', 'tech_tf_tools_used', 'people_prf_project_user_involvement', 'project_prf_cost_currency', 'project_prf_currency_multiple', 'project_prf_application_group__business application', 'project_prf_application_group__infrastructure software', 'project_prf_application_group__mathematically intensive application', 'project_prf_application_group__mathematically_intensive_application', 'project_prf_application_group__nan', 'project_prf_application_group__real_time application', 'tech_tf_clientserver_description__browser_server architecture', 'tech_tf_clientserver_description__client-server', 'tech_tf_clientserver_description__client: presentation', 'tech_tf_clientserver_description__client: presentation, processing', 'tech_tf_clientserver_description__client_server architecture', 'tech_tf_clientserver_description__client_server architecture/p2p', 'tech_tf_clientserver_description__nan', 'tech_tf_clientserver_description__server: processing', 'tech_tf_clientserver_description__stand_alone', 'tech_tf_clientserver_description__web']\n",
      "CategoricalEncoder.fit() CALLED\n",
      "Found all categorical columns for single-valued encoding: ['external_eef_data_quality_rating', 'external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_type', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'tech_tf_primary_programming_language', 'project_prf_relative_size', 'project_prf_team_size_group', 'project_prf_case_tool_used', 'process_pmf_development_methodologies', 'process_pmf_prototyping_used', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_client_roles', 'tech_tf_server_roles', 'tech_tf_type_of_server', 'tech_tf_web_development', 'tech_tf_dbms_used', 'people_prf_project_user_involvement', 'project_prf_cost_currency', 'project_prf_currency_multiple']\n",
      "One-hot encoding for single value categorical data14 categorical columns: ['external_eef_data_quality_rating', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'project_prf_relative_size', 'project_prf_case_tool_used', 'process_pmf_prototyping_used', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_type_of_server', 'tech_tf_web_development', 'tech_tf_dbms_used', 'people_prf_project_user_involvement', 'project_prf_currency_multiple']\n",
      "ColumnNameFixer.fit() CALLED\n",
      "Fixed 29 column names for compatibility\n",
      "\n",
      "=== Final Data Validation ===\n",
      "Final shape: (3786, 110)\n",
      "Target column: project_prf_normalised_work_effort\n",
      "Total missing values: 0\n",
      "Total infinite values: 0\n",
      "\n",
      "Data types:\n",
      "  Numeric columns: 28\n",
      "  Categorical columns: 9\n",
      "\n",
      "Target variable 'project_prf_normalised_work_effort' statistics:\n",
      "  Count: 3786.0\n",
      "  Mean: 3920.57\n",
      "  Std: 9280.23\n",
      "  Min: 2.00\n",
      "  Max: 266946.00\n",
      "  Missing: 0\n",
      "\n",
      "============================================================\n",
      "Preprocessing completed successfully!\n",
      "Shape: (3786, 52) -> (3786, 110)\n",
      "============================================================\n",
      "============================================================\n",
      "DataFrame Preprocessing Pipeline\n",
      "============================================================\n",
      "Input shape: (7518, 52)\n",
      "Target column: project_prf_normalised_work_effort\n",
      "Timestamp: 2025-06-02 16:14:42.438796\n",
      "DataFrameValidator.fit() CALLED\n",
      "Processing DataFrame with shape: (7518, 52)\n",
      "Target column found: 'Project_PRF_Normalised Work Effort' -> will be standardized to 'project_prf_normalised_work_effort'\n",
      "ColumnNameStandardizer.fit() CALLED\n",
      "Standardized 52 column names\n",
      "MissingValueAnalyzer.fit() CALLED\n",
      "\n",
      "Missing value analysis:\n",
      "Columns with >50% missing: 16\n",
      "Columns with >70% missing: 15\n",
      "Dropping 15 columns with >70.0% missing values\n",
      "Columns to be dropped due to high missing values: ['project_prf_defect_density', 'project_prf_manpower_delivery_rate', 'people_prf_ba_team_experience_less_than_1_yr', 'people_prf_ba_team_experience_1_to_3_yr', 'people_prf_ba_team_experience_great_than_3_yr', 'people_prf_it_experience_less_than_1_yr', 'people_prf_it_experience_1_to_3_yr', 'people_prf_it_experience_great_than_3_yr', 'people_prf_it_experience_less_than_3_yr', 'people_prf_it_experience_3_to_9_yr', 'people_prf_it_experience_great_than_9_yr', 'people_prf_project_manage_experience', 'people_prf_project_manage_changes', 'people_prf_personnel_changes', 'project_prf_total_project_cost']\n",
      "Data shape after missing value handling: (7518, 37)\n",
      "CategoricalValueCleaner.fit() CALLED\n",
      "SemicolonProcessor.fit() CALLED\n",
      "Found 8 columns with semicolons: ['external_eef_organisation_type', 'project_prf_application_group', 'project_prf_application_type', 'process_pmf_development_methodologies', 'tech_tf_client_roles', 'tech_tf_server_roles', 'tech_tf_type_of_server', 'tech_tf_clientserver_description']\n",
      "CategoricalValueStandardizer.fit() CALLED\n",
      "single-value columns are: None\n",
      "CategoricalValueStandardizer[fit] Single-value categorical columns selected for mapping: ['external_eef_data_quality_rating', 'external_eef_industry_sector', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'tech_tf_primary_programming_language', 'project_prf_relative_size', 'project_prf_team_size_group', 'project_prf_case_tool_used', 'process_pmf_prototyping_used', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_type_of_server', 'tech_tf_web_development', 'tech_tf_dbms_used', 'people_prf_project_user_involvement', 'project_prf_cost_currency', 'project_prf_currency_multiple']\n",
      "[transform] Single-value Columns to be mapped: ['external_eef_data_quality_rating', 'external_eef_industry_sector', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'tech_tf_primary_programming_language', 'project_prf_relative_size', 'project_prf_team_size_group', 'project_prf_case_tool_used', 'process_pmf_prototyping_used', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_type_of_server', 'tech_tf_web_development', 'tech_tf_dbms_used', 'people_prf_project_user_involvement', 'project_prf_cost_currency', 'project_prf_currency_multiple']\n",
      "Encoding 1 multi-value columns: ['project_prf_application_group']\n",
      "Encoded project_prf_application_group into 6 binary columns\n",
      "Columns after multi-value encoding:\n",
      "['isbsg_project_id', 'external_eef_data_quality_rating', 'project_prf_year_of_project', 'external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_type', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'tech_tf_primary_programming_language', 'project_prf_functional_size', 'project_prf_relative_size', 'project_prf_normalised_work_effort_level_1', 'project_prf_normalised_work_effort', 'project_prf_normalised_level_1_pdr_ufp', 'project_prf_normalised_pdr_ufp', 'project_prf_speed_of_delivery', 'project_prf_project_elapsed_time', 'project_prf_team_size_group', 'project_prf_max_team_size', 'project_prf_case_tool_used', 'process_pmf_development_methodologies', 'process_pmf_prototyping_used', 'process_pmf_docs', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_client_roles', 'tech_tf_server_roles', 'tech_tf_type_of_server', 'tech_tf_clientserver_description', 'tech_tf_web_development', 'tech_tf_dbms_used', 'tech_tf_tools_used', 'people_prf_project_user_involvement', 'project_prf_cost_currency', 'project_prf_currency_multiple', 'project_prf_application_group__business application', 'project_prf_application_group__infrastructure software', 'project_prf_application_group__mathematically intensive application', 'project_prf_application_group__mathematically_intensive_application', 'project_prf_application_group__nan', 'project_prf_application_group__real_time application']\n",
      "CategoricalEncoder.fit() CALLED\n",
      "Found all categorical columns for single-valued encoding: ['external_eef_data_quality_rating', 'external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_type', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'tech_tf_primary_programming_language', 'project_prf_relative_size', 'project_prf_team_size_group', 'project_prf_case_tool_used', 'process_pmf_development_methodologies', 'process_pmf_prototyping_used', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_client_roles', 'tech_tf_server_roles', 'tech_tf_type_of_server', 'tech_tf_clientserver_description', 'tech_tf_web_development', 'tech_tf_dbms_used', 'people_prf_project_user_involvement', 'project_prf_cost_currency', 'project_prf_currency_multiple']\n",
      "One-hot encoding for single value categorical data14 categorical columns: ['external_eef_data_quality_rating', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'project_prf_relative_size', 'project_prf_case_tool_used', 'process_pmf_prototyping_used', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_type_of_server', 'tech_tf_web_development', 'tech_tf_dbms_used', 'people_prf_project_user_involvement', 'project_prf_currency_multiple']\n",
      "ColumnNameFixer.fit() CALLED\n",
      "Fixed 24 column names for compatibility\n",
      "\n",
      "=== Final Data Validation ===\n",
      "Final shape: (7518, 107)\n",
      "Target column: project_prf_normalised_work_effort\n",
      "Total missing values: 0\n",
      "Total infinite values: 0\n",
      "\n",
      "Data types:\n",
      "  Numeric columns: 18\n",
      "  Categorical columns: 10\n",
      "\n",
      "Target variable 'project_prf_normalised_work_effort' statistics:\n",
      "  Count: 7518.0\n",
      "  Mean: 4747.39\n",
      "  Std: 15957.40\n",
      "  Min: 2.00\n",
      "  Max: 645694.00\n",
      "  Missing: 0\n",
      "\n",
      "============================================================\n",
      "Preprocessing completed successfully!\n",
      "Shape: (7518, 52) -> (7518, 107)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PIPELINE COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "Final dataset saved to: ../data\\enhanced_sample_final.csv\n",
      "Final shape: (3861, 161)\n",
      "Final meta: {'original_sample_shape': (3786, 150), 'original_full_shape': (7518, 147), 'final_shape': (3861, 161), 'categorical_columns_detected': ['process_pmf_development_methodologies'], 'updated_categorical_columns': ['process_pmf_development_methodologies'], 'final_augmented_categorical_columns': ['process_pmf_development_methodologies'], 'high_cardinality_columns_processed': ['external_eef_organisation_type', 'project_prf_application_type'], 'column_mapping': {'external_eef_organisation_type': ['external_eef_organisation_type_top_insurance', 'external_eef_organisation_type_top_medical and health care', 'external_eef_organisation_type_top_manufacturing', 'external_eef_organisation_type_top_telecommunications', 'external_eef_organisation_type_top_government', 'external_eef_organisation_type_top_nan', 'external_eef_organisation_type_top_communications', 'external_eef_organisation_type_top_banking', 'external_eef_organisation_type_top_computers & software', 'external_eef_organisation_type_top_defence', 'external_eef_organisation_type_top_public administration', 'external_eef_organisation_type_top_aerospace / automotive', 'external_eef_organisation_type_top_transport & storage', 'external_eef_organisation_type_top_financial, property & business services', 'external_eef_organisation_type_top_education institution', 'external_eef_organisation_type_top_community services', 'external_eef_organisation_type_top_electricity, gas, water', 'external_eef_organisation_type_top_logistics', 'external_eef_organisation_type_top_wholesale & retail trade', 'external_eef_organisation_type_top_telecommunication', 'external_eef_organisation_type_other'], 'project_prf_application_type': ['project_prf_application_type_top_financial transaction process/accounting', 'project_prf_application_type_top_not recorded', 'project_prf_application_type_top_nan', 'project_prf_application_type_top_unknown', 'project_prf_application_type_top_customer relationship management', 'project_prf_application_type_top_relatively complex application', 'project_prf_application_type_top_workflow support & management', 'project_prf_application_type_top_business application', 'project_prf_application_type_top_embedded system/real_time application', 'project_prf_application_type_top_online. esales', 'project_prf_application_type_top_management of licences and permits', 'project_prf_application_type_top_online analysis and reporting', 'project_prf_application_type_top_catalogue/register of things or events', 'project_prf_application_type_top_software for machine control', 'project_prf_application_type_top_document management', 'project_prf_application_type_top_electronic data interchange', 'project_prf_application_type_top_management information system', 'project_prf_application_type_top_data warehouse system', 'project_prf_application_type_top_stock control & order processing', 'project_prf_application_type_top_management or performance reporting', 'project_prf_application_type_other']}, 'missing_categories_report': {'process_pmf_development_methodologies': ['multifunctional teams; unified process', 'joint application development (jad); rapid application development (rad); timeboxing', 'personal software process (psp)', 'extreme programming (xp)', 'incremental', 'interactive', 'timeboxing', 'rapid application development (rad)', 'personal software process (psp); unified process', 'joint application development (jad)', 'it unified process (itup)', 'multifunctional teams', 'multifunctional teams; timeboxing', 'nan', 'iterative; unified process', 'iterative', 'waterfall (incl linear processing & ssadm)', 'multifunctional teams; waterfall (incl linear processing & ssadm)', 'joint application development (jad); multifunctional teams; rapid application development (rad)', 'joint application development (jad); multifunctional teams', 'rapid application development (rad); timeboxing', 'spiral', 'joint application development (jad); rapid application development (rad)', 'oce', 'multifunctional teams; rapid application development (rad); timeboxing', 'multifunctional teams; rapid application development (rad)', 'joint application development (jad); timeboxing', 'unified process', 'scrum', 'joint application development (jad); multifunctional teams; timeboxing', 'joint application development (jad); multifunctional teams; rapid application development (rad); timeboxing', 'lean']}, 'rows_added_from_full_dataset': 75}\n",
      "Ready for PyCaret setup!\n",
      "\n",
      "SUMMARY:\n",
      "Cell executed at: 2025-06-02 16:14:44.644856\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    final_df, metadata = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf5d5f-ab57-45e8-bb58-55259744c7da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a288a0a-53a6-47b3-9c4b-38803a74b443",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe2cfd-3580-44c3-b135-5ce5a03651cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43544ca-d464-44e5-b10c-046b0024a184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b27979-6652-4826-9c02-0de021f83aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f173d9b-7b2c-4fef-9a15-9f1e74e6caf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
