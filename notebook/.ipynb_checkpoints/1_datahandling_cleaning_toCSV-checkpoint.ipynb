{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e7795e-2a7a-4a33-b741-e6cb1e55684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "from pathlib import Path\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37373c75-ca19-4f7d-940a-3ffbb2fe46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder paths (using Path for cross-platform compatibility)\n",
    "models_folder = '../models'\n",
    "plots_folder = '../plots'\n",
    "temp_folder = '../temp'\n",
    "data_folder = '../data'\n",
    "logs_folder = '../logs'\n",
    "sample_file = 'sample_clean_a_agile_only.xlsx'\n",
    "data_file = 'ISBSG2016R1_1_Formatted4CSVAgileOnly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a30b9b-f803-4261-a18d-c95980369988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp printing activated.\n",
      "Cell executed at: 2025-05-27 15:25:30.698243\n"
     ]
    }
   ],
   "source": [
    "# Sets up an automatic timestamp printout after each Jupyter cell execution \n",
    "# and configures the default visualization style.\n",
    "from IPython import get_ipython\n",
    "\n",
    "def setup_timestamp_callback():\n",
    "    \"\"\"Setup a timestamp callback for Jupyter cells without clearing existing callbacks.\"\"\"\n",
    "    ip = get_ipython()\n",
    "    if ip is not None:\n",
    "        # Define timestamp function\n",
    "        def print_timestamp(*args, **kwargs):\n",
    "            \"\"\"Print timestamp after cell execution.\"\"\"\n",
    "            print(f\"Cell executed at: {datetime.now()}\")\n",
    "        \n",
    "        # Check if our callback is already registered\n",
    "        callbacks = ip.events.callbacks.get('post_run_cell', [])\n",
    "        for cb in callbacks:\n",
    "            if hasattr(cb, '__name__') and cb.__name__ == 'print_timestamp':\n",
    "                # Already registered\n",
    "                return\n",
    "                \n",
    "        # Register new callback if not already present\n",
    "        ip.events.register('post_run_cell', print_timestamp)\n",
    "        print(\"Timestamp printing activated.\")\n",
    "    else:\n",
    "        print(\"Not running in IPython/Jupyter environment.\")\n",
    "\n",
    "# Setup timestamp callback\n",
    "setup_timestamp_callback()\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc45b413-775a-45b7-8d16-c0ef95c83cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "ISBSG2016R1_1_Formatted4CSVAgileOnly\n",
      "Cell executed at: 2025-05-27 15:25:31.123106\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "\n",
    "file_path = f\"{data_folder}/{sample_file}\"  # should use data_file for model training\n",
    "file_name_no_ext = Path(file_path).stem                # 'ISBSG2016R1.1 - FormattedForCSV'\n",
    "print(file_name_no_ext)\n",
    "\n",
    "\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb8f49e0-ac5e-43e5-bc14-9c3346f62810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-05-27 15:25:31.152661\n"
     ]
    }
   ],
   "source": [
    "# Cleans and standardizes string columns and column names by removing spaces, \n",
    "# converting to lowercase, and normalizing formatting.\n",
    "\n",
    "# Cleaning category values\n",
    "def clean_category(val):\n",
    "    if pd.isnull(val):\n",
    "        return val\n",
    "    val = str(val).strip().lower()\n",
    "    val = re.sub(r'\\s+', ' ', val)\n",
    "    val = val.rstrip(';,.')\n",
    "    val = val.replace('(', '').replace(')', '')\n",
    "    val = re.sub(r';\\s*;', ';', val)\n",
    "    val = re.sub(r';\\s+', '; ', val)\n",
    "    return val\n",
    "\n",
    "# Clean column names\n",
    "df.columns = (\n",
    "    df.columns\n",
    "      .str.strip()\n",
    "      .str.lower()\n",
    "      .str.replace(' ', '_', regex=False)\n",
    "      .str.replace('-', '_', regex=False)\n",
    "      .str.replace('__', '_', regex=False)\n",
    "      .str.replace('(', '', regex=False)\n",
    "      .str.replace(')', '', regex=False)\n",
    "      .str.replace('<', 'less_than_', regex=False)\n",
    "      .str.replace('>', 'great_than_', regex=False)\n",
    "      .str.replace('?', '', regex=False)\n",
    ")\n",
    "\n",
    "# After cleaning columns, re-detect categorical columns\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].map(clean_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "797b1b39-8511-4cf7-b81e-87ba01da7258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with semicolon-separated values: ['external_eef_organisation_type', 'project_prf_application_type', 'process_pmf_development_methodologies', 'tech_tf_client_roles', 'tech_tf_server_roles', 'tech_tf_client_server_description']\n",
      "Cell executed at: 2025-05-27 15:25:31.186955\n"
     ]
    }
   ],
   "source": [
    "# Find columns with semicolons\n",
    "def find_semicolon_columns(df, min_count=1, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Returns a list of columns in df where at least `min_count` cell(s) \n",
    "    contain a semicolon (';'). To improve speed, only the first `sample_size` \n",
    "    non-null values in each column are scanned by default.\n",
    "    \"\"\"\n",
    "    semicolon_cols = []\n",
    "    for col in df.columns:\n",
    "        # Drop missing values, convert to string, sample up to `sample_size`\n",
    "        sample = df[col].dropna().astype(str).head(sample_size)\n",
    "        count = sample.str.contains(';').sum()\n",
    "        if count >= min_count:\n",
    "            semicolon_cols.append(col)\n",
    "    return semicolon_cols\n",
    "\n",
    "# Usage:\n",
    "cols_with_semicolons = find_semicolon_columns(df)\n",
    "print(\"Columns with semicolon-separated values:\", cols_with_semicolons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81e678e4-1c43-40dc-a912-55b4cdc52d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current columns: ['isbsg_project_id', 'external_eef_data_quality_rating', 'project_prf_year_of_project', 'external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_group', 'project_prf_application_type', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'tech_tf_primary_programming_language', 'project_prf_functional_size', 'project_prf_relative_size', 'project_prf_normalised_work_effort_level_1', 'project_prf_normalised_work_effort', 'project_prf_normalised_level_1_pdr_ufp', 'project_prf_normalised_pdr_ufp', 'project_prf_defect_density', 'project_prf_speed_of_delivery', 'project_prf_manpower_delivery_rate', 'project_prf_project_elapsed_time', 'project_prf_team_size_group', 'project_prf_max_team_size', 'project_prf_case_tool_used', 'process_pmf_development_methodologies', 'process_pmf_prototyping_used', 'process_pmf_docs', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_client_roles', 'tech_tf_server_roles', 'tech_tf_type_of_server', 'tech_tf_client_server_description', 'tech_tf_web_development', 'tech_tf_dbms_used', 'tech_tf_tools_used', 'tech_tf_upper_case_used', 'people_prf_project_user_involvement', 'people_prf_ba_team_experience_less_than_1_yr', 'people_prf_ba_team_experience_1_to_3_yr', 'people_prf_ba_team_experience_great_than_3_yr', 'people_prf_it_experience_less_than_1_yr', 'people_prf_it_experience_1_to_3_yr', 'people_prf_it_experience_great_than_3_yr', 'people_prf_it_experience_less_than_3_yr', 'people_prf_it_experience_3_to_9_yr', 'people_prf_it_experience_great_than_9_yr', 'people_prf_project_manage_experience', 'people_prf_project_manage_changes', 'people_prf_personnel_changes', 'project_prf_total_project_cost', 'project_prf_cost_currency', 'project_prf_currency_multiple']\n",
      "Cell executed at: 2025-05-27 15:25:31.192584\n"
     ]
    }
   ],
   "source": [
    "print(\"Current columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0bf28be-58f2-401d-9cd9-2616596d3770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-05-27 15:25:31.209453\n"
     ]
    }
   ],
   "source": [
    "# Save the entire cleaned DataFrame (not just the column names) to CSV\n",
    "df.to_csv(f'../data/{file_name_no_ext}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19db0f42-f7e1-4aaf-beb2-38303b2627ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-05-27 15:25:31.222242\n"
     ]
    }
   ],
   "source": [
    "# Clean data\n",
    "# Cleans, de-duplicates, and sorts semicolon-separated categorical values in specified columns.\n",
    "# Clean and sort semicolon-separated categorical values\n",
    "# Clean data\n",
    "# Cleans, de-duplicates, and sorts semicolon-separated categorical values in specified columns.\n",
    "def clean_and_sort_semicolon(val, apply_standardization=False):\n",
    "    \"\"\"Clean and standardise a semicolon-separated categorical string.\"\"\"\n",
    "    if pd.isnull(val):\n",
    "        return val\n",
    "    \n",
    "    # Convert to string in case of mixed types\n",
    "    val_str = str(val).strip()\n",
    "    \n",
    "    # Handle empty strings\n",
    "    if not val_str or val_str.lower() == 'nan':\n",
    "        return None\n",
    "    \n",
    "    # Split, strip, lower, remove trailing punctuation\n",
    "    parts = []\n",
    "    for p in val_str.split(';'):\n",
    "        stripped_p = p.strip()\n",
    "        if stripped_p and stripped_p.lower() != 'nan':  # Only process non-empty parts after stripping\n",
    "            # Normalize internal multiple spaces to a single space\n",
    "            cleaned_p = re.sub(r'\\s+', ' ', stripped_p)\n",
    "            cleaned_p = cleaned_p.lower().rstrip(';,.')\n",
    "            \n",
    "            # Apply standardization rules if requested\n",
    "            if apply_standardization:\n",
    "                cleaned_p = apply_individual_standardization(cleaned_p)\n",
    "            \n",
    "            parts.append(cleaned_p)\n",
    "    \n",
    "    # Remove duplicates, sort\n",
    "    if parts:\n",
    "        parts = sorted(set(parts))\n",
    "        return '; '.join(parts)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def apply_individual_standardization(val):\n",
    "    \"\"\"Apply standardization rules to individual values within semicolon-separated strings.\"\"\"\n",
    "    if not val:\n",
    "        return val\n",
    "    \n",
    "    # Normalize whitespace first\n",
    "    val = re.sub(r'\\s+', ' ', val.strip())\n",
    "    \n",
    "    # Apply Excel-style cleaning rules (in order)\n",
    "    # Replace \" ;\" with \";\"\n",
    "    val = val.replace(' ;', ';')\n",
    "    # Replace \"; \" with \";\" \n",
    "    val = val.replace('; ', ';')\n",
    "    # Replace \" & \" with \"_\"\n",
    "    val = val.replace(' & ', '_')\n",
    "    # Replace \"&/or\" with \"_\"\n",
    "    val = val.replace('&/or', '_')\n",
    "    # Replace \" &\" with \"_\"\n",
    "    val = val.replace(' &', '_')\n",
    "    # Replace \"/\" with \"_\"\n",
    "    val = val.replace('/', '_')\n",
    "    # Replace \": \" with \"_\"\n",
    "    val = val.replace(': ', '_')\n",
    "    # Replace \" (\" with \"_\"\n",
    "    val = val.replace(' (', '_')\n",
    "    # Replace \"(\" with \"\"\n",
    "    val = val.replace('(', '')\n",
    "    # Replace \")\" with \"\"\n",
    "    val = val.replace(')', '')\n",
    "    # Replace \" + \" with \"_\"\n",
    "    val = val.replace(' + ', '_')\n",
    "    \n",
    "    # Clean up any double underscores or trailing underscores\n",
    "    val = re.sub(r'_+', '_', val)  # Replace multiple underscores with single\n",
    "    val = val.strip('_')  # Remove leading/trailing underscores\n",
    "    \n",
    "    # Specific standardization rules for individual components (after cleaning)\n",
    "    standardization_map = {\n",
    "        'stand alone': 'stand-alone',\n",
    "        'stand-alone': 'stand-alone',\n",
    "        'client server': 'client-server',\n",
    "        'mathematically intensive': 'mathematically-intensive',\n",
    "        'mathematically intensive application': 'mathematically-intensive application',\n",
    "    }\n",
    "    \n",
    "    # Check if value matches any standardization rule\n",
    "    if val in standardization_map:\n",
    "        return standardization_map[val]\n",
    "    \n",
    "    # Remove question mark from web dev\n",
    "    if val.replace('?', '').strip() == 'web':\n",
    "        return 'web'\n",
    "    \n",
    "    # Clean up common abbreviations and inconsistencies\n",
    "    val = re.sub(r'\\bpsp\\b', 'personal_software_process', val)\n",
    "    val = re.sub(r'\\bjad\\b', 'joint_application_development', val)\n",
    "    \n",
    "    return val\n",
    "\n",
    "# Standardizes specific categorical columns by normalizing case and correcting inconsistent formatting.\n",
    "def standardize_single_value(val):\n",
    "    \"\"\"Standardize individual categorical values (for non-semicolon columns).\"\"\"\n",
    "    if pd.isnull(val):\n",
    "        return val\n",
    "    \n",
    "    # Convert to string and normalize\n",
    "    val_str = str(val).strip().lower()\n",
    "    \n",
    "    # Handle empty strings or 'nan' strings\n",
    "    if not val_str or val_str == 'nan':\n",
    "        return None\n",
    "    \n",
    "    # Apply the same standardization logic\n",
    "    return apply_individual_standardization(val_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f1a9c17-1557-490a-acef-12190a59742d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLEANING SEMICOLON-SEPARATED COLUMNS ===\n",
      "Cleaning column: external_eef_organisation_type\n",
      "  Unique values: 22 → 22\n",
      "  Sample values: ['banking; communications; education institution; government; medical and health care; transport_storage; wholesale_retail trade', 'government', 'community services']\n",
      "Cleaning column: project_prf_application_type\n",
      "  Unique values: 29 → 29\n",
      "  Sample values: ['surveillance and security', 'business application', 'complex process control; workflow support_management']\n",
      "Cleaning column: process_pmf_development_methodologies\n",
      "  Unique values: 6 → 6\n",
      "  Sample values: ['agile development', 'agile development; unified process', 'agile development; personal software process_psp; unified process']\n",
      "Cleaning column: tech_tf_client_roles\n",
      "  Unique values: 10 → 10\n",
      "  Sample values: ['data entry_validation; data retrieval_presentation; web_html browser', 'web public interface', 'data entry_validation; data retrieval_presentation; run a computer-human interface; security; web_html browser']\n",
      "Cleaning column: tech_tf_server_roles\n",
      "  Unique values: 11 → 11\n",
      "  Sample values: ['html_web server; security_authentication', 'multi-user legacy application', 'database server; file_ print server; html_web server; multi-user legacy application']\n",
      "Cleaning column: tech_tf_client_server_description\n",
      "  Unique values: 4 → 4\n",
      "  Sample values: ['c_s', '10%', 'tech_tf_client_server_description']\n",
      "\n",
      "=== CLEANING SEMICOLON COLUMNS WITH STANDARDIZATION ===\n",
      "Cleaning column: project_prf_application_group\n",
      "  Unique values: 4 → 4\n",
      "  Sample values: ['business application', 'mathematically-intensive application', 'real-time application']\n",
      "\n",
      "=== STANDARDIZING SINGLE-VALUE COLUMNS ===\n",
      "Standardizing column: tech_tf_architecture\n",
      "  Unique values: 3 → 3\n",
      "Standardizing column: tech_tf_web_development\n",
      "  Unique values: 2 → 2\n",
      "\n",
      "=== NORMALIZING LANGUAGE TYPE ===\n",
      "Normalizing tech_tf_language_type to uppercase\n",
      "  Unique values: 3 → 3\n",
      "\n",
      "=== VERIFICATION RESULTS ===\n",
      "\n",
      "Column: external_eef_organisation_type\n",
      "  Total unique values: 22\n",
      "  ✓ All semicolon-separated values appear properly sorted and deduplicated\n",
      "  ✓ All semicolon separators are consistent ('; ')\n",
      "  Sample values: ['banking; communications; education institution; government; medical and health care; transport_storage; wholesale_retail trade', 'government', 'community services']\n",
      "\n",
      "Column: project_prf_application_type\n",
      "  Total unique values: 29\n",
      "  ✓ All semicolon-separated values appear properly sorted and deduplicated\n",
      "  ✓ All semicolon separators are consistent ('; ')\n",
      "  Sample values: ['surveillance and security', 'business application', 'complex process control; workflow support_management']\n",
      "\n",
      "Column: process_pmf_development_methodologies\n",
      "  Total unique values: 6\n",
      "  ✓ All semicolon-separated values appear properly sorted and deduplicated\n",
      "  ✓ All semicolon separators are consistent ('; ')\n",
      "  Sample values: ['agile development', 'agile development; unified process', 'agile development; personal software process_psp; unified process']\n",
      "\n",
      "Column: tech_tf_client_roles\n",
      "  Total unique values: 10\n",
      "  ✓ All semicolon-separated values appear properly sorted and deduplicated\n",
      "  ✓ All semicolon separators are consistent ('; ')\n",
      "  Sample values: ['data entry_validation; data retrieval_presentation; web_html browser', 'web public interface', 'data entry_validation; data retrieval_presentation; run a computer-human interface; security; web_html browser']\n",
      "\n",
      "Column: tech_tf_server_roles\n",
      "  Total unique values: 11\n",
      "  ✓ All semicolon-separated values appear properly sorted and deduplicated\n",
      "  ✓ All semicolon separators are consistent ('; ')\n",
      "  Sample values: ['html_web server; security_authentication', 'multi-user legacy application', 'database server; file_ print server; html_web server; multi-user legacy application']\n",
      "\n",
      "Column: tech_tf_client_server_description\n",
      "  Total unique values: 4\n",
      "  ✓ All semicolon-separated values appear properly sorted and deduplicated\n",
      "  ✓ All semicolon separators are consistent ('; ')\n",
      "  Sample values: ['c_s', '10%', 'tech_tf_client_server_description']\n",
      "\n",
      "Column: project_prf_application_group\n",
      "  Total unique values: 4\n",
      "  ✓ All semicolon-separated values appear properly sorted and deduplicated\n",
      "  Sample values: ['business application', 'mathematically-intensive application', 'real-time application']\n",
      "\n",
      "=== DETAILED CLEANING EXAMPLES ===\n",
      "\n",
      "Column: external_eef_organisation_type\n",
      "  Complex semicolon-separated values found:\n",
      "    1. 'banking; communications; education institution; government; medical and health care; transport_storage; wholesale_retail trade'\n",
      "       Part 1: 'banking'\n",
      "       Part 2: 'communications'\n",
      "       Part 3: 'education institution'\n",
      "       Part 4: 'government'\n",
      "       Part 5: 'medical and health care'\n",
      "       Part 6: 'transport_storage'\n",
      "       Part 7: 'wholesale_retail trade'\n",
      "    2. 'education institution; electricity, gas, water; ieee'\n",
      "       Part 1: 'education institution'\n",
      "       Part 2: 'electricity, gas, water'\n",
      "       Part 3: 'ieee'\n",
      "    3. 'community services; government; public administration'\n",
      "       Part 1: 'community services'\n",
      "       Part 2: 'government'\n",
      "       Part 3: 'public administration'\n",
      "\n",
      "Column: project_prf_application_type\n",
      "  Complex semicolon-separated values found:\n",
      "    1. 'complex process control; workflow support_management'\n",
      "       Part 1: 'complex process control'\n",
      "       Part 2: 'workflow support_management'\n",
      "    2. 'data warehouse system; financial transaction process_accounting; management or performance reporting'\n",
      "       Part 1: 'data warehouse system'\n",
      "       Part 2: 'financial transaction process_accounting'\n",
      "       Part 3: 'management or performance reporting'\n",
      "    3. 'mathematical modelling_finance or eng; software development tool'\n",
      "       Part 1: 'mathematical modelling_finance or eng'\n",
      "       Part 2: 'software development tool'\n",
      "\n",
      "Column: process_pmf_development_methodologies\n",
      "  Complex semicolon-separated values found:\n",
      "    1. 'agile development; unified process'\n",
      "       Part 1: 'agile development'\n",
      "       Part 2: 'unified process'\n",
      "    2. 'agile development; personal software process_psp; unified process'\n",
      "       Part 1: 'agile development'\n",
      "       Part 2: 'personal software process_psp'\n",
      "       Part 3: 'unified process'\n",
      "    3. 'agile development; scrum'\n",
      "       Part 1: 'agile development'\n",
      "       Part 2: 'scrum'\n",
      "\n",
      "Column: tech_tf_client_roles\n",
      "  Complex semicolon-separated values found:\n",
      "    1. 'data entry_validation; data retrieval_presentation; web_html browser'\n",
      "       Part 1: 'data entry_validation'\n",
      "       Part 2: 'data retrieval_presentation'\n",
      "       Part 3: 'web_html browser'\n",
      "    2. 'data entry_validation; data retrieval_presentation; run a computer-human interface; security; web_html browser'\n",
      "       Part 1: 'data entry_validation'\n",
      "       Part 2: 'data retrieval_presentation'\n",
      "       Part 3: 'run a computer-human interface'\n",
      "       Part 4: 'security'\n",
      "       Part 5: 'web_html browser'\n",
      "    3. 'data entry_validation; run a computer-human interface; web public interface'\n",
      "       Part 1: 'data entry_validation'\n",
      "       Part 2: 'run a computer-human interface'\n",
      "       Part 3: 'web public interface'\n",
      "\n",
      "Column: tech_tf_server_roles\n",
      "  Complex semicolon-separated values found:\n",
      "    1. 'html_web server; security_authentication'\n",
      "       Part 1: 'html_web server'\n",
      "       Part 2: 'security_authentication'\n",
      "    2. 'database server; file_ print server; html_web server; multi-user legacy application'\n",
      "       Part 1: 'database server'\n",
      "       Part 2: 'file_ print server'\n",
      "       Part 3: 'html_web server'\n",
      "       Part 4: 'multi-user legacy application'\n",
      "    3. 'database server; html_web server; mail server; security_authentication'\n",
      "       Part 1: 'database server'\n",
      "       Part 2: 'html_web server'\n",
      "       Part 3: 'mail server'\n",
      "       Part 4: 'security_authentication'\n",
      "\n",
      "Column: tech_tf_client_server_description\n",
      "  Complex semicolon-separated values found:\n",
      "    1. 'client_presentation; server_processing'\n",
      "       Part 1: 'client_presentation'\n",
      "       Part 2: 'server_processing'\n",
      "\n",
      "Column: project_prf_application_group\n",
      "  Sample values (no semicolons):\n",
      "    1. 'business application'\n",
      "    2. 'mathematically-intensive application'\n",
      "    3. 'real-time application'\n",
      "\n",
      "=== TRANSFORMATION EXAMPLES ===\n",
      "Original → Cleaned:\n",
      "  'workflow support & management' → 'workflow support_management'\n",
      "  'data entry &/or validation' → 'data entry _ validation'\n",
      "  'file &/or print server' → 'file _ print server'\n",
      "  'html/web server: security' → 'html_web server_security'\n",
      "  'client (desktop) application' → 'client_desktop application'\n",
      "  'database + file server' → 'database_file server'\n",
      "  'agile development ; scrum' → 'agile development;scrum'\n",
      "  'web development: html & css' → 'web development_html_css'\n",
      "\n",
      "=== POTENTIAL REMAINING ISSUES ===\n",
      "✓ No obvious formatting issues detected!\n",
      "\n",
      "=== ADDITIONAL CHECKS ===\n",
      "Column external_eef_organisation_type: Found values with untrimmed parts: ['banking; communications; education institution; government; medical and health care; transport_storage; wholesale_retail trade', 'education institution; electricity, gas, water; ieee']\n",
      "Column project_prf_application_type: Found values with untrimmed parts: ['complex process control; workflow support_management', 'data warehouse system; financial transaction process_accounting; management or performance reporting']\n",
      "Column process_pmf_development_methodologies: Found values with untrimmed parts: ['agile development; unified process', 'agile development; personal software process_psp; unified process']\n",
      "Column tech_tf_client_roles: Found values with untrimmed parts: ['data entry_validation; data retrieval_presentation; web_html browser', 'data entry_validation; data retrieval_presentation; run a computer-human interface; security; web_html browser']\n",
      "Column tech_tf_server_roles: Found values with untrimmed parts: ['html_web server; security_authentication', 'database server; file_ print server; html_web server; multi-user legacy application']\n",
      "Column tech_tf_client_server_description: Found values with untrimmed parts: ['client_presentation; server_processing']\n",
      "Column project_prf_application_group: ✓ All parts properly trimmed\n",
      "Cell executed at: 2025-05-27 15:25:31.272416\n"
     ]
    }
   ],
   "source": [
    "# cols with semicolon as seperator is stored in cols_with_semicolons\n",
    "\n",
    "# Manual specification for your case:\n",
    "cols_semicolon_with_standardization = ['project_prf_application_group']\n",
    "cols_single_standardization = ['tech_tf_architecture', 'tech_tf_web_development']\n",
    "\n",
    "# Clean semicolon-separated columns (without standardization)\n",
    "print(\"=== CLEANING SEMICOLON-SEPARATED COLUMNS ===\")\n",
    "for col in cols_with_semicolons:\n",
    "    if col in df.columns:\n",
    "        print(f\"Cleaning column: {col}\")\n",
    "        original_unique = len(df[col].dropna().unique())\n",
    "        df[col] = df[col].map(clean_and_sort_semicolon)\n",
    "        new_unique = len(df[col].dropna().unique())\n",
    "        print(f\"  Unique values: {original_unique} → {new_unique}\")\n",
    "        \n",
    "        # Show sample values\n",
    "        sample_vals = df[col].dropna().unique()[:3]\n",
    "        print(f\"  Sample values: {[str(v) for v in sample_vals]}\")\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found in dataframe\")\n",
    "\n",
    "# Clean semicolon-separated columns WITH standardization\n",
    "print(f\"\\n=== CLEANING SEMICOLON COLUMNS WITH STANDARDIZATION ===\")\n",
    "for col in cols_semicolon_with_standardization:\n",
    "    if col in df.columns:\n",
    "        print(f\"Cleaning column: {col}\")\n",
    "        original_unique = len(df[col].dropna().unique())\n",
    "        df[col] = df[col].map(lambda x: clean_and_sort_semicolon(x, apply_standardization=True))\n",
    "        new_unique = len(df[col].dropna().unique())\n",
    "        print(f\"  Unique values: {original_unique} → {new_unique}\")\n",
    "        \n",
    "        # Show sample values\n",
    "        sample_vals = df[col].dropna().unique()[:3]\n",
    "        print(f\"  Sample values: {[str(v) for v in sample_vals]}\")\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found in dataframe\")\n",
    "\n",
    "# Apply standardization to single-value columns\n",
    "print(f\"\\n=== STANDARDIZING SINGLE-VALUE COLUMNS ===\")\n",
    "for col in cols_single_standardization:\n",
    "    if col in df.columns:\n",
    "        print(f\"Standardizing column: {col}\")\n",
    "        original_unique = len(df[col].dropna().unique())\n",
    "        df[col] = df[col].map(standardize_single_value)\n",
    "        new_unique = len(df[col].dropna().unique())\n",
    "        print(f\"  Unique values: {original_unique} → {new_unique}\")\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found in dataframe\")\n",
    "\n",
    "# Special case for language type (uppercase normalization)\n",
    "if 'tech_tf_language_type' in df.columns:\n",
    "    print(f\"\\n=== NORMALIZING LANGUAGE TYPE ===\")\n",
    "    print(\"Normalizing tech_tf_language_type to uppercase\")\n",
    "    original_unique = len(df['tech_tf_language_type'].dropna().unique())\n",
    "    df['tech_tf_language_type'] = df['tech_tf_language_type'].astype(str).str.upper().str.strip()\n",
    "    # Replace 'NAN' with actual NaN\n",
    "    df['tech_tf_language_type'] = df['tech_tf_language_type'].replace('NAN', pd.NA)\n",
    "    new_unique = len(df['tech_tf_language_type'].dropna().unique())\n",
    "    print(f\"  Unique values: {original_unique} → {new_unique}\")\n",
    "\n",
    "# Verification function to check the cleaning results\n",
    "def verify_semicolon_cleaning(df, columns):\n",
    "    \"\"\"Verify that semicolon-separated columns are properly cleaned.\"\"\"\n",
    "    print(\"\\n=== VERIFICATION RESULTS ===\")\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            print(f\"\\nColumn: {col}\")\n",
    "            unique_vals = df[col].dropna().unique()\n",
    "            print(f\"  Total unique values: {len(unique_vals)}\")\n",
    "            \n",
    "            # Check for unsorted or duplicate issues\n",
    "            problematic = []\n",
    "            for val in unique_vals:\n",
    "                if ';' in str(val):\n",
    "                    parts = str(val).split(';')\n",
    "                    parts_stripped = [p.strip() for p in parts]\n",
    "                    if parts_stripped != sorted(set(parts_stripped)):\n",
    "                        problematic.append(val)\n",
    "            \n",
    "            if problematic:\n",
    "                print(f\"  ⚠️  Potentially problematic values: {problematic[:3]}\")\n",
    "            else:\n",
    "                print(\"  ✓ All semicolon-separated values appear properly sorted and deduplicated\")\n",
    "            \n",
    "            # Check for consistency in separators\n",
    "            semicolon_vals = [v for v in unique_vals if ';' in str(v)]\n",
    "            if semicolon_vals:\n",
    "                inconsistent_separators = [v for v in semicolon_vals if '; ' not in str(v) and ';' in str(v)]\n",
    "                if inconsistent_separators:\n",
    "                    print(f\"  ⚠️  Inconsistent separators: {inconsistent_separators[:3]}\")\n",
    "                else:\n",
    "                    print(\"  ✓ All semicolon separators are consistent ('; ')\")\n",
    "            \n",
    "            # Show sample of values\n",
    "            sample_vals = [str(v) for v in unique_vals[:3] if pd.notna(v)]\n",
    "            print(f\"  Sample values: {sample_vals}\")\n",
    "\n",
    "# Run verification on all semicolon columns\n",
    "all_semicolon_cols = cols_with_semicolons + cols_semicolon_with_standardization\n",
    "verify_semicolon_cleaning(df, all_semicolon_cols)\n",
    "\n",
    "# Show detailed before/after comparison for a few problematic values\n",
    "def show_detailed_cleaning_examples(df, columns, max_examples=3):\n",
    "    \"\"\"Show detailed before/after examples of cleaning for verification.\"\"\"\n",
    "    print(f\"\\n=== DETAILED CLEANING EXAMPLES ===\")\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nColumn: {col}\")\n",
    "        \n",
    "        # Get some complex values (those with semicolons)\n",
    "        complex_vals = [v for v in df[col].dropna().unique() if ';' in str(v)][:max_examples]\n",
    "        \n",
    "        if complex_vals:\n",
    "            print(\"  Complex semicolon-separated values found:\")\n",
    "            for i, val in enumerate(complex_vals, 1):\n",
    "                print(f\"    {i}. '{val}'\")\n",
    "                # Show the individual parts\n",
    "                parts = str(val).split(';')\n",
    "                for j, part in enumerate(parts):\n",
    "                    print(f\"       Part {j+1}: '{part.strip()}'\")\n",
    "        else:\n",
    "            # Show simple values\n",
    "            simple_vals = df[col].dropna().unique()[:max_examples]\n",
    "            print(\"  Sample values (no semicolons):\")\n",
    "            for i, val in enumerate(simple_vals, 1):\n",
    "                print(f\"    {i}. '{val}'\")\n",
    "\n",
    "show_detailed_cleaning_examples(df, all_semicolon_cols)\n",
    "\n",
    "# Function to identify potential remaining issues\n",
    "def identify_remaining_issues(df, columns):\n",
    "    \"\"\"Identify potential issues that might still need attention.\"\"\"\n",
    "    print(f\"\\n=== POTENTIAL REMAINING ISSUES ===\")\n",
    "    \n",
    "    issues_found = False\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        col_issues = []\n",
    "        unique_vals = df[col].dropna().unique()\n",
    "        \n",
    "        for val in unique_vals:\n",
    "            val_str = str(val)\n",
    "            \n",
    "            # Check for various potential issues that should be cleaned\n",
    "            if ' & ' in val_str:\n",
    "                col_issues.append(f\"Still contains ' & ': '{val_str}'\")\n",
    "            if '&/or' in val_str:\n",
    "                col_issues.append(f\"Still contains '&/or': '{val_str}'\")\n",
    "            if ' &' in val_str:\n",
    "                col_issues.append(f\"Still contains ' &': '{val_str}'\")\n",
    "            if '/' in val_str:\n",
    "                col_issues.append(f\"Still contains '/': '{val_str}'\")\n",
    "            if ': ' in val_str:\n",
    "                col_issues.append(f\"Still contains ': ': '{val_str}'\")\n",
    "            if ' (' in val_str or '(' in val_str or ')' in val_str:\n",
    "                col_issues.append(f\"Still contains parentheses: '{val_str}'\")\n",
    "            if ' + ' in val_str:\n",
    "                col_issues.append(f\"Still contains ' + ': '{val_str}'\")\n",
    "            if '  ' in val_str:  # Double spaces\n",
    "                col_issues.append(f\"Contains double spaces: '{val_str}'\")\n",
    "            if val_str.endswith(' ') or val_str.startswith(' '):\n",
    "                col_issues.append(f\"Has leading/trailing spaces: '{val_str}'\")\n",
    "            if re.search(r'[A-Z]', val_str):  # Contains uppercase\n",
    "                col_issues.append(f\"Contains uppercase: '{val_str}'\")\n",
    "            if '__' in val_str:  # Double underscores\n",
    "                col_issues.append(f\"Contains double underscores: '{val_str}'\")\n",
    "        \n",
    "        if col_issues:\n",
    "            print(f\"\\nColumn: {col}\")\n",
    "            issues_found = True\n",
    "            for issue in col_issues[:5]:  # Show first 5 issues\n",
    "                print(f\"  - {issue}\")\n",
    "            if len(col_issues) > 5:\n",
    "                print(f\"  ... and {len(col_issues) - 5} more issues\")\n",
    "    \n",
    "    if not issues_found:\n",
    "        print(\"✓ No obvious formatting issues detected!\")\n",
    "\n",
    "# Show examples of transformations\n",
    "def show_transformation_examples():\n",
    "    \"\"\"Show examples of how the cleaning rules transform values.\"\"\"\n",
    "    print(f\"\\n=== TRANSFORMATION EXAMPLES ===\")\n",
    "    \n",
    "    test_cases = [\n",
    "        \"workflow support & management\",\n",
    "        \"data entry &/or validation\", \n",
    "        \"file &/or print server\",\n",
    "        \"html/web server: security\",\n",
    "        \"client (desktop) application\",\n",
    "        \"database + file server\",\n",
    "        \"agile development ; scrum\",\n",
    "        \"web development: html & css\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Original → Cleaned:\")\n",
    "    for case in test_cases:\n",
    "        cleaned = apply_individual_standardization(case.lower())\n",
    "        print(f\"  '{case}' → '{cleaned}'\")\n",
    "\n",
    "show_transformation_examples()\n",
    "\n",
    "identify_remaining_issues(df, all_semicolon_cols)\n",
    "\n",
    "# Additional verification: Check for common issues\n",
    "print(f\"\\n=== ADDITIONAL CHECKS ===\")\n",
    "for col in all_semicolon_cols:\n",
    "    if col in df.columns:\n",
    "        # Check for trailing/leading spaces in parts\n",
    "        problem_vals = []\n",
    "        for val in df[col].dropna().unique():\n",
    "            if ';' in str(val):\n",
    "                parts = str(val).split(';')\n",
    "                for part in parts:\n",
    "                    if part != part.strip():\n",
    "                        problem_vals.append(val)\n",
    "                        break\n",
    "        \n",
    "        if problem_vals:\n",
    "            print(f\"Column {col}: Found values with untrimmed parts: {problem_vals[:2]}\")\n",
    "        else:\n",
    "            print(f\"Column {col}: ✓ All parts properly trimmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535dd9d2-1e7a-44a6-a18f-4341a6f9c312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea001aed-6c12-440d-ae68-ae3d948a4115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Dropping: Unique values for categorical columns saved to '../temp/all_categorical_unique_values_beforeDropping.txt'\n",
      "Cell executed at: 2025-05-27 15:25:31.301745\n"
     ]
    }
   ],
   "source": [
    "# Writes the unique values of all categorical columns to a text file for reference or auditing.\n",
    "\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "with open(f\"{temp_folder}/all_categorical_unique_values_beforeDropping.txt\", 'w') as f:\n",
    "    for col in cat_cols:\n",
    "        f.write(f\"Column: {col} (n_unique = {df[col].nunique()})\\n\")\n",
    "        f.write(f\"{df[col].unique()}\\n\")\n",
    "        f.write('-' * 40 + '\\n')\n",
    "\n",
    "print(f\"Before Dropping: Unique values for categorical columns saved to '{temp_folder}/{file_name_no_ext}_all_categorical_unique_values_beforeDropping.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31086bd0-d2c5-4a61-bd38-4358f4303656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-05-27 15:25:31.317988\n"
     ]
    }
   ],
   "source": [
    "# Save the entire cleaned DataFrame (not just the column names) to CSV\n",
    "df.to_csv(f\"{data_folder}/{file_name_no_ext}_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd70455-798a-49a2-b564-2765c63cdcc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3df8f-1004-4c7e-a548-fe22b8847afc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
