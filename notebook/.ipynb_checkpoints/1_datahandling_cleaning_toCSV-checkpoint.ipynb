{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e7795e-2a7a-4a33-b741-e6cb1e55684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37373c75-ca19-4f7d-940a-3ffbb2fe46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the foler path\n",
    "models_folder = '../models'\n",
    "plots_folder = '../plots'\n",
    "temp_folder = '../temp'\n",
    "data_folder = '../data'\n",
    "logs_folder = '../logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a30b9b-f803-4261-a18d-c95980369988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp printing activated.\n",
      "Cell executed at: 2025-05-25 14:00:56.358660\n"
     ]
    }
   ],
   "source": [
    "# Sets up an automatic timestamp printout after each Jupyter cell execution \n",
    "# and configures the default visualization style.\n",
    "from IPython import get_ipython\n",
    "\n",
    "def setup_timestamp_callback():\n",
    "    \"\"\"Setup a timestamp callback for Jupyter cells without clearing existing callbacks.\"\"\"\n",
    "    ip = get_ipython()\n",
    "    if ip is not None:\n",
    "        # Define timestamp function\n",
    "        def print_timestamp(*args, **kwargs):\n",
    "            \"\"\"Print timestamp after cell execution.\"\"\"\n",
    "            print(f\"Cell executed at: {datetime.now()}\")\n",
    "        \n",
    "        # Check if our callback is already registered\n",
    "        callbacks = ip.events.callbacks.get('post_run_cell', [])\n",
    "        for cb in callbacks:\n",
    "            if hasattr(cb, '__name__') and cb.__name__ == 'print_timestamp':\n",
    "                # Already registered\n",
    "                return\n",
    "                \n",
    "        # Register new callback if not already present\n",
    "        ip.events.register('post_run_cell', print_timestamp)\n",
    "        print(\"Timestamp printing activated.\")\n",
    "    else:\n",
    "        print(\"Not running in IPython/Jupyter environment.\")\n",
    "\n",
    "# Setup timestamp callback\n",
    "setup_timestamp_callback()\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc45b413-775a-45b7-8d16-c0ef95c83cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "ISBSG2016R1.1_Formatted4CSVAgileOnly\n",
      "Cell executed at: 2025-05-25 14:00:56.735443\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "file_path = f\"{data_folder}/ISBSG2016R1.1_Formatted4CSVAgileOnly.xlsx\"\n",
    "file_name_no_ext = Path(file_path).stem                # 'ISBSG2016R1.1 - FormattedForCSV'\n",
    "print(file_name_no_ext)\n",
    "\n",
    "\n",
    "df = pd.read_excel(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb1617d-2715-4890-8d46-d81457477b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-05-25 14:00:56.769222\n"
     ]
    }
   ],
   "source": [
    "# Cleans and standardizes string columns and column names by removing spaces, \n",
    "# converting to lowercase, and normalizing formatting.\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_category(val):\n",
    "    if pd.isnull(val):\n",
    "        return val\n",
    "    # Lowercase, strip spaces, remove trailing punctuation\n",
    "    val = val.strip().lower()\n",
    "    val = re.sub(r'\\s+', ' ', val)  # collapse multiple spaces\n",
    "    val = val.rstrip(';,.')\n",
    "    val = val.replace('(', '').replace(')', '')\n",
    "    # Remove duplicate semicolons and extra spaces between separated values\n",
    "    val = re.sub(r';\\s*;', ';', val)\n",
    "    val = re.sub(r';\\s+', '; ', val)\n",
    "    return val\n",
    "\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].map(clean_category)\n",
    "\n",
    "# Clean column names: lowercase, replace spaces with underscores, strip\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.strip()                # remove leading/trailing spaces\n",
    "    .str.lower()                # make lowercase\n",
    "    .str.replace(' ', '_')      # replace spaces with underscores\n",
    "    .str.replace('-', '_')      # optional: replace hyphens with underscores\n",
    "    .str.replace('__', '_')      \n",
    "    .str.replace('(', '')     \n",
    "    .str.replace(')', '')      \n",
    "    .str.replace('<', 'less_than_')     \n",
    "    .str.replace('>', 'great_than_')\n",
    "    .str.replace('?', '')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e678e4-1c43-40dc-a912-55b4cdc52d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current columns: ['isbsg_project_id', 'external_eef_data_quality_rating', 'project_prf_year_of_project', 'external_eef_industry_sector', 'external_eef_organisation_type', 'project_prf_application_group', 'project_prf_application_type', 'project_prf_development_type', 'tech_tf_development_platform', 'tech_tf_language_type', 'tech_tf_primary_programming_language', 'project_prf_functional_size', 'project_prf_relative_size', 'project_prf_normalised_work_effort', 'project_prf_normalised_level_1_pdr_ufp', 'project_prf_normalised_pdr_ufp', 'project_prf_defect_density', 'project_prf_speed_of_delivery', 'project_prf_manpower_delivery_rate', 'project_prf_project_elapsed_time', 'project_prf_team_size_group', 'project_prf_max_team_size', 'project_prf_case_tool_used', 'process_pmf_development_methodologies', 'process_pmf_prototyping_used', 'process_pmf_docs', 'tech_tf_architecture', 'tech_tf_client_server', 'tech_tf_client_roles', 'tech_tf_server_roles', 'tech_tf_type_of_server', 'tech_tf_web_development', 'tech_tf_dbms_used', 'tech_tf_tools_used', 'people_prf_project_user_involvement', 'people_prf_ba_team_experience_less_than_1_yr', 'people_prf_ba_team_experience_1_to_3_yr', 'people_prf_ba_team_experience_great_than_3_yr', 'people_prf_it_experience_less_than_1_yr', 'people_prf_it_experience_1_to_3_yr', 'people_prf_it_experience_great_than_3_yr', 'people_prf_it_experience_less_than_3_yr', 'people_prf_it_experience_3_to_9_yr', 'people_prf_it_experience_great_than_9_yr', 'people_prf_project_manage_experience', 'people_prf_project_manage_changes', 'people_prf_personnel_changes', 'project_prf_total_project_cost', 'project_prf_cost_currency', 'project_prf_currency_multiple']\n",
      "Cell executed at: 2025-05-25 14:00:56.774595\n"
     ]
    }
   ],
   "source": [
    "print(\"Current columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0bf28be-58f2-401d-9cd9-2616596d3770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-05-25 14:00:56.789770\n"
     ]
    }
   ],
   "source": [
    "# Save the entire cleaned DataFrame (not just the column names) to CSV\n",
    "df.to_csv(f'../data/{file_name_no_ext}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19db0f42-f7e1-4aaf-beb2-38303b2627ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-05-25 14:00:56.800350\n"
     ]
    }
   ],
   "source": [
    "# Clean data\n",
    "# Cleans, de-duplicates, and sorts semicolon-separated categorical values in specified columns.\n",
    "def clean_and_sort_semicolon(val, apply_standardization=False):\n",
    "    \"\"\"Clean and standardise a semicolon-separated categorical string.\"\"\"\n",
    "    if pd.isnull(val):\n",
    "        return val\n",
    "    \n",
    "    # Convert to string in case of mixed types\n",
    "    val_str = str(val).strip()\n",
    "    \n",
    "    # Handle empty strings\n",
    "    if not val_str or val_str.lower() == 'nan':\n",
    "        return None\n",
    "    \n",
    "    # Split, strip, lower, remove trailing punctuation\n",
    "    parts = []\n",
    "    for p in val_str.split(';'):\n",
    "        stripped_p = p.strip()\n",
    "        if stripped_p and stripped_p.lower() != 'nan':  # Only process non-empty parts after stripping\n",
    "            # Normalize internal multiple spaces to a single space\n",
    "            cleaned_p = re.sub(r'\\s+', ' ', stripped_p)\n",
    "            cleaned_p = cleaned_p.lower().rstrip(';,.')\n",
    "            \n",
    "            # Apply standardization rules if requested\n",
    "            if apply_standardization:\n",
    "                cleaned_p = apply_individual_standardization(cleaned_p)\n",
    "            \n",
    "            parts.append(cleaned_p)\n",
    "    \n",
    "    # Remove duplicates, sort\n",
    "    if parts:\n",
    "        parts = sorted(set(parts))\n",
    "        return '; '.join(parts)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def apply_individual_standardization(val):\n",
    "    \"\"\"Apply standardization rules to individual values within semicolon-separated strings.\"\"\"\n",
    "    if not val:\n",
    "        return val\n",
    "    \n",
    "    # Normalize whitespace first\n",
    "    val = re.sub(r'\\s+', ' ', val.strip())\n",
    "    \n",
    "    # Apply Excel-style cleaning rules (in order)\n",
    "    # Replace \" ;\" with \";\"\n",
    "    val = val.replace(' ;', ';')\n",
    "    # Replace \"; \" with \";\" \n",
    "    val = val.replace('; ', ';')\n",
    "    # Replace \" & \" with \"_\"\n",
    "    val = val.replace(' & ', '_')\n",
    "    # Replace \"&/or\" with \"_\"\n",
    "    val = val.replace('&/or', '_')\n",
    "    # Replace \" &\" with \"_\"\n",
    "    val = val.replace(' &', '_')\n",
    "    # Replace \"/\" with \"_\"\n",
    "    val = val.replace('/', '_')\n",
    "    # Replace \": \" with \"_\"\n",
    "    val = val.replace(': ', '_')\n",
    "    # Replace \" (\" with \"_\"\n",
    "    val = val.replace(' (', '_')\n",
    "    # Replace \"(\" with \"\"\n",
    "    val = val.replace('(', '')\n",
    "    # Replace \")\" with \"\"\n",
    "    val = val.replace(')', '')\n",
    "    # Replace \" + \" with \"_\"\n",
    "    val = val.replace(' + ', '_')\n",
    "    \n",
    "    # Clean up any double underscores or trailing underscores\n",
    "    val = re.sub(r'_+', '_', val)  # Replace multiple underscores with single\n",
    "    val = val.strip('_')  # Remove leading/trailing underscores\n",
    "    \n",
    "    # Specific standardization rules for individual components (after cleaning)\n",
    "    standardization_map = {\n",
    "        'stand alone': 'stand-alone',\n",
    "        'stand-alone': 'stand-alone',\n",
    "        'client server': 'client-server',\n",
    "        'mathematically intensive': 'mathematically-intensive',\n",
    "        'mathematically intensive application': 'mathematically-intensive application',\n",
    "    }\n",
    "    \n",
    "    # Check if value matches any standardization rule\n",
    "    if val in standardization_map:\n",
    "        return standardization_map[val]\n",
    "    \n",
    "    # Remove question mark from web dev\n",
    "    if val.replace('?', '').strip() == 'web':\n",
    "        return 'web'\n",
    "    \n",
    "    # Clean up common abbreviations and inconsistencies\n",
    "    val = re.sub(r'\\bpsp\\b', 'personal_software_process', val)\n",
    "    val = re.sub(r'\\bjad\\b', 'joint_application_development', val)\n",
    "    \n",
    "    return val\n",
    "\n",
    "# Standardizes specific categorical columns by normalizing case and correcting inconsistent formatting.\n",
    "def standardize_single_value(val):\n",
    "    \"\"\"Standardize individual categorical values (for non-semicolon columns).\"\"\"\n",
    "    if pd.isnull(val):\n",
    "        return val\n",
    "    \n",
    "    # Convert to string and normalize\n",
    "    val_str = str(val).strip().lower()\n",
    "    \n",
    "    # Handle empty strings or 'nan' strings\n",
    "    if not val_str or val_str == 'nan':\n",
    "        return None\n",
    "    \n",
    "    # Apply the same standardization logic\n",
    "    return apply_individual_standardization(val_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f1a9c17-1557-490a-acef-12190a59742d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLEANING SEMICOLON-SEPARATED COLUMNS ===\n",
      "Cleaning column: project_prf_application_group\n",
      "  Unique values: 4 → 4\n",
      "  Sample values: ['business application', 'real-time application', 'mathematically-intensive application']\n",
      "Cleaning column: external_eef_organisation_type\n",
      "  Unique values: 25 → 25\n",
      "  Sample values: ['banking; communications; education institution; government; medical and health care; transport & storage; wholesale & retail trade', 'government', 'community services']\n",
      "Cleaning column: project_prf_application_type\n",
      "  Unique values: 32 → 32\n",
      "  Sample values: ['surveillance and security', 'business application', 'complex process control; workflow support & management']\n",
      "Cleaning column: process_pmf_development_methodologies\n",
      "  Unique values: 7 → 7\n",
      "  Sample values: ['agile development', 'agile development; unified process', 'agile development; personal software process psp; unified process']\n",
      "Cleaning column: tech_tf_client_roles\n",
      "  Unique values: 10 → 10\n",
      "  Sample values: ['data entry & validation; data retrieval & presentation; web/html browser', 'web public interface', 'data entry & validation; data retrieval & presentation; run a computer-human interface; security; web/html browser']\n",
      "Cleaning column: tech_tf_server_roles\n",
      "  Unique values: 12 → 12\n",
      "  Sample values: ['html/web server; security/authentication', 'multi-user legacy application', 'database server; file &/or print server; html/web server; multi-user legacy application']\n",
      "\n",
      "=== CLEANING SEMICOLON COLUMNS WITH STANDARDIZATION ===\n",
      "Cleaning column: project_prf_application_group\n",
      "  Unique values: 4 → 4\n",
      "  Sample values: ['business application', 'real-time application', 'mathematically-intensive application']\n",
      "\n",
      "=== STANDARDIZING SINGLE-VALUE COLUMNS ===\n",
      "Standardizing column: tech_tf_architecture\n",
      "  Unique values: 4 → 4\n",
      "Standardizing column: tech_tf_web_development\n",
      "  Unique values: 1 → 1\n",
      "\n",
      "=== NORMALIZING LANGUAGE TYPE ===\n",
      "Normalizing tech_tf_language_type to uppercase\n",
      "  Unique values: 3 → 3\n",
      "\n",
      "=== VERIFICATION RESULTS ===\n",
      "\n",
      "Column: project_prf_application_group\n",
      "  Total unique values: 4\n",
      "  All semicolon-separated values appear properly sorted and deduplicated\n",
      "  Sample values: ['business application', 'real-time application', 'mathematically-intensive application']\n",
      "\n",
      "Column: external_eef_organisation_type\n",
      "  Total unique values: 25\n",
      "  All semicolon-separated values appear properly sorted and deduplicated\n",
      "  All semicolon separators are consistent ('; ')\n",
      "  Sample values: ['banking; communications; education institution; government; medical and health care; transport & storage; wholesale & retail trade', 'government', 'community services']\n",
      "\n",
      "Column: project_prf_application_type\n",
      "  Total unique values: 32\n",
      "  All semicolon-separated values appear properly sorted and deduplicated\n",
      "  All semicolon separators are consistent ('; ')\n",
      "  Sample values: ['surveillance and security', 'business application', 'complex process control; workflow support & management']\n",
      "\n",
      "Column: process_pmf_development_methodologies\n",
      "  Total unique values: 7\n",
      "  All semicolon-separated values appear properly sorted and deduplicated\n",
      "  All semicolon separators are consistent ('; ')\n",
      "  Sample values: ['agile development', 'agile development; unified process', 'agile development; personal software process psp; unified process']\n",
      "\n",
      "Column: tech_tf_client_roles\n",
      "  Total unique values: 10\n",
      "  All semicolon-separated values appear properly sorted and deduplicated\n",
      "  All semicolon separators are consistent ('; ')\n",
      "  Sample values: ['data entry & validation; data retrieval & presentation; web/html browser', 'web public interface', 'data entry & validation; data retrieval & presentation; run a computer-human interface; security; web/html browser']\n",
      "\n",
      "Column: tech_tf_server_roles\n",
      "  Total unique values: 12\n",
      "  All semicolon-separated values appear properly sorted and deduplicated\n",
      "  All semicolon separators are consistent ('; ')\n",
      "  Sample values: ['html/web server; security/authentication', 'multi-user legacy application', 'database server; file &/or print server; html/web server; multi-user legacy application']\n",
      "\n",
      "Column: project_prf_application_group\n",
      "  Total unique values: 4\n",
      "  All semicolon-separated values appear properly sorted and deduplicated\n",
      "  Sample values: ['business application', 'real-time application', 'mathematically-intensive application']\n",
      "\n",
      "=== ADDITIONAL CHECKS ===\n",
      "Column project_prf_application_group: All parts properly trimmed\n",
      "Column external_eef_organisation_type: Found values with untrimmed parts: ['banking; communications; education institution; government; medical and health care; transport & storage; wholesale & retail trade', 'education institution; electricity, gas, water; ieee']\n",
      "Column project_prf_application_type: Found values with untrimmed parts: ['complex process control; workflow support & management', 'embedded software - simple device control; online analysis and reporting; telecom & network management']\n",
      "Column process_pmf_development_methodologies: Found values with untrimmed parts: ['agile development; unified process', 'agile development; personal software process psp; unified process']\n",
      "Column tech_tf_client_roles: Found values with untrimmed parts: ['data entry & validation; data retrieval & presentation; web/html browser', 'data entry & validation; data retrieval & presentation; run a computer-human interface; security; web/html browser']\n",
      "Column tech_tf_server_roles: Found values with untrimmed parts: ['html/web server; security/authentication', 'database server; file &/or print server; html/web server; multi-user legacy application']\n",
      "Column project_prf_application_group: All parts properly trimmed\n",
      "Cell executed at: 2025-05-25 14:00:56.857565\n"
     ]
    }
   ],
   "source": [
    "# Define column categories\n",
    "cols_with_semicolons = [\n",
    "    'external_eef_organisation_type', \n",
    "    'project_prf_application_type',\n",
    "    'project_prf_development_type',\n",
    "    'process_pmf_development_methodologies',\n",
    "    'tech_tf_client_server',\n",
    "    'tech_tf_client_roles',\n",
    "    'tech_tf_server_roles'\n",
    "]\n",
    "\n",
    "# Columns that have semicolons AND need standardization\n",
    "cols_semicolon_with_standardization = [\n",
    "    'project_prf_application_group'  # This one needs both treatments\n",
    "]\n",
    "\n",
    "# Columns that only need single-value standardization (no semicolons)\n",
    "cols_single_standardization = [\n",
    "    'tech_tf_architecture',\n",
    "    'tech_tf_web_development'\n",
    "]\n",
    "\n",
    "# Clean semicolon-separated columns (without standardization)\n",
    "print(\"=== CLEANING SEMICOLON-SEPARATED COLUMNS ===\")\n",
    "for col in cols_with_semicolons:\n",
    "    if col in df.columns:\n",
    "        print(f\"Cleaning column: {col}\")\n",
    "        original_unique = len(df[col].dropna().unique())\n",
    "        df[col] = df[col].map(clean_and_sort_semicolon)\n",
    "        new_unique = len(df[col].dropna().unique())\n",
    "        print(f\"  Unique values: {original_unique} → {new_unique}\")\n",
    "        \n",
    "        # Show sample values\n",
    "        sample_vals = df[col].dropna().unique()[:3]\n",
    "        print(f\"  Sample values: {[str(v) for v in sample_vals]}\")\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found in dataframe\")\n",
    "\n",
    "# Clean semicolon-separated columns WITH standardization\n",
    "print(f\"\\n=== CLEANING SEMICOLON COLUMNS WITH STANDARDIZATION ===\")\n",
    "for col in cols_semicolon_with_standardization:\n",
    "    if col in df.columns:\n",
    "        print(f\"Cleaning column: {col}\")\n",
    "        original_unique = len(df[col].dropna().unique())\n",
    "        df[col] = df[col].map(lambda x: clean_and_sort_semicolon(x, apply_standardization=True))\n",
    "        new_unique = len(df[col].dropna().unique())\n",
    "        print(f\"  Unique values: {original_unique} → {new_unique}\")\n",
    "        \n",
    "        # Show sample values\n",
    "        sample_vals = df[col].dropna().unique()[:3]\n",
    "        print(f\"  Sample values: {[str(v) for v in sample_vals]}\")\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found in dataframe\")\n",
    "\n",
    "# Apply standardization to single-value columns\n",
    "print(f\"\\n=== STANDARDIZING SINGLE-VALUE COLUMNS ===\")\n",
    "for col in cols_single_standardization:\n",
    "    if col in df.columns:\n",
    "        print(f\"Standardizing column: {col}\")\n",
    "        original_unique = len(df[col].dropna().unique())\n",
    "        df[col] = df[col].map(standardize_single_value)\n",
    "        new_unique = len(df[col].dropna().unique())\n",
    "        print(f\"  Unique values: {original_unique} → {new_unique}\")\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found in dataframe\")\n",
    "\n",
    "# Special case for language type (uppercase normalization)\n",
    "if 'tech_tf_language_type' in df.columns:\n",
    "    print(f\"\\n=== NORMALIZING LANGUAGE TYPE ===\")\n",
    "    print(\"Normalizing tech_tf_language_type to uppercase\")\n",
    "    original_unique = len(df['tech_tf_language_type'].dropna().unique())\n",
    "    df['tech_tf_language_type'] = df['tech_tf_language_type'].astype(str).str.upper().str.strip()\n",
    "    # Replace 'NAN' with actual NaN\n",
    "    df['tech_tf_language_type'] = df['tech_tf_language_type'].replace('NAN', pd.NA)\n",
    "    new_unique = len(df['tech_tf_language_type'].dropna().unique())\n",
    "    print(f\"  Unique values: {original_unique} → {new_unique}\")\n",
    "\n",
    "# Verification function to check the cleaning results\n",
    "def verify_semicolon_cleaning(df, columns):\n",
    "    \"\"\"Verify that semicolon-separated columns are properly cleaned.\"\"\"\n",
    "    print(\"\\n=== VERIFICATION RESULTS ===\")\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            print(f\"\\nColumn: {col}\")\n",
    "            unique_vals = df[col].dropna().unique()\n",
    "            print(f\"  Total unique values: {len(unique_vals)}\")\n",
    "            \n",
    "            # Check for unsorted or duplicate issues\n",
    "            problematic = []\n",
    "            for val in unique_vals:\n",
    "                if ';' in str(val):\n",
    "                    parts = str(val).split(';')\n",
    "                    parts_stripped = [p.strip() for p in parts]\n",
    "                    if parts_stripped != sorted(set(parts_stripped)):\n",
    "                        problematic.append(val)\n",
    "            \n",
    "            if problematic:\n",
    "                print(f\"  ⚠️  Potentially problematic values: {problematic[:3]}\")\n",
    "            else:\n",
    "                print(\"  ✓ All semicolon-separated values appear properly sorted and deduplicated\")\n",
    "            \n",
    "            # Check for consistency in separators\n",
    "            semicolon_vals = [v for v in unique_vals if ';' in str(v)]\n",
    "            if semicolon_vals:\n",
    "                inconsistent_separators = [v for v in semicolon_vals if '; ' not in str(v) and ';' in str(v)]\n",
    "                if inconsistent_separators:\n",
    "                    print(f\"  ⚠️  Inconsistent separators: {inconsistent_separators[:3]}\")\n",
    "                else:\n",
    "                    print(\"  ✓ All semicolon separators are consistent ('; ')\")\n",
    "            \n",
    "            # Show sample of values\n",
    "            sample_vals = [str(v) for v in unique_vals[:3] if pd.notna(v)]\n",
    "            print(f\"  Sample values: {sample_vals}\")\n",
    "\n",
    "# Run verification on all semicolon columns\n",
    "all_semicolon_cols = cols_with_semicolons + cols_semicolon_with_standardization\n",
    "verify_semicolon_cleaning(df, all_semicolon_cols)\n",
    "\n",
    "# Show detailed before/after comparison for a few problematic values\n",
    "def show_detailed_cleaning_examples(df, columns, max_examples=3):\n",
    "    \"\"\"Show detailed before/after examples of cleaning for verification.\"\"\"\n",
    "    print(f\"\\n=== DETAILED CLEANING EXAMPLES ===\")\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nColumn: {col}\")\n",
    "        \n",
    "        # Get some complex values (those with semicolons)\n",
    "        complex_vals = [v for v in df[col].dropna().unique() if ';' in str(v)][:max_examples]\n",
    "        \n",
    "        if complex_vals:\n",
    "            print(\"  Complex semicolon-separated values found:\")\n",
    "            for i, val in enumerate(complex_vals, 1):\n",
    "                print(f\"    {i}. '{val}'\")\n",
    "                # Show the individual parts\n",
    "                parts = str(val).split(';')\n",
    "                for j, part in enumerate(parts):\n",
    "                    print(f\"       Part {j+1}: '{part.strip()}'\")\n",
    "        else:\n",
    "            # Show simple values\n",
    "            simple_vals = df[col].dropna().unique()[:max_examples]\n",
    "            print(\"  Sample values (no semicolons):\")\n",
    "            for i, val in enumerate(simple_vals, 1):\n",
    "                print(f\"    {i}. '{val}'\")\n",
    "\n",
    "show_detailed_cleaning_examples(df, all_semicolon_cols)\n",
    "\n",
    "# Function to identify potential remaining issues\n",
    "def identify_remaining_issues(df, columns):\n",
    "    \"\"\"Identify potential issues that might still need attention.\"\"\"\n",
    "    print(f\"\\n=== POTENTIAL REMAINING ISSUES ===\")\n",
    "    \n",
    "    issues_found = False\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        col_issues = []\n",
    "        unique_vals = df[col].dropna().unique()\n",
    "        \n",
    "        for val in unique_vals:\n",
    "            val_str = str(val)\n",
    "            \n",
    "            # Check for various potential issues that should be cleaned\n",
    "            if ' & ' in val_str:\n",
    "                col_issues.append(f\"Still contains ' & ': '{val_str}'\")\n",
    "            if '&/or' in val_str:\n",
    "                col_issues.append(f\"Still contains '&/or': '{val_str}'\")\n",
    "            if ' &' in val_str:\n",
    "                col_issues.append(f\"Still contains ' &': '{val_str}'\")\n",
    "            if '/' in val_str:\n",
    "                col_issues.append(f\"Still contains '/': '{val_str}'\")\n",
    "            if ': ' in val_str:\n",
    "                col_issues.append(f\"Still contains ': ': '{val_str}'\")\n",
    "            if ' (' in val_str or '(' in val_str or ')' in val_str:\n",
    "                col_issues.append(f\"Still contains parentheses: '{val_str}'\")\n",
    "            if ' + ' in val_str:\n",
    "                col_issues.append(f\"Still contains ' + ': '{val_str}'\")\n",
    "            if '  ' in val_str:  # Double spaces\n",
    "                col_issues.append(f\"Contains double spaces: '{val_str}'\")\n",
    "            if val_str.endswith(' ') or val_str.startswith(' '):\n",
    "                col_issues.append(f\"Has leading/trailing spaces: '{val_str}'\")\n",
    "            if re.search(r'[A-Z]', val_str):  # Contains uppercase\n",
    "                col_issues.append(f\"Contains uppercase: '{val_str}'\")\n",
    "            if '__' in val_str:  # Double underscores\n",
    "                col_issues.append(f\"Contains double underscores: '{val_str}'\")\n",
    "        \n",
    "        if col_issues:\n",
    "            print(f\"\\nColumn: {col}\")\n",
    "            issues_found = True\n",
    "            for issue in col_issues[:5]:  # Show first 5 issues\n",
    "                print(f\"  - {issue}\")\n",
    "            if len(col_issues) > 5:\n",
    "                print(f\"  ... and {len(col_issues) - 5} more issues\")\n",
    "    \n",
    "    if not issues_found:\n",
    "        print(\"✓ No obvious formatting issues detected!\")\n",
    "\n",
    "# Show examples of transformations\n",
    "def show_transformation_examples():\n",
    "    \"\"\"Show examples of how the cleaning rules transform values.\"\"\"\n",
    "    print(f\"\\n=== TRANSFORMATION EXAMPLES ===\")\n",
    "    \n",
    "    test_cases = [\n",
    "        \"workflow support & management\",\n",
    "        \"data entry &/or validation\", \n",
    "        \"file &/or print server\",\n",
    "        \"html/web server: security\",\n",
    "        \"client (desktop) application\",\n",
    "        \"database + file server\",\n",
    "        \"agile development ; scrum\",\n",
    "        \"web development: html & css\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Original → Cleaned:\")\n",
    "    for case in test_cases:\n",
    "        cleaned = apply_individual_standardization(case.lower())\n",
    "        print(f\"  '{case}' → '{cleaned}'\")\n",
    "\n",
    "show_transformation_examples()\n",
    "\n",
    "identify_remaining_issues(df, all_semicolon_cols)\n",
    "\n",
    "# Additional verification: Check for common issues\n",
    "print(f\"\\n=== ADDITIONAL CHECKS ===\")\n",
    "for col in all_semicolon_cols:\n",
    "    if col in df.columns:\n",
    "        # Check for trailing/leading spaces in parts\n",
    "        problem_vals = []\n",
    "        for val in df[col].dropna().unique():\n",
    "            if ';' in str(val):\n",
    "                parts = str(val).split(';')\n",
    "                for part in parts:\n",
    "                    if part != part.strip():\n",
    "                        problem_vals.append(val)\n",
    "                        break\n",
    "        \n",
    "        if problem_vals:\n",
    "            print(f\"Column {col}: Found values with untrimmed parts: {problem_vals[:2]}\")\n",
    "        else:\n",
    "            print(f\"Column {col}: ✓ All parts properly trimmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea001aed-6c12-440d-ae68-ae3d948a4115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Dropping: Unique values for categorical columns saved to '../temp/all_categorical_unique_values_beforeDropping.txt'\n",
      "Cell executed at: 2025-05-25 14:00:56.879212\n"
     ]
    }
   ],
   "source": [
    "# Writes the unique values of all categorical columns to a text file for reference or auditing.\n",
    "\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "with open(f\"{temp_folder}/all_categorical_unique_values_beforeDropping.txt\", 'w') as f:\n",
    "    for col in cat_cols:\n",
    "        f.write(f\"Column: {col} (n_unique = {df[col].nunique()})\\n\")\n",
    "        f.write(f\"{df[col].unique()}\\n\")\n",
    "        f.write('-' * 40 + '\\n')\n",
    "\n",
    "print(f\"Before Dropping: Unique values for categorical columns saved to '{temp_folder}/all_categorical_unique_values_beforeDropping.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31086bd0-d2c5-4a61-bd38-4358f4303656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell executed at: 2025-05-24 19:08:06.763176\n"
     ]
    }
   ],
   "source": [
    "# Save the entire cleaned DataFrame (not just the column names) to CSV\n",
    "df.to_csv(f\"{data_folder}/{file_name_no_ext}_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd70455-798a-49a2-b564-2765c63cdcc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3df8f-1004-4c7e-a548-fe22b8847afc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
